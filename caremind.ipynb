{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da58641a",
   "metadata": {},
   "source": [
    "# CareMind: a MVP CDSS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473557e2",
   "metadata": {},
   "source": [
    "## 0 目标与范围（MVP 边界）\n",
    "\n",
    "覆盖 1–2 个优先疾病场景（如高血压、2 型糖尿病）。\n",
    "\n",
    "知识源：3–5 份卫健委指南 + 1–2 份护理共识 + 10 种常用药（阿司匹林、二甲双胍、氨氯地平等）。\n",
    "\n",
    "能力：自然语言提问 →（向量检索指南/共识 + SQL 查询药品表）→ 生成带引用与合规提示的回答。\n",
    "\n",
    "UI：简单的 Streamlit 单页（输入问题、展示依据片段与来源、药品表结构化信息、免责声明）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636d9ab8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51b8d710",
   "metadata": {},
   "source": [
    "现有本地环境（Win11 + WSL + RTX 4070 SUPER + VS Code/Jupyter），并以**RAG（向量库）+ 结构化药品库（SQLite）**的混合检索为核心。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66414576",
   "metadata": {},
   "source": [
    "## 1. 环境准备（WSL 内执行）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdbea64",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6923260d",
   "metadata": {},
   "source": [
    "### 1.1 CUDA/驱动"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1add5400",
   "metadata": {},
   "source": [
    "Windows 侧已装好 NVIDIA 驱动与 CUDA/cuDNN；WSL2 中建议使用 nvidia-smi 与 torch.cuda.is_available() 验证 GPU 可用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edf5e03",
   "metadata": {},
   "source": [
    "(base) myunix@40VFO2U:~$ mkdir caremind\n",
    "\n",
    "(base) myunix@40VFO2U:~$ cd caremind\n",
    "\n",
    "(base) myunix@40VFO2U:~/caremind$\n",
    "\n",
    "(base) myunix@40VFO2U:~/caremind$ nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ed62a4",
   "metadata": {},
   "source": [
    "### 1.2 本地推理服务（推荐 Ollama + Qwen2）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5276a91b",
   "metadata": {},
   "source": [
    "我用 4070 SUPER，本地 7B–14B 推理很合适。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62969a7c",
   "metadata": {},
   "source": [
    "(base) myunix@40VFO2U:~/caremind$ curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    ">>> Cleaning up old version at /usr/local/lib/ollama\n",
    "\n",
    "[sudo] password for myunix:\n",
    "\n",
    ">>> Installing ollama to /usr/local\n",
    "\n",
    ">>> Downloading Linux amd64 bundle\n",
    "\n",
    "######################################################################## 100.0%\n",
    "\n",
    ">>> Creating ollama user...\n",
    "\n",
    ">>> Adding ollama user to render group...\n",
    "\n",
    ">>> Adding ollama user to video group...\n",
    "\n",
    ">>> Adding current user to ollama group...\n",
    "\n",
    ">>> Creating ollama systemd service...\n",
    "\n",
    ">>> Enabling and starting ollama service...\n",
    "\n",
    "Created symlink /etc/systemd/system/default.target.wants/ollama.service → /etc/systemd/system/ollama.service.\n",
    "\n",
    ">>> Nvidia GPU detected.\n",
    "\n",
    ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
    "\n",
    ">>> Install complete. Run \"ollama\" from the command line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e27f398",
   "metadata": {},
   "source": [
    "## 2 项目结构与配置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951903e2",
   "metadata": {},
   "source": [
    "caremind/\n",
    "  ├─ .env                           # 环境变量\n",
    "  ├─ data/\n",
    "  │   ├─ guidelines/                # 卫健委/护理共识 PDF/HTML/Word\n",
    "  │   └─ drugs.xlsx                 # 10种药的Excel（初期）\n",
    "  ├─ db/\n",
    "  │   └─ drugs.sqlite               # SQLite 实例\n",
    "  ├─ embeddings/\n",
    "  │   └─ bge-large-zh/              # 可选：缓存模型权重\n",
    "  ├─ ingest/\n",
    "  │   ├─ parse_docs.py              # 文档抽取与切片\n",
    "  │   ├─ build_vectors.py           # 嵌入与写入 Chroma\n",
    "  │   └─ load_drugs.py              # Excel→SQLite\n",
    "  ├─ rag/\n",
    "  │   ├─ retriever.py               # 混合检索（Chroma+SQLite）\n",
    "  │   ├─ prompt.py                  # 提示词模板（含合规要求与引用格式）\n",
    "  │   └─ pipeline.py                # 端到端 RAG 管线\n",
    "  ├─ app.py                         # Streamlit 前端\n",
    "  └─ README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f15568",
   "metadata": {},
   "source": [
    "(base) myunix@40VFO2U:~/caremind$ mkdir data\n",
    "\n",
    "(base) myunix@40VFO2U:~/caremind$ cd data\n",
    "\n",
    "(base) myunix@40VFO2U:~/caremind/data$ mkdir guidlines\n",
    "\n",
    "(base) myunix@40VFO2U:~/caremind/data$ cd ..\n",
    "\n",
    "(base) myunix@40VFO2U:~/caremind$ mkdir db\n",
    "\n",
    "(base) myunix@40VFO2U:~/caremind$ mkdir embeddings\n",
    "\n",
    "(base) myunix@40VFO2U:~/caremind$ cd embeddings\n",
    "\n",
    "(base) myunix@40VFO2U:~/caremind/embeddings$ mkdir bge-large-zh\n",
    "\n",
    "(base) myunix@40VFO2U:~/caremind/embeddings$ cd ..\n",
    "\n",
    "(base) myunix@40VFO2U:~/caremind$ mkdir ingest\n",
    "\n",
    "(base) myunix@40VFO2U:~/caremind$ mkdir rag\n",
    "\n",
    "(base) myunix@40VFO2U:~/caremind$ ls\n",
    "\n",
    "data  db  embeddings  ingest  rag\n",
    "\n",
    "(base) myunix@40VFO2U:~/caremind$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30358c55",
   "metadata": {},
   "source": [
    "#### .env 示例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf3358a",
   "metadata": {},
   "source": [
    "##### # LLM/Ollama\n",
    "OLLAMA_BASE_URL=http://localhost:11434\n",
    "LLM_MODEL=qwen2:7b-instruct\n",
    "\n",
    "##### # Embedding 模型（中文优先）\n",
    "EMBEDDING_MODEL=BAAI/bge-large-zh-v1.5\n",
    "\n",
    "##### # Chroma 向量库存储路径\n",
    "CHROMA_PERSIST_DIR=./chroma_store\n",
    "\n",
    "##### # SQLite\n",
    "SQLITE_PATH=./db/drugs.sqlite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7b4baf",
   "metadata": {},
   "source": [
    "### 1.4 Conda 环境与依赖"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12bc2e1",
   "metadata": {},
   "source": [
    "(base) myunix@40VFO2U:~/caremind$conda create -n caremind python=3.10.18 -y\n",
    "\n",
    "(base) myunix@40VFO2U:~/caremind$conda activate caremind\n",
    "\n",
    "(caremind) myunix@40VFO2U:~/caremind$ pip install --upgrade pip\n",
    "\n",
    "#### 核心：RAG + 分词/嵌入 + 本地LLM(走Ollama) + UI + PDF抽取 + 中文处理\n",
    "(caremind) myunix@40VFO2U:~/caremind$ pip install langchain langchain-community chromadb pydantic-settings\n",
    "\n",
    "(caremind) myunix@40VFO2U:~/caremind$ pip install sentence-transformers # 用于中文 bge\n",
    "\n",
    "(caremind) myunix@40VFO2U:~/caremind$ pip install transformers accelerate torch --extra-index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "(caremind) myunix@40VFO2U:~/caremind$ pip install pdfplumber pypdf pdf2image pytesseract pillow beautifulsoup4 lxml\n",
    "\n",
    "(caremind) myunix@40VFO2U:~/caremind$ pip install jieba nltk\n",
    "\n",
    "(caremind) myunix@40VFO2U:~/caremind$ pip install streamlit\n",
    "\n",
    "(caremind) myunix@40VFO2U:~/caremind$ pip install python-dotenv\n",
    "\n",
    "(caremind) myunix@40VFO2U:~/caremind$ pip install sqlalchemy aiosqlite\n",
    "\n",
    "Newly added:\n",
    "\n",
    "(caremind) myunix@40VFO2U:~/caremind$ pip install -U \"chromadb>=0.5\"\n",
    "\n",
    "pip install requests pandas openpyxl tenacity pdfminer.six pypdf2 chardet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf94c176",
   "metadata": {},
   "source": [
    "### 1.5 Open VS Code from WSL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b77ec79",
   "metadata": {},
   "source": [
    "(caremind) myunix@40VFO2U:~/caremind$ code ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7fbb45",
   "metadata": {},
   "source": [
    "create or open file: caremind.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d95589",
   "metadata": {},
   "source": [
    "use ctrl+shit+p to select the Python Environment: caremind(Python3.10.18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70851a4",
   "metadata": {},
   "source": [
    "## 3 数据准备与入库"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6990110",
   "metadata": {},
   "source": [
    "### 3.1 文档抽取与切片（指南/护理共识）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d4f7c1",
   "metadata": {},
   "source": [
    "策略：按标题层级 + 语义切片，每块 500–1000 字，保留元数据（来源、年份/版本、证据等级、人群适用）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12578ec9",
   "metadata": {},
   "source": [
    "Working dir assumption (you can change paths if needed): /home/myunix/caremind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bd34d80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/myunix/caremind'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dc6f80",
   "metadata": {},
   "source": [
    "#### Pipeline\n",
    "1. Extract year from filename (Chinese guideline naming patterns).\n",
    "2. Extract text (pdfplumber → OCR fallback with pdf2image + pytesseract).\n",
    "3. Heuristically split into titled chunks.\n",
    "4. Write JSONL with metadata (content, meta)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be660df3",
   "metadata": {},
   "source": [
    "🧱 SECTION BREAKDOWN\n",
    "SECTION PURPOSE\n",
    "1. Metadata Extraction Utils\n",
    "Self-contained functions to extract: year,title,authors,typefrom filename — easily testable and reusable.\n",
    "2. Text Extraction\n",
    "pdfplumber, wrapper + optional OCR fallback (commented out).\n",
    "3. Chunking Logic\n",
    "Enhanced heuristic-based chunking tuned for Chinese medical text — outputs structured chunks with full metadata.\n",
    "4. Main Pipeline\n",
    "Orchestrates file discovery → metadata extraction → text extraction → chunking → JSONL output. Includes debug prints.\n",
    "5. Run\n",
    "Standard Python entry point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f44a3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Input directory: /home/myunix/caremind/data/guidelines\n",
      "📄 Found 16 PDF files.\n",
      "\n",
      "--- 📄 Processing: 妊娠期糖尿病患者产前血糖管理的证据总结_秦煜(2023).pdf ---\n",
      "  📅 Year (from filename): 2023\n",
      "  🏷️ Title (from filename): 妊娠期糖尿病患者产前血糖管理的证据总结\n",
      "  👥 Authors (from filename): ['秦煜']\n",
      "  📑 Type (from filename): evidence_summary\n",
      "  🔤 Extracted 11758 characters.\n",
      "  ✍️  Authors (final): ['秦煜']\n",
      "  📚 Journal:  (), , \n",
      "  🔗 DOI: \n",
      "  🏷️ Keywords: []\n",
      "  🧭 Original Guideline: \n",
      "  📑 Doc Type (final): evidence_summary\n",
      "  🧩 Generated 34 chunks.\n",
      "\n",
      "--- 📄 Processing: 冠心病合并2 型糖尿病患者的血糖管理专家共识(2024)_中国医疗保健国际交流促进会心血管病学分会.pdf ---\n",
      "  📅 Year (from filename): 2024\n",
      "  🏷️ Title (from filename): 冠心病合并2 型糖尿病患者的血糖管理专家共识\n",
      "  👥 Authors (from filename): ['中国医疗保健国际交流促进会心血管病学分会']\n",
      "  📑 Type (from filename): consensus\n",
      "  🔤 Extracted 29204 characters.\n",
      "  ✍️  Authors (final): ['中国医疗保健国际交流促进会心血管病学分会']\n",
      "  📚 Journal:  (), , \n",
      "  🔗 DOI: 10.3969/j.issn.1000-3614.2024.04.003ChaoXing\n",
      "  🏷️ Keywords: []\n",
      "  🧭 Original Guideline: \n",
      "  📑 Doc Type (final): consensus\n",
      "  🧩 Generated 103 chunks.\n",
      "\n",
      "--- 📄 Processing: 中国高血压患者心率管理多学科专家共识（2021年版）_高血压心率管理多学科共识组.pdf ---\n",
      "  📅 Year (from filename): 2021\n",
      "  🏷️ Title (from filename): 中国高血压患者心率管理多学科专家共识\n",
      "  👥 Authors (from filename): ['高血压心率管理多学科共识组']\n",
      "  📑 Type (from filename): consensus\n",
      "  🔤 Extracted 18607 characters.\n",
      "  ✍️  Authors (final): ['高血压心率管理多学科共识组']\n",
      "  📚 Journal:  (), 2501, \n",
      "  🔗 DOI: 10.12114/j.issn.1007-9572.2021.00.595\n",
      "  🏷️ Keywords: ['高血压', '心率', '疾病管理', '多学科诊疗模式', '专家共识']\n",
      "  🧭 Original Guideline: \n",
      "  📑 Doc Type (final): consensus\n",
      "  🧩 Generated 107 chunks.\n",
      "\n",
      "--- 📄 Processing: 成人2型糖尿病的高血压管理中国专家共识(2025).pdf ---\n",
      "  📅 Year (from filename): 2025\n",
      "  🏷️ Title (from filename): 成人2型糖尿病的高血压管理中国专家共识\n",
      "  👥 Authors (from filename): []\n",
      "  📑 Type (from filename): consensus\n",
      "  🔤 Extracted 39421 characters.\n",
      "  ✍️  Authors (final): ['重庆', '： ；李舍予', '四川大学华西医院内分泌代谢科', '成都']\n",
      "  📚 Journal:  (), , \n",
      "  🔗 DOI: 10.1111/\n",
      "  🏷️ Keywords: ['糖尿病', '型', '高血压', '高血压筛查', '血压监测', '降压治疗']\n",
      "  🧭 Original Guideline: \n",
      "  📑 Doc Type (final): consensus\n",
      "  🧩 Generated 279 chunks.\n",
      "\n",
      "--- 📄 Processing: 《中国高血压防治指南(2024年修订版)》新增内容解读 ——以改善血压变异和降压目标范围内时间为核心的高质量降压策略浅析_张新军.pdf ---\n",
      "  📅 Year (from filename): 2024\n",
      "  🏷️ Title (from filename): 《中国高血压防治指南》新增内容解读 以改善血压变异和降压目标范围内时间为核心的高质量降压策略浅析\n",
      "  👥 Authors (from filename): ['张新军']\n",
      "  📑 Type (from filename): guideline_interpretation\n",
      "  🔤 Extracted 12125 characters.\n",
      "  ✍️  Authors (final): ['张新军']\n",
      "  📚 Journal:  (), , \n",
      "  🔗 DOI: 10.3969/j.issn.1007-5410.2024.05.002\n",
      "  🏷️ Keywords: ['高血压 血压管理 指南 解读 血压变异 目标范围内时间']\n",
      "  🧭 Original Guideline: 中国高血压防治指南 (2024 年修订版)\n",
      "  📑 Doc Type (final): guideline_interpretation\n",
      "  🧩 Generated 81 chunks.\n",
      "\n",
      "--- 📄 Processing: 中国2型糖尿病防治指南（2020年版）.pdf ---\n",
      "  📅 Year (from filename): 2020\n",
      "  🏷️ Title (from filename): 中国2型糖尿病防治指南\n",
      "  👥 Authors (from filename): []\n",
      "  📑 Type (from filename): guideline\n",
      "  🔤 Extracted 329233 characters.\n",
      "  ✍️  Authors (final): ['通信作者：朱大龙', '南京大学医学院附属鼓楼医院内分泌科']\n",
      "  📚 Journal:  (), , \n",
      "  🔗 DOI: 10.3760/cma.j.cn115791-20210221-00095\n",
      "  🏷️ Keywords: ['糖尿病', '型', '指南']\n",
      "  🧭 Original Guideline: 中国 型糖尿病防治指南（ 年版）\n",
      "  📑 Doc Type (final): guideline\n",
      "  🧩 Generated 1631 chunks.\n",
      "\n",
      "--- 📄 Processing: 国家基层高血压防治管理指南 2020版.pdf ---\n",
      "  📅 Year (from filename): 2020\n",
      "  🏷️ Title (from filename): 国家基层高血压防治管理指南 2020版\n",
      "  👥 Authors (from filename): []\n",
      "  📑 Type (from filename): guideline\n",
      "  🔤 Extracted 24188 characters.\n",
      "  ✍️  Authors (final): []\n",
      "  📚 Journal:  (), , \n",
      "  🔗 DOI: \n",
      "  🏷️ Keywords: []\n",
      "  🧭 Original Guideline: \n",
      "  📑 Doc Type (final): guideline\n",
      "  🧩 Generated 148 chunks.\n",
      "\n",
      "--- 📄 Processing: 中国慢性肾脏病早期评价与管理指南(2023).pdf ---\n",
      "  📅 Year (from filename): 2023\n",
      "  🏷️ Title (from filename): 中国慢性肾脏病早期评价与管理指南\n",
      "  👥 Authors (from filename): []\n",
      "  📑 Type (from filename): guideline\n",
      "  🔤 Extracted 98010 characters.\n",
      "  ✍️  Authors (final): ['通信作者：张路霞', '北京大学健康医疗大数据国家研究院', '北京']\n",
      "  📚 Journal:  (), , \n",
      "  🔗 DOI: 10.3760/cma.j.cn112138-20221013-00755\n",
      "  🏷️ Keywords: ['慢性肾脏病', '指南', '评价', '管理', '预防']\n",
      "  🧭 Original Guideline: \n",
      "  📑 Doc Type (final): guideline\n",
      "  🧩 Generated 412 chunks.\n",
      "\n",
      "--- 📄 Processing: 成人糖尿病患者血压管理专家共识(2021).pdf ---\n",
      "  📅 Year (from filename): 2021\n",
      "  🏷️ Title (from filename): 成人糖尿病患者血压管理专家共识\n",
      "  👥 Authors (from filename): []\n",
      "  📑 Type (from filename): consensus\n",
      "  🔤 Extracted 26954 characters.\n",
      "  ✍️  Authors (final): []\n",
      "  📚 Journal:  (), ８, \n",
      "  🔗 DOI: \n",
      "  🏷️ Keywords: ['糖尿病', '高血压', '血压', '控制目标', '抗高血压药']\n",
      "  🧭 Original Guideline: \n",
      "  📑 Doc Type (final): consensus\n",
      "  🧩 Generated 94 chunks.\n",
      "\n",
      "--- 📄 Processing: 国家基层糖尿病防治管理指南（2022）.pdf ---\n",
      "  📅 Year (from filename): 2022\n",
      "  🏷️ Title (from filename): 国家基层糖尿病防治管理指南\n",
      "  👥 Authors (from filename): []\n",
      "  📑 Type (from filename): guideline\n",
      "  🔤 Extracted 26924 characters.\n",
      "  ✍️  Authors (final): ['医学中心上海市糖尿病重点实验室国家基层糖尿病防治管理办公室', '上海']\n",
      "  📚 Journal:  (), , \n",
      "  🔗 DOI: 10.3760/cma.j.cn112138-20220120-000063\n",
      "  🏷️ Keywords: ['糖尿病', '型', '防治', '基层卫生机构']\n",
      "  🧭 Original Guideline: 国家基层糖尿病防治管理指南（ ）\n",
      "  📑 Doc Type (final): guideline\n",
      "  🧩 Generated 203 chunks.\n",
      "\n",
      "--- 📄 Processing: 糖尿病患者甲病管理的最佳证据总结_陈欢(2022).pdf ---\n",
      "  📅 Year (from filename): 2022\n",
      "  🏷️ Title (from filename): 糖尿病患者甲病管理的最佳证据总结\n",
      "  👥 Authors (from filename): ['陈欢']\n",
      "  📑 Type (from filename): evidence_summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-07 22:16:28,210 | WARNING  | Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  🔤 Extracted 22961 characters.\n",
      "  ✍️  Authors (final): ['陈欢']\n",
      "  📚 Journal:  (), 3984, \n",
      "  🔗 DOI: 10.12114/j.issn.1007-9572.2022.0362\n",
      "  🏷️ Keywords: ['糖尿病足', '糖尿病', '甲病', '指（趾）甲', '循证医学', '证据总结']\n",
      "  🧭 Original Guideline: \n",
      "  📑 Doc Type (final): evidence_summary\n",
      "  🧩 Generated 65 chunks.\n",
      "\n",
      "--- 📄 Processing: 2型糖尿病患者运动方案的最佳证据总结(2019).pdf ---\n",
      "  📅 Year (from filename): 2019\n",
      "  🏷️ Title (from filename): 2型糖尿病患者运动方案的最佳证据总结\n",
      "  👥 Authors (from filename): []\n",
      "  📑 Type (from filename): evidence_summary\n",
      "  🔤 Extracted 15099 characters.\n",
      "  ✍️  Authors (final): []\n",
      "  📚 Journal:  (), , \n",
      "  🔗 DOI: 10\n",
      "  🏷️ Keywords: ['糖尿病', '2型', '运动治疗', '生活方式', '循证护理学']\n",
      "  🧭 Original Guideline: \n",
      "  📑 Doc Type (final): evidence_summary\n",
      "  🧩 Generated 47 chunks.\n",
      "\n",
      "--- 📄 Processing: 《妊娠期糖尿病临床护理实践指南》推荐意见专家共识_周英凤(2020).pdf ---\n",
      "  📅 Year (from filename): 2020\n",
      "  🏷️ Title (from filename): 《妊娠期糖尿病临床护理实践指南》推荐意见专家共识\n",
      "  👥 Authors (from filename): ['周英凤']\n",
      "  📑 Type (from filename): guideline\n",
      "  🔤 Extracted 16024 characters.\n",
      "  ✍️  Authors (final): ['周英凤']\n",
      "  📚 Journal:  (), , \n",
      "  🔗 DOI: R473.71A10.12102/j.issn.1009-6493.2020.24.001\n",
      "  🏷️ Keywords: []\n",
      "  🧭 Original Guideline: 妊娠期糖尿病临床护理实践指南\n",
      "  📑 Doc Type (final): guideline\n",
      "  🧩 Generated 40 chunks.\n",
      "\n",
      "--- 📄 Processing: 中国高血压防治指南(2024年修订版).pdf ---\n",
      "  📅 Year (from filename): 2024\n",
      "  🏷️ Title (from filename): 中国高血压防治指南\n",
      "  👥 Authors (from filename): []\n",
      "  📑 Type (from filename): guideline\n",
      "  🔤 Extracted 300858 characters.\n",
      "  ✍️  Authors (final): []\n",
      "  📚 Journal:  (), , \n",
      "  🔗 DOI: 10.16439/j.issn.1673-7245.2024.07.002\n",
      "  🏷️ Keywords: []\n",
      "  🧭 Original Guideline: \n",
      "  📑 Doc Type (final): guideline\n",
      "  🧩 Generated 2238 chunks.\n",
      "\n",
      "--- 📄 Processing: 糖尿病患者体重管理专家共识(2024版).pdf ---\n",
      "  📅 Year (from filename): 2024\n",
      "  🏷️ Title (from filename): 糖尿病患者体重管理专家共识\n",
      "  👥 Authors (from filename): []\n",
      "  📑 Type (from filename): consensus\n",
      "  🔤 Extracted 38020 characters.\n",
      "  ✍️  Authors (final): []\n",
      "  📚 Journal:  (), , \n",
      "  🔗 DOI: 10.3760/cma.j.cn115791-20240731-00396\n",
      "  🏷️ Keywords: ['糖尿病', '体重管理', '评估', '管理策略', '专家共识']\n",
      "  🧭 Original Guideline: \n",
      "  📑 Doc Type (final): consensus\n",
      "  🧩 Generated 244 chunks.\n",
      "\n",
      "--- 📄 Processing: 糖尿病患者血脂管理中国专家共识（2024版）.pdf ---\n",
      "  📅 Year (from filename): 2024\n",
      "  🏷️ Title (from filename): 糖尿病患者血脂管理中国专家共识\n",
      "  👥 Authors (from filename): []\n",
      "  📑 Type (from filename): consensus\n",
      "  🔤 Extracted 72407 characters.\n",
      "  ✍️  Authors (final): []\n",
      "  📚 Journal:  (), , \n",
      "  🔗 DOI: 10.3969/j.issn.1000-3614.2024.04.002\n",
      "  🏷️ Keywords: []\n",
      "  🧭 Original Guideline: \n",
      "  📑 Doc Type (final): consensus\n",
      "  🧩 Generated 182 chunks.\n",
      "\n",
      "✅ Output written to: /home/myunix/caremind/data/guidelines.parsed.jsonl\n",
      "💾 File size: 4531135 bytes\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # 📄 Medical Guideline PDF Parser v1.1\n",
    "# Enhanced to extract rich metadata from Chinese clinical guidelines and interpretation articles.\n",
    "# Distinguishes between official guidelines and expert interpretations.\n",
    "# Outputs: `guidelines.parsed.jsonl` with full bibliographic & structural metadata.\n",
    "\n",
    "# %%\n",
    "from pathlib import Path\n",
    "import pdfplumber \n",
    "import re\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# %%\n",
    "# =============================\n",
    "# 🧩 1. METADATA EXTRACTION UTILS (from filename)\n",
    "# =============================\n",
    "\n",
    "def extract_year_from_filename(filename: str) -> str:\n",
    "    \"\"\"Extracts 4-digit year from Chinese medical guideline filenames.\"\"\"\n",
    "    patterns = [\n",
    "        r'[（\\(]([12]\\d{3})[）\\)]',\n",
    "        r'[（\\(]([12]\\d{3})[年\\s]*(?:修订版|版|年版|年)?[）\\)]?',\n",
    "        r'([12]\\d{3})[年\\s]*(?:修订版|版|年版|年)?(?=[\\s_）\\)。\\.\\-]|$)',\n",
    "        r'[（\\(]?([12]\\d{3})[）\\)]?\\s*\\.pdf$',\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, filename)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    return \"unknown\"\n",
    "\n",
    "def extract_doc_title(filename: str) -> str:\n",
    "    \"\"\"Extract clean document title from filename.\"\"\"\n",
    "    base = re.sub(r\"_[^_]*\\.pdf$\", \"\", filename)\n",
    "    base = re.sub(r\"\\.pdf$\", \"\", base)\n",
    "    base = re.sub(r\"[（\\(][^）\\)]*[）\\)]\", \"\", base)\n",
    "    base = re.sub(r\"\\s*—+\\s*\", \" \", base)\n",
    "    base = re.sub(r\"\\s+\", \" \", base).strip()\n",
    "    return base or \"未命名文档\"\n",
    "\n",
    "def extract_authors_from_filename(filename: str) -> List[str]:\n",
    "    \"\"\"Extract author names from filename (after last underscore).\"\"\"\n",
    "    match = re.search(r\"_([^_\\(]+?)(?:\\([12]\\d{3}\\))?\\.pdf$\", filename)\n",
    "    if match:\n",
    "        author_str = match.group(1).strip()\n",
    "        authors = re.split(r\"[,，、]\", author_str)\n",
    "        return [a.strip() for a in authors if a.strip()]\n",
    "    return []\n",
    "\n",
    "def extract_doc_type_from_filename(filename: str) -> str:\n",
    "    \"\"\"Classify document type from filename.\"\"\"\n",
    "    if \"指南\" in filename and \"解读\" not in filename:\n",
    "        return \"guideline\"\n",
    "    elif \"解读\" in filename or \"浅析\" in filename or \"解析\" in filename:\n",
    "        return \"guideline_interpretation\"\n",
    "    elif \"共识\" in filename:\n",
    "        return \"consensus\"\n",
    "    elif \"证据总结\" in filename:\n",
    "        return \"evidence_summary\"\n",
    "    else:\n",
    "        return \"other\"\n",
    "\n",
    "# %%\n",
    "# =============================\n",
    "# 📚 2. METADATA EXTRACTION UTILS (from document text — PREFERRED)\n",
    "# =============================\n",
    "\n",
    "def extract_metadata_from_text(text: str) -> Dict[str, Any]:\n",
    "    \"\"\"Extract rich metadata from first page of document text.\"\"\"\n",
    "    meta = {\n",
    "        \"authors\": [],\n",
    "        \"corresponding_author\": \"\",\n",
    "        \"affiliations\": [],\n",
    "        \"journal_name\": \"\",\n",
    "        \"volume\": \"\",\n",
    "        \"issue\": \"\",\n",
    "        \"pages\": \"\",\n",
    "        \"doi\": \"\",\n",
    "        \"keywords\": [],\n",
    "        \"publish_date\": \"\",\n",
    "        \"original_guideline_title\": \"\",\n",
    "        \"doc_type\": \"other\"  # Will be overridden if detected\n",
    "    }\n",
    "\n",
    "    # Extract author (first non-empty line after title, before postal code or affiliation)\n",
    "    lines = [ln.strip() for ln in text.splitlines() if ln.strip()] #\n",
    "    for i, line in enumerate(lines[:10]):  # Look in first 10 lines\n",
    "        if re.match(r\"^\\d{6}\", line):  # Postal code → previous line is likely author\n",
    "            if i > 0:\n",
    "                author_line = lines[i-1]\n",
    "                authors = re.split(r\"[,，、]\", author_line)\n",
    "                meta[\"authors\"] = [a.strip() for a in authors if len(a.strip()) >= 2]\n",
    "            break\n",
    "        if \"通信作者\" in line:\n",
    "            author_match = re.search(r\"通信作者[:：]\\s*([^\\s，,、]+)\", line)\n",
    "            if author_match:\n",
    "                meta[\"corresponding_author\"] = author_match.group(1).strip()\n",
    "                if not meta[\"authors\"]:\n",
    "                    meta[\"authors\"] = [meta[\"corresponding_author\"]]\n",
    "\n",
    "    # Extract affiliation (lines with postal code or university)\n",
    "    for line in lines[:15]:\n",
    "        if re.search(r\"\\d{6}|大学|医院|中心\", line) and len(line) > 10:\n",
    "            meta[\"affiliations\"].append(line)\n",
    "\n",
    "    # Extract DOI\n",
    "    doi_match = re.search(r\"DOI\\s*[:：]?\\s*([0-9\\.\\s\\/a-z-]+)\", text, re.IGNORECASE)\n",
    "    if doi_match:\n",
    "        meta[\"doi\"] = re.sub(r\"\\s+\", \"\", doi_match.group(1)).strip()\n",
    "\n",
    "    # Extract journal, volume, issue, pages from footer pattern\n",
    "    # e.g., \"·396· 中国心血管杂志 2024年 10月第 29卷第 5期\"\n",
    "    journal_match = re.search(r\"·\\d+·\\s*([^\\s]+?杂志|学报)\\s*(\\d{4})年\\s*\\d+月第\\s*(\\d+)卷第\\s*(\\d+)期\", text)\n",
    "    if journal_match:\n",
    "        meta[\"journal_name\"] = journal_match.group(1).strip()\n",
    "        meta[\"publish_date\"] = journal_match.group(2).strip()  # e.g., \"2024\"\n",
    "        meta[\"volume\"] = journal_match.group(3).strip()\n",
    "        meta[\"issue\"] = journal_match.group(4).strip()\n",
    "\n",
    "    # Extract pages from header/footer (e.g., \"·396·\")\n",
    "    page_match = re.search(r\"·(\\d+)·\", text.splitlines()[0] if text.splitlines() else \"\")\n",
    "    if page_match:\n",
    "        start_page = page_match.group(1)\n",
    "        # Try to find end page (often not available, so leave as single page)\n",
    "        meta[\"pages\"] = start_page\n",
    "\n",
    "    # Extract keywords\n",
    "    kw_match = re.search(r\"【关键词】\\s*([^\\n【】]+)\", text)\n",
    "    if kw_match:\n",
    "        kw_str = kw_match.group(1).strip()\n",
    "        meta[\"keywords\"] = [k.strip() for k in re.split(r\"[,，;；、]\", kw_str) if k.strip()]\n",
    "\n",
    "    # Detect if this is an interpretation of a guideline\n",
    "    guideline_ref_match = re.search(r\"《([^》]+?指南[^》]*)》\", text[:500])\n",
    "    if guideline_ref_match:\n",
    "        meta[\"original_guideline_title\"] = guideline_ref_match.group(1).strip()\n",
    "        if \"解读\" in text[:200] or \"浅析\" in text[:200]:\n",
    "            meta[\"doc_type\"] = \"guideline_interpretation\"\n",
    "\n",
    "    # If no authors found but filename has them, fallback\n",
    "    # (Handled in main function)\n",
    "\n",
    "    return meta\n",
    "\n",
    "# %%\n",
    "# =============================\n",
    "# 📄 3. TEXT EXTRACTION\n",
    "# =============================\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: Path) -> str:\n",
    "    \"\"\"Extract text from PDF using pdfplumber.\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(str(pdf_path)) as pdf: \n",
    "            pages = [p.extract_text() or \"\" for p in pdf.pages] \n",
    "        return \"\\n\".join(pages)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  PDF extraction error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# %%\n",
    "# =============================\n",
    "# 🧱 4. CHUNKING LOGIC\n",
    "# =============================\n",
    "\n",
    "def chunk_by_rules(\n",
    "    text: str,\n",
    "    source_filename: str,\n",
    "    year: str,\n",
    "    doc_title: str,\n",
    "    authors: List[str],\n",
    "    doc_type: str,\n",
    "    original_guideline_title: str = \"\",\n",
    "    journal_name: str = \"\",\n",
    "    volume: str = \"\",\n",
    "    issue: str = \"\",\n",
    "    pages: str = \"\",\n",
    "    doi: str = \"\",\n",
    "    keywords: List[str] = [],\n",
    "    publish_date: str = \"\"\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Split text into chunks by section titles, with rich metadata.\"\"\"\n",
    "    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n",
    "    if not lines:\n",
    "        return []\n",
    "\n",
    "    chunks, buf = [], []\n",
    "    current_title = \"未命名章节\"\n",
    "\n",
    "    TITLE_KEYWORDS = [\n",
    "        \"章\", \"节\", \"篇\", \"部分\", \"概述\", \"背景\", \"目的\", \"方法\", \"结果\", \"结论\",\n",
    "        \"推荐\", \"建议\", \"管理\", \"治疗\", \"诊断\", \"评估\", \"定义\", \"目标\",\n",
    "        \"一、\", \"二、\", \"三、\", \"四、\", \"五、\", \"六、\", \"七、\", \"八、\", \"九、\", \"十、\",\n",
    "        \"1.\", \"2.\", \"3.\", \"4.\", \"5.\", \"6.\", \"7.\", \"8.\", \"9.\", \"10.\",\n",
    "        \"第一\", \"第二\", \"第三\", \"第四\", \"第五\",\n",
    "        \"【\", \"】\", \"（\", \"）\", \"(\", \")\", \"：\", \":\"\n",
    "    ]\n",
    "\n",
    "    for ln in lines:\n",
    "        is_title = False\n",
    "\n",
    "        if any(kw in ln for kw in TITLE_KEYWORDS) and 3 <= len(ln) <= 100:\n",
    "            is_title = True\n",
    "        elif ln.endswith(\"：\") or ln.endswith(\":\") or \\\n",
    "             (len(ln) <= 50 and (ln.startswith(\"【\") and ln.endswith(\"】\"))):\n",
    "            is_title = True\n",
    "        elif 3 <= len(ln) <= 25 and not ln.endswith(\"。\") and not ln.endswith(\".\"):\n",
    "            is_title = True\n",
    "        elif re.match(r\"^[0-9]+[\\.、]\\s*\\S{3,}\", ln):\n",
    "            is_title = True\n",
    "\n",
    "        if is_title:\n",
    "            if buf:\n",
    "                chunk_id = f\"{re.sub(r'[^a-zA-Z0-9]', '_', doc_title)}_{year}_{len(chunks):03d}\"\n",
    "                chunk_meta = {\n",
    "                    \"source_filename\": source_filename,\n",
    "                    \"doc_title\": doc_title,\n",
    "                    \"section_title\": current_title,\n",
    "                    \"authors\": authors,\n",
    "                    \"year\": year,\n",
    "                    \"doc_type\": doc_type,\n",
    "                    \"original_guideline_title\": original_guideline_title,\n",
    "                    \"journal_name\": journal_name,\n",
    "                    \"volume\": volume,\n",
    "                    \"issue\": issue,\n",
    "                    \"pages\": pages,\n",
    "                    \"doi\": doi,\n",
    "                    \"keywords\": keywords,\n",
    "                    \"publish_date\": publish_date,\n",
    "                    \"chunk_id\": chunk_id,\n",
    "                    \"extraction_method\": \"pdfplumber + rule-based + metadata extraction\"\n",
    "                }\n",
    "                chunks.append({\n",
    "                    \"content\": \"\\n\".join(buf),\n",
    "                    \"meta\": chunk_meta\n",
    "                })\n",
    "                buf = []\n",
    "            current_title = ln\n",
    "        else:\n",
    "            buf.append(ln)\n",
    "\n",
    "    # Flush final buffer\n",
    "    if buf:\n",
    "        chunk_id = f\"{re.sub(r'[^a-zA-Z0-9]', '_', doc_title)}_{year}_{len(chunks):03d}\"\n",
    "        chunk_meta = {\n",
    "            \"source_filename\": source_filename,\n",
    "            \"doc_title\": doc_title,\n",
    "            \"section_title\": current_title,\n",
    "            \"authors\": authors,\n",
    "            \"year\": year,\n",
    "            \"doc_type\": doc_type,\n",
    "            \"original_guideline_title\": original_guideline_title,\n",
    "            \"journal_name\": journal_name,\n",
    "            \"volume\": volume,\n",
    "            \"issue\": issue,\n",
    "            \"pages\": pages,\n",
    "            \"doi\": doi,\n",
    "            \"keywords\": keywords,\n",
    "            \"publish_date\": publish_date,\n",
    "            \"chunk_id\": chunk_id,\n",
    "            \"extraction_method\": \"pdfplumber + rule-based + metadata extraction\"\n",
    "        }\n",
    "        chunks.append({\n",
    "            \"content\": \"\\n\".join(buf),\n",
    "            \"meta\": chunk_meta\n",
    "        })\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# %%\n",
    "# =============================\n",
    "# 🚀 5. MAIN PROCESSING PIPELINE\n",
    "# =============================\n",
    "\n",
    "def main():\n",
    "    in_dir = Path(\"data/guidelines\")\n",
    "    out_path = Path(\"data/guidelines.parsed.jsonl\")\n",
    "\n",
    "    print(f\"📂 Input directory: {in_dir.absolute()}\")\n",
    "    pdf_files = list(in_dir.glob(\"*.pdf\"))\n",
    "    print(f\"📄 Found {len(pdf_files)} PDF files.\")\n",
    "\n",
    "    if not pdf_files:\n",
    "        print(\"❌ No PDFs found. Check directory path.\")\n",
    "        return\n",
    "\n",
    "    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for pdf in pdf_files:\n",
    "            print(f\"\\n--- 📄 Processing: {pdf.name} ---\")\n",
    "\n",
    "            # Step 1: Extract preliminary metadata from filename\n",
    "            year = extract_year_from_filename(pdf.name)\n",
    "            doc_title = extract_doc_title(pdf.name)\n",
    "            authors_filename = extract_authors_from_filename(pdf.name)\n",
    "            doc_type = extract_doc_type_from_filename(pdf.name)\n",
    "\n",
    "            print(f\"  📅 Year (from filename): {year}\")\n",
    "            print(f\"  🏷️ Title (from filename): {doc_title}\")\n",
    "            print(f\"  👥 Authors (from filename): {authors_filename}\")\n",
    "            print(f\"  📑 Type (from filename): {doc_type}\")\n",
    "\n",
    "            # Step 2: Extract text\n",
    "            text = extract_text_from_pdf(pdf)\n",
    "            char_count = len(text.strip())\n",
    "            print(f\"  🔤 Extracted {char_count} characters.\")\n",
    "\n",
    "            if char_count == 0:\n",
    "                print(\"  ⚠️  WARNING: No text extracted. File may be scanned.\")\n",
    "                continue\n",
    "\n",
    "            # Step 3: Extract rich metadata from text (overrides filename where possible)\n",
    "            text_meta = extract_metadata_from_text(text)\n",
    "\n",
    "            # Merge: Prefer text-extracted metadata, fallback to filename\n",
    "            authors = text_meta[\"authors\"] if text_meta[\"authors\"] else authors_filename\n",
    "            doc_type_final = text_meta[\"doc_type\"] if text_meta[\"doc_type\"] != \"other\" else doc_type\n",
    "            original_guideline_title = text_meta[\"original_guideline_title\"]\n",
    "            journal_name = text_meta[\"journal_name\"]\n",
    "            volume = text_meta[\"volume\"]\n",
    "            issue = text_meta[\"issue\"]\n",
    "            pages = text_meta[\"pages\"]\n",
    "            doi = text_meta[\"doi\"]\n",
    "            keywords = text_meta[\"keywords\"]\n",
    "            publish_date = text_meta[\"publish_date\"]\n",
    "\n",
    "            print(f\"  ✍️  Authors (final): {authors}\")\n",
    "            print(f\"  📚 Journal: {journal_name} {volume}({issue}), {pages}, {publish_date}\")\n",
    "            print(f\"  🔗 DOI: {doi}\")\n",
    "            print(f\"  🏷️ Keywords: {keywords}\")\n",
    "            print(f\"  🧭 Original Guideline: {original_guideline_title}\")\n",
    "            print(f\"  📑 Doc Type (final): {doc_type_final}\")\n",
    "\n",
    "            # Step 4: Chunk text with full metadata\n",
    "            chunks = chunk_by_rules(\n",
    "                text=text,\n",
    "                source_filename=pdf.name,\n",
    "                year=year,\n",
    "                doc_title=doc_title,\n",
    "                authors=authors,\n",
    "                doc_type=doc_type_final,\n",
    "                original_guideline_title=original_guideline_title,\n",
    "                journal_name=journal_name,\n",
    "                volume=volume,\n",
    "                issue=issue,\n",
    "                pages=pages,\n",
    "                doi=doi,\n",
    "                keywords=keywords,\n",
    "                publish_date=publish_date\n",
    "            )\n",
    "            print(f\"  🧩 Generated {len(chunks)} chunks.\")\n",
    "\n",
    "            if not chunks:\n",
    "                print(\"  ❌ No chunks generated. Check chunking logic.\")\n",
    "\n",
    "            # Step 5: Write to JSONL\n",
    "            for chunk in chunks:\n",
    "                f.write(json.dumps(chunk, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"\\n✅ Output written to: {out_path.absolute()}\")\n",
    "    print(f\"💾 File size: {out_path.stat().st_size} bytes\")\n",
    "\n",
    "# %%\n",
    "# =============================\n",
    "# ▶️ 6. RUN\n",
    "# =============================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e194928",
   "metadata": {},
   "source": [
    "### 向量化与写入 Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e950af01",
   "metadata": {},
   "source": [
    "#### 📑 Embedding Chinese Guideline Chunks into ChromaDB\n",
    "\n",
    "This notebook cell runs a **robust embedding pipeline** designed to convert parsed\n",
    "Chinese medical guideline chunks into vector representations and store them in a\n",
    "**persistent ChromaDB collection** for downstream **RAG (Retrieval-Augmented Generation)**\n",
    "applications.\n",
    "\n",
    "##### 🔍 What this program does\n",
    "- **Stream input**: Reads a JSONL file (`guidelines.parsed.jsonl`), where each line\n",
    "  contains a `content` field (text chunk) and a `meta` field (metadata such as source,\n",
    "  year, section, authors, etc.).\n",
    "- **Embed text**: Uses a **Chinese-capable SentenceTransformer** model\n",
    "  (default: `BAAI/bge-small-zh`) to convert text into dense embeddings.\n",
    "- **Store vectors**: Saves embeddings, documents, and sanitized metadata into a\n",
    "  **ChromaDB collection** on disk for fast similarity search and retrieval.\n",
    "\n",
    "##### ⚙️ Key features and safeguards\n",
    "- **OOM resilience (RTX 4070 friendly)**:\n",
    "  - Dynamic batch-size backoff (halves batch size if VRAM runs out).\n",
    "  - FP16 autocast and lowered max sequence length to reduce memory use.\n",
    "  - Optional CPU fallback if GPU memory is completely exhausted.\n",
    "- **Metadata safety**:\n",
    "  - Converts lists (e.g., `[\"秦煜\"]`) and dicts into scalar or JSON-safe forms.\n",
    "  - Prevents `ValueError: Expected metadata value to be a str, int, float, bool, or None`.\n",
    "- **Duplicate ID protection**:\n",
    "  - Generates **stable, low-collision IDs** from source + chunk_id + content hash.\n",
    "  - Removes duplicate IDs **inside each batch**.\n",
    "  - Uses **upsert** (or add+update fallback) so re-runs are **idempotent**.\n",
    "- **Progress and monitoring**:\n",
    "  - Optional progress bars with `tqdm`.\n",
    "  - Periodic VRAM usage reports (on CUDA).\n",
    "\n",
    "##### 📥 Input format (one JSON object per line)\n",
    "```json\n",
    "{\"content\": \"某段中文文本…\", \"meta\": {\n",
    "  \"source\": \"中国高血压防治指南(2024年修订版).pdf\",\n",
    "  \"year\": 2024,\n",
    "  \"section\": \"2.1 定义\",\n",
    "  \"chunk_id\": 12,\n",
    "  \"authors\": [\"秦煜\",\"张三\"]\n",
    "}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96d6c839",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:   0%|          | 0/5908 [00:00<?, ?it/s]/tmp/ipykernel_25240/2454140094.py:224: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.float16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Persist dir: ./chroma_store\n",
      "🗃️  Collection: guideline_chunks\n",
      "🧠 Model: BAAI/bge-large-zh-v1.5 | Device: cuda | fp16: True | max_seq_len: 384\n",
      "📑 Input: /home/myunix/caremind/data/guidelines.parsed.jsonl\n",
      "⚙️  Start batch size: 16 | CPU fallback: True\n",
      "[VRAM] start used=2.42 GB / total=11.99 GB (free=9.57 GB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:   1%|          | 32/5908 [00:01<02:42, 36.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=16)\n",
      "✅ Flushed 16 items (total=32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:   1%|          | 64/5908 [00:01<01:33, 62.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "✅ Flushed 15 items (total=47)\n",
      "[dedupe] removed 4 duplicate id(s) in the same batch)\n",
      "✅ Flushed 12 items (total=59)\n",
      "[dedupe] removed 3 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:   2%|▏         | 96/5908 [00:01<01:04, 89.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 13 items (total=72)\n",
      "[dedupe] removed 3 duplicate id(s) in the same batch)\n",
      "✅ Flushed 13 items (total=85)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:   2%|▏         | 112/5908 [00:01<00:57, 100.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=101)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:   2%|▏         | 144/5908 [00:02<01:01, 93.71it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=117)\n",
      "✅ Flushed 16 items (total=133)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:   3%|▎         | 176/5908 [00:02<00:52, 109.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 14 items (total=147)\n",
      "✅ Flushed 16 items (total=163)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:   4%|▎         | 208/5908 [00:02<00:49, 115.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=179)\n",
      "✅ Flushed 16 items (total=195)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:   4%|▍         | 240/5908 [00:02<00:51, 110.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n",
      "✅ Flushed 14 items (total=209)\n",
      "✅ Flushed 16 items (total=225)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:   5%|▍         | 272/5908 [00:03<00:50, 112.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "✅ Flushed 15 items (total=240)\n",
      "✅ Flushed 16 items (total=256)\n",
      "[VRAM] after 256 used=2.49 GB / total=11.99 GB (free=9.50 GB)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:   5%|▌         | 304/5908 [00:03<00:47, 119.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 15 items (total=271)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "✅ Flushed 15 items (total=286)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:   6%|▌         | 336/5908 [00:03<00:45, 122.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 14 items (total=300)\n",
      "[dedupe] removed 7 duplicate id(s) in the same batch)\n",
      "✅ Flushed 9 items (total=309)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:   6%|▌         | 368/5908 [00:03<00:47, 117.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 15 items (total=324)\n",
      "✅ Flushed 16 items (total=340)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:   7%|▋         | 400/5908 [00:04<00:46, 117.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 15 items (total=355)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "✅ Flushed 15 items (total=370)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:   7%|▋         | 432/5908 [00:04<00:46, 116.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=386)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "✅ Flushed 15 items (total=401)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:   8%|▊         | 464/5908 [00:04<00:47, 114.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=417)\n",
      "✅ Flushed 16 items (total=433)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:   8%|▊         | 496/5908 [00:05<00:51, 105.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=449)\n",
      "✅ Flushed 16 items (total=465)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:   9%|▉         | 528/5908 [00:05<00:56, 95.67it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=481)\n",
      "✅ Flushed 16 items (total=497)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:   9%|▉         | 560/5908 [00:05<00:52, 101.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "✅ Flushed 15 items (total=512)\n",
      "[VRAM] after 512 used=2.49 GB / total=11.99 GB (free=9.50 GB)\n",
      "✅ Flushed 16 items (total=528)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  10%|▉         | 576/5908 [00:05<00:52, 101.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "✅ Flushed 15 items (total=543)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  10%|█         | 608/5908 [00:06<00:59, 89.38it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 15 items (total=558)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "✅ Flushed 15 items (total=573)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  11%|█         | 640/5908 [00:06<01:02, 83.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n",
      "✅ Flushed 14 items (total=587)\n",
      "[dedupe] removed 3 duplicate id(s) in the same batch)\n",
      "✅ Flushed 13 items (total=600)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  11%|█         | 656/5908 [00:06<00:59, 87.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 4 duplicate id(s) in the same batch)\n",
      "✅ Flushed 12 items (total=612)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  12%|█▏        | 688/5908 [00:07<01:02, 83.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 14 items (total=626)\n",
      "✅ Flushed 16 items (total=642)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  12%|█▏        | 720/5908 [00:07<01:04, 80.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n",
      "✅ Flushed 14 items (total=656)\n",
      "✅ Flushed 16 items (total=672)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  13%|█▎        | 752/5908 [00:08<01:03, 81.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=688)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "✅ Flushed 15 items (total=703)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  13%|█▎        | 784/5908 [00:08<01:03, 80.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=719)\n",
      "[dedupe] removed 3 duplicate id(s) in the same batch)\n",
      "✅ Flushed 13 items (total=732)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  14%|█▎        | 800/5908 [00:08<01:03, 81.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=748)\n",
      "[dedupe] removed 5 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  14%|█▍        | 832/5908 [00:09<01:02, 80.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 11 items (total=759)\n",
      "✅ Flushed 16 items (total=775)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  15%|█▍        | 864/5908 [00:09<01:04, 78.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=791)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "✅ Flushed 15 items (total=806)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  15%|█▍        | 880/5908 [00:09<01:05, 77.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=822)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  15%|█▌        | 896/5908 [00:10<01:07, 73.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 14 items (total=836)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  15%|█▌        | 912/5908 [00:10<01:10, 70.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 15 items (total=851)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  16%|█▌        | 944/5908 [00:10<01:08, 72.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 15 items (total=866)\n",
      "✅ Flushed 16 items (total=882)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  17%|█▋        | 976/5908 [00:11<01:06, 73.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 3 duplicate id(s) in the same batch)\n",
      "✅ Flushed 13 items (total=895)\n",
      "✅ Flushed 16 items (total=911)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  17%|█▋        | 992/5908 [00:11<01:04, 76.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=927)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  17%|█▋        | 1008/5908 [00:11<01:03, 77.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=943)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  17%|█▋        | 1024/5908 [00:11<01:04, 75.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 15 items (total=958)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  18%|█▊        | 1056/5908 [00:12<01:03, 76.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 14 items (total=972)\n",
      "[dedupe] removed 4 duplicate id(s) in the same batch)\n",
      "✅ Flushed 12 items (total=984)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  18%|█▊        | 1088/5908 [00:12<00:52, 92.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=1000)\n",
      "✅ Flushed 16 items (total=1016)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  19%|█▉        | 1120/5908 [00:12<00:47, 101.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=1032)\n",
      "[dedupe] removed 4 duplicate id(s) in the same batch)\n",
      "✅ Flushed 12 items (total=1044)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  19%|█▉        | 1152/5908 [00:13<00:43, 108.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=1060)\n",
      "✅ Flushed 16 items (total=1076)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  20%|██        | 1184/5908 [00:13<00:48, 98.04it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=1092)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n",
      "✅ Flushed 14 items (total=1106)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  21%|██        | 1216/5908 [00:13<00:50, 93.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 4 duplicate id(s) in the same batch)\n",
      "✅ Flushed 12 items (total=1118)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "✅ Flushed 15 items (total=1133)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  21%|██        | 1248/5908 [00:14<00:46, 100.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n",
      "✅ Flushed 14 items (total=1147)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "✅ Flushed 15 items (total=1162)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  22%|██▏       | 1280/5908 [00:14<00:45, 101.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "✅ Flushed 15 items (total=1177)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "✅ Flushed 15 items (total=1192)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  22%|██▏       | 1312/5908 [00:14<00:43, 105.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n",
      "✅ Flushed 14 items (total=1206)\n",
      "[dedupe] removed 4 duplicate id(s) in the same batch)\n",
      "✅ Flushed 12 items (total=1218)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  23%|██▎       | 1344/5908 [00:15<00:43, 104.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 6 duplicate id(s) in the same batch)\n",
      "✅ Flushed 10 items (total=1228)\n",
      "✅ Flushed 16 items (total=1244)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  23%|██▎       | 1376/5908 [00:15<00:43, 103.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n",
      "✅ Flushed 14 items (total=1258)\n",
      "✅ Flushed 16 items (total=1274)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  24%|██▍       | 1408/5908 [00:15<00:45, 99.19it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 14 items (total=1288)\n",
      "✅ Flushed 16 items (total=1304)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  24%|██▍       | 1440/5908 [00:16<00:42, 104.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 3 duplicate id(s) in the same batch)\n",
      "✅ Flushed 13 items (total=1317)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n",
      "✅ Flushed 14 items (total=1331)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  25%|██▍       | 1472/5908 [00:16<00:44, 98.78it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 5 duplicate id(s) in the same batch)\n",
      "✅ Flushed 11 items (total=1342)\n",
      "[dedupe] removed 3 duplicate id(s) in the same batch)\n",
      "✅ Flushed 13 items (total=1355)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  25%|██▌       | 1488/5908 [00:16<00:41, 105.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=1371)\n",
      "[dedupe] removed 3 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  26%|██▌       | 1520/5908 [00:16<00:43, 101.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 13 items (total=1384)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "✅ Flushed 15 items (total=1399)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  26%|██▋       | 1552/5908 [00:17<00:45, 95.78it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=1415)\n",
      "[dedupe] removed 3 duplicate id(s) in the same batch)\n",
      "✅ Flushed 13 items (total=1428)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  27%|██▋       | 1584/5908 [00:17<00:48, 89.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 3 duplicate id(s) in the same batch)\n",
      "✅ Flushed 13 items (total=1441)\n",
      "✅ Flushed 16 items (total=1457)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  27%|██▋       | 1616/5908 [00:17<00:48, 88.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=1473)\n",
      "✅ Flushed 16 items (total=1489)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  28%|██▊       | 1648/5908 [00:18<00:45, 93.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "✅ Flushed 15 items (total=1504)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "✅ Flushed 15 items (total=1519)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  28%|██▊       | 1664/5908 [00:18<00:42, 99.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n",
      "✅ Flushed 14 items (total=1533)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  29%|██▊       | 1696/5908 [00:18<00:44, 94.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 15 items (total=1548)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n",
      "✅ Flushed 14 items (total=1562)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  29%|██▉       | 1712/5908 [00:18<00:44, 93.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "✅ Flushed 15 items (total=1577)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  30%|██▉       | 1744/5908 [00:19<00:46, 90.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=1593)\n",
      "✅ Flushed 16 items (total=1609)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  30%|███       | 1776/5908 [00:19<00:42, 97.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=1625)\n",
      "✅ Flushed 16 items (total=1641)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  31%|███       | 1808/5908 [00:19<00:41, 99.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=1657)\n",
      "✅ Flushed 16 items (total=1673)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  31%|███       | 1840/5908 [00:20<00:41, 96.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=1689)\n",
      "✅ Flushed 16 items (total=1705)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  32%|███▏      | 1872/5908 [00:20<00:44, 90.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=1721)\n",
      "✅ Flushed 16 items (total=1737)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  32%|███▏      | 1888/5908 [00:20<00:44, 90.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=1753)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  32%|███▏      | 1904/5908 [00:21<00:49, 81.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=1769)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  32%|███▏      | 1920/5908 [00:21<00:49, 80.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=1785)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  32%|███▏      | 1920/5908 [00:20<00:49, 80.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=1801)\n",
      "✅ Flushed 16 items (total=1817)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  32%|███▏      | 1920/5908 [00:21<00:49, 80.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=1833)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  34%|███▍      | 2000/5908 [00:21<00:23, 163.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=1849)\n",
      "✅ Flushed 16 items (total=1865)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  34%|███▍      | 2032/5908 [00:21<00:32, 120.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=1881)\n",
      "✅ Flushed 16 items (total=1897)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  35%|███▍      | 2048/5908 [00:22<00:37, 103.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=1913)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  35%|███▌      | 2080/5908 [00:22<00:43, 88.80it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=1929)\n",
      "✅ Flushed 16 items (total=1945)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  35%|███▌      | 2096/5908 [00:22<00:43, 86.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=1961)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  36%|███▌      | 2128/5908 [00:23<00:44, 85.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=1977)\n",
      "✅ Flushed 16 items (total=1993)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  37%|███▋      | 2160/5908 [00:23<00:34, 107.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=2009)\n",
      "✅ Flushed 16 items (total=2025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  37%|███▋      | 2192/5908 [00:23<00:30, 120.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=2041)\n",
      "✅ Flushed 16 items (total=2057)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  38%|███▊      | 2224/5908 [00:23<00:30, 119.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=2073)\n",
      "✅ Flushed 16 items (total=2089)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  38%|███▊      | 2256/5908 [00:24<00:35, 103.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=2105)\n",
      "✅ Flushed 16 items (total=2121)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  39%|███▊      | 2288/5908 [00:24<00:30, 120.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=2137)\n",
      "✅ Flushed 16 items (total=2153)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  39%|███▉      | 2320/5908 [00:24<00:28, 126.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=2169)\n",
      "✅ Flushed 16 items (total=2185)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  40%|███▉      | 2352/5908 [00:24<00:28, 124.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=2201)\n",
      "✅ Flushed 16 items (total=2217)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  40%|████      | 2384/5908 [00:25<00:26, 131.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=2233)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n",
      "✅ Flushed 14 items (total=2247)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  41%|████      | 2416/5908 [00:25<00:27, 127.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=2263)\n",
      "✅ Flushed 16 items (total=2279)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  41%|████▏     | 2448/5908 [00:25<00:27, 127.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 14 items (total=2293)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "✅ Flushed 15 items (total=2308)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  42%|████▏     | 2480/5908 [00:25<00:26, 128.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=2324)\n",
      "✅ Flushed 16 items (total=2340)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  43%|████▎     | 2512/5908 [00:26<00:27, 125.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=2356)\n",
      "✅ Flushed 16 items (total=2372)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  43%|████▎     | 2544/5908 [00:26<00:27, 123.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=2388)\n",
      "✅ Flushed 16 items (total=2404)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  44%|████▎     | 2576/5908 [00:26<00:27, 121.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=2420)\n",
      "✅ Flushed 16 items (total=2436)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  44%|████▍     | 2608/5908 [00:27<00:27, 119.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=2452)\n",
      "✅ Flushed 16 items (total=2468)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  45%|████▍     | 2640/5908 [00:27<00:27, 117.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=2484)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "✅ Flushed 15 items (total=2499)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  45%|████▌     | 2672/5908 [00:27<00:29, 110.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=2515)\n",
      "✅ Flushed 16 items (total=2531)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  45%|████▌     | 2688/5908 [00:27<00:30, 105.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=2547)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  46%|████▌     | 2720/5908 [00:28<00:35, 91.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=2563)\n",
      "✅ Flushed 16 items (total=2579)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  46%|████▋     | 2736/5908 [00:28<00:35, 88.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=2595)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  47%|████▋     | 2752/5908 [00:28<00:36, 85.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=2611)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  47%|████▋     | 2784/5908 [00:28<00:37, 82.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=2627)\n",
      "✅ Flushed 16 items (total=2643)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  48%|████▊     | 2816/5908 [00:29<00:37, 82.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=2659)\n",
      "✅ Flushed 16 items (total=2675)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  48%|████▊     | 2832/5908 [00:29<00:37, 82.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=2691)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  48%|████▊     | 2848/5908 [00:29<00:37, 80.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=2707)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  48%|████▊     | 2864/5908 [00:30<00:46, 64.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=2723)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  49%|████▊     | 2880/5908 [00:30<00:49, 61.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=2739)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  49%|████▉     | 2896/5908 [00:30<00:52, 57.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 15 items (total=2754)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  49%|████▉     | 2912/5908 [00:31<00:54, 54.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=2770)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  50%|████▉     | 2928/5908 [00:31<00:53, 55.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=2786)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  50%|████▉     | 2944/5908 [00:31<00:52, 56.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=2802)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  50%|█████     | 2960/5908 [00:31<00:49, 58.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 14 items (total=2816)\n",
      "[VRAM] after 2816 used=2.49 GB / total=11.99 GB (free=9.50 GB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  50%|█████     | 2976/5908 [00:32<00:49, 59.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=2832)\n",
      "[dedupe] removed 3 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  51%|█████     | 2992/5908 [00:32<00:46, 62.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 13 items (total=2845)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  51%|█████     | 3008/5908 [00:32<00:46, 62.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 15 items (total=2860)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  51%|█████     | 3024/5908 [00:32<00:47, 61.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 15 items (total=2875)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  51%|█████▏    | 3040/5908 [00:33<00:47, 59.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 14 items (total=2889)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  52%|█████▏    | 3056/5908 [00:33<00:50, 57.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 14 items (total=2903)\n",
      "[dedupe] removed 6 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  52%|█████▏    | 3072/5908 [00:33<00:49, 57.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 10 items (total=2913)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  52%|█████▏    | 3088/5908 [00:34<00:49, 56.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 15 items (total=2928)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  53%|█████▎    | 3104/5908 [00:34<01:01, 45.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=2944)\n",
      "[VRAM] after 2944 used=2.49 GB / total=11.99 GB (free=9.50 GB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  53%|█████▎    | 3120/5908 [00:34<00:58, 48.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=2960)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  53%|█████▎    | 3136/5908 [00:35<00:56, 49.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=2976)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  53%|█████▎    | 3152/5908 [00:35<01:02, 44.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=2992)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  54%|█████▎    | 3168/5908 [00:35<00:57, 47.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=3008)\n",
      "[VRAM] after 3008 used=2.49 GB / total=11.99 GB (free=9.50 GB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  54%|█████▍    | 3184/5908 [00:36<00:51, 52.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=3024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  54%|█████▍    | 3200/5908 [00:36<00:46, 58.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=3040)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  55%|█████▍    | 3232/5908 [00:36<00:40, 66.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=3056)\n",
      "✅ Flushed 16 items (total=3072)\n",
      "[VRAM] after 3072 used=2.49 GB / total=11.99 GB (free=9.50 GB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  55%|█████▍    | 3248/5908 [00:36<00:40, 65.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=3088)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  55%|█████▌    | 3264/5908 [00:37<00:38, 67.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=3104)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  56%|█████▌    | 3296/5908 [00:37<00:35, 72.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 14 items (total=3118)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "✅ Flushed 15 items (total=3133)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  56%|█████▌    | 3312/5908 [00:37<00:36, 70.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "✅ Flushed 15 items (total=3148)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  57%|█████▋    | 3344/5908 [00:38<00:34, 73.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=3164)\n",
      "✅ Flushed 16 items (total=3180)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  57%|█████▋    | 3360/5908 [00:38<00:34, 72.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 3 duplicate id(s) in the same batch)\n",
      "✅ Flushed 13 items (total=3193)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  57%|█████▋    | 3376/5908 [00:38<00:33, 74.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=3209)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  57%|█████▋    | 3392/5908 [00:38<00:33, 74.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=3225)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  58%|█████▊    | 3408/5908 [00:39<00:33, 73.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=3241)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  58%|█████▊    | 3424/5908 [00:39<00:35, 70.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=3257)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  58%|█████▊    | 3440/5908 [00:39<00:37, 66.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 15 items (total=3272)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  58%|█████▊    | 3456/5908 [00:39<00:37, 64.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=3288)\n",
      "[dedupe] removed 3 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  59%|█████▉    | 3472/5908 [00:40<00:35, 67.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 13 items (total=3301)\n",
      "[dedupe] removed 3 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  59%|█████▉    | 3488/5908 [00:40<00:34, 70.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 13 items (total=3314)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  59%|█████▉    | 3504/5908 [00:40<00:34, 70.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=3330)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  60%|█████▉    | 3520/5908 [00:40<00:33, 71.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 15 items (total=3345)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  60%|█████▉    | 3536/5908 [00:41<00:33, 69.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=3361)\n",
      "[dedupe] removed 3 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  60%|██████    | 3552/5908 [00:41<00:33, 69.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 13 items (total=3374)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  60%|██████    | 3568/5908 [00:41<00:34, 68.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 14 items (total=3388)\n",
      "[dedupe] removed 7 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  61%|██████    | 3584/5908 [00:41<00:32, 70.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 9 items (total=3397)\n",
      "[dedupe] removed 8 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  61%|██████    | 3600/5908 [00:41<00:31, 73.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 8 items (total=3405)\n",
      "[dedupe] removed 4 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  61%|██████    | 3616/5908 [00:42<00:30, 74.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 12 items (total=3417)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  61%|██████▏   | 3632/5908 [00:42<00:31, 72.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 15 items (total=3432)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  62%|██████▏   | 3648/5908 [00:42<00:33, 68.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=3448)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  62%|██████▏   | 3664/5908 [00:42<00:33, 66.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 15 items (total=3463)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  62%|██████▏   | 3680/5908 [00:43<00:32, 67.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 14 items (total=3477)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  63%|██████▎   | 3696/5908 [00:43<00:32, 68.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=3493)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  63%|██████▎   | 3712/5908 [00:43<00:31, 69.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=3509)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  63%|██████▎   | 3728/5908 [00:43<00:31, 69.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 14 items (total=3523)\n",
      "[dedupe] removed 5 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  63%|██████▎   | 3744/5908 [00:43<00:31, 69.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 11 items (total=3534)\n",
      "[dedupe] removed 4 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  64%|██████▎   | 3760/5908 [00:44<00:31, 69.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 12 items (total=3546)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  64%|██████▍   | 3776/5908 [00:44<00:31, 68.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=3562)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  64%|██████▍   | 3792/5908 [00:44<00:31, 67.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=3578)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  64%|██████▍   | 3808/5908 [00:44<00:33, 63.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=3594)\n",
      "[dedupe] removed 3 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  65%|██████▍   | 3840/5908 [00:45<00:27, 74.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 13 items (total=3607)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "✅ Flushed 15 items (total=3622)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  66%|██████▌   | 3872/5908 [00:45<00:24, 83.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "✅ Flushed 15 items (total=3637)\n",
      "✅ Flushed 16 items (total=3653)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  66%|██████▌   | 3904/5908 [00:46<00:22, 89.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=3669)\n",
      "✅ Flushed 16 items (total=3685)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  66%|██████▋   | 3920/5908 [00:46<00:22, 89.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=3701)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  67%|██████▋   | 3936/5908 [00:46<00:36, 54.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=3717)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  67%|██████▋   | 3952/5908 [00:47<00:34, 56.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 14 items (total=3731)\n",
      "[dedupe] removed 3 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  67%|██████▋   | 3968/5908 [00:47<00:32, 60.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 13 items (total=3744)\n",
      "[dedupe] removed 3 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  67%|██████▋   | 3984/5908 [00:47<00:33, 57.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 13 items (total=3757)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  68%|██████▊   | 4000/5908 [00:47<00:32, 58.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 14 items (total=3771)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  68%|██████▊   | 4016/5908 [00:48<00:32, 58.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 15 items (total=3786)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  68%|██████▊   | 4032/5908 [00:48<00:30, 62.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 15 items (total=3801)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  69%|██████▊   | 4048/5908 [00:48<00:28, 64.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=3817)\n",
      "[dedupe] removed 4 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  69%|██████▉   | 4064/5908 [00:48<00:27, 67.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 12 items (total=3829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  69%|██████▉   | 4080/5908 [00:49<00:27, 65.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=3845)\n",
      "[dedupe] removed 5 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  69%|██████▉   | 4096/5908 [00:49<00:27, 66.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 11 items (total=3856)\n",
      "[dedupe] removed 6 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  70%|██████▉   | 4112/5908 [00:49<00:28, 63.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 10 items (total=3866)\n",
      "[dedupe] removed 7 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  70%|██████▉   | 4128/5908 [00:49<00:27, 64.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 9 items (total=3875)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  70%|███████   | 4144/5908 [00:49<00:26, 66.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 14 items (total=3889)\n",
      "[dedupe] removed 5 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  70%|███████   | 4160/5908 [00:50<00:25, 68.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 11 items (total=3900)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  71%|███████   | 4176/5908 [00:50<00:24, 70.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 14 items (total=3914)\n",
      "[dedupe] removed 3 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  71%|███████   | 4192/5908 [00:50<00:24, 69.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 13 items (total=3927)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  71%|███████   | 4208/5908 [00:50<00:25, 67.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 15 items (total=3942)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  71%|███████▏  | 4224/5908 [00:51<00:24, 70.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 14 items (total=3956)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  72%|███████▏  | 4240/5908 [00:51<00:24, 69.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=3972)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  72%|███████▏  | 4256/5908 [00:51<00:23, 68.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 15 items (total=3987)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  72%|███████▏  | 4272/5908 [00:52<00:29, 55.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 14 items (total=4001)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  73%|███████▎  | 4304/5908 [00:52<00:23, 69.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 14 items (total=4015)\n",
      "✅ Flushed 16 items (total=4031)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  73%|███████▎  | 4336/5908 [00:52<00:19, 82.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "✅ Flushed 15 items (total=4046)\n",
      "✅ Flushed 16 items (total=4062)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  74%|███████▎  | 4352/5908 [00:52<00:18, 85.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n",
      "✅ Flushed 14 items (total=4076)\n",
      "✅ Flushed 16 items (total=4092)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  74%|███████▎  | 4352/5908 [00:52<00:18, 85.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "✅ Flushed 15 items (total=4107)\n",
      "[dedupe] removed 5 duplicate id(s) in the same batch)\n",
      "✅ Flushed 11 items (total=4118)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  74%|███████▎  | 4352/5908 [00:52<00:18, 85.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 14 items (total=4132)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n",
      "✅ Flushed 14 items (total=4146)\n",
      "[dedupe] removed 6 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  76%|███████▌  | 4464/5908 [00:53<00:05, 249.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 10 items (total=4156)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "✅ Flushed 15 items (total=4171)\n",
      "[dedupe] removed 4 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  76%|███████▌  | 4496/5908 [00:53<00:06, 215.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 12 items (total=4183)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "✅ Flushed 15 items (total=4198)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  77%|███████▋  | 4543/5908 [00:53<00:06, 210.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=4214)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "✅ Flushed 15 items (total=4229)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  77%|███████▋  | 4565/5908 [00:53<00:08, 162.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=4245)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n",
      "✅ Flushed 14 items (total=4259)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  78%|███████▊  | 4602/5908 [00:53<00:08, 159.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 15 items (total=4274)\n",
      "[dedupe] removed 5 duplicate id(s) in the same batch)\n",
      "✅ Flushed 11 items (total=4285)\n",
      "[dedupe] removed 4 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  78%|███████▊  | 4636/5908 [00:54<00:08, 151.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 12 items (total=4297)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "✅ Flushed 15 items (total=4312)\n",
      "[dedupe] removed 4 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  79%|███████▉  | 4668/5908 [00:54<00:08, 146.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 12 items (total=4324)\n",
      "[dedupe] removed 3 duplicate id(s) in the same batch)\n",
      "✅ Flushed 13 items (total=4337)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  80%|███████▉  | 4697/5908 [00:54<00:09, 123.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 14 items (total=4351)\n",
      "✅ Flushed 16 items (total=4367)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  80%|███████▉  | 4722/5908 [00:54<00:11, 103.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=4383)\n",
      "✅ Flushed 16 items (total=4399)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  80%|████████  | 4752/5908 [00:55<00:10, 106.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=4415)\n",
      "✅ Flushed 16 items (total=4431)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  81%|████████  | 4784/5908 [00:55<00:10, 107.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=4447)\n",
      "✅ Flushed 16 items (total=4463)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  82%|████████▏ | 4816/5908 [00:55<00:09, 112.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=4479)\n",
      "✅ Flushed 16 items (total=4495)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  82%|████████▏ | 4848/5908 [00:56<00:09, 116.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=4511)\n",
      "✅ Flushed 16 items (total=4527)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  83%|████████▎ | 4880/5908 [00:56<00:09, 107.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=4543)\n",
      "✅ Flushed 16 items (total=4559)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  83%|████████▎ | 4912/5908 [00:56<00:10, 95.11it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=4575)\n",
      "✅ Flushed 16 items (total=4591)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  84%|████████▎ | 4944/5908 [00:57<00:09, 99.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=4607)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "✅ Flushed 15 items (total=4622)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  84%|████████▍ | 4976/5908 [00:57<00:08, 105.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 3 duplicate id(s) in the same batch)\n",
      "✅ Flushed 13 items (total=4635)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "✅ Flushed 15 items (total=4650)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  85%|████████▍ | 5008/5908 [00:57<00:08, 110.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=4666)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "✅ Flushed 15 items (total=4681)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  85%|████████▌ | 5040/5908 [00:57<00:07, 112.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=4697)\n",
      "✅ Flushed 16 items (total=4713)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  86%|████████▌ | 5072/5908 [00:58<00:08, 103.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=4729)\n",
      "✅ Flushed 16 items (total=4745)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  86%|████████▋ | 5104/5908 [00:58<00:08, 94.34it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=4761)\n",
      "✅ Flushed 16 items (total=4777)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  87%|████████▋ | 5136/5908 [00:59<00:08, 86.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=4793)\n",
      "✅ Flushed 16 items (total=4809)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  87%|████████▋ | 5152/5908 [00:59<00:08, 88.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=4825)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  88%|████████▊ | 5184/5908 [00:59<00:08, 83.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=4841)\n",
      "✅ Flushed 16 items (total=4857)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  88%|████████▊ | 5216/5908 [00:59<00:07, 87.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=4873)\n",
      "✅ Flushed 16 items (total=4889)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  89%|████████▉ | 5248/5908 [01:00<00:07, 86.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=4905)\n",
      "✅ Flushed 16 items (total=4921)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  89%|████████▉ | 5264/5908 [01:00<00:07, 87.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=4937)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  90%|████████▉ | 5296/5908 [01:00<00:07, 81.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=4953)\n",
      "✅ Flushed 16 items (total=4969)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  90%|████████▉ | 5312/5908 [01:01<00:07, 85.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=4985)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  90%|█████████ | 5344/5908 [01:01<00:06, 87.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=5001)\n",
      "✅ Flushed 16 items (total=5017)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  91%|█████████ | 5376/5908 [01:01<00:05, 104.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=5033)\n",
      "✅ Flushed 16 items (total=5049)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  92%|█████████▏| 5408/5908 [01:01<00:04, 120.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=5065)\n",
      "✅ Flushed 16 items (total=5081)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  92%|█████████▏| 5440/5908 [01:02<00:03, 120.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=5097)\n",
      "✅ Flushed 16 items (total=5113)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  93%|█████████▎| 5472/5908 [01:02<00:03, 124.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=5129)\n",
      "✅ Flushed 16 items (total=5145)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  93%|█████████▎| 5504/5908 [01:02<00:03, 115.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=5161)\n",
      "✅ Flushed 16 items (total=5177)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  94%|█████████▎| 5536/5908 [01:02<00:02, 128.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n",
      "✅ Flushed 14 items (total=5191)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n",
      "✅ Flushed 14 items (total=5205)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  94%|█████████▍| 5568/5908 [01:03<00:02, 138.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 14 items (total=5219)\n",
      "✅ Flushed 16 items (total=5235)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  95%|█████████▍| 5600/5908 [01:03<00:02, 141.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=5251)\n",
      "[dedupe] removed 5 duplicate id(s) in the same batch)\n",
      "✅ Flushed 11 items (total=5262)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  95%|█████████▌| 5632/5908 [01:03<00:01, 141.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 15 items (total=5277)\n",
      "[dedupe] removed 3 duplicate id(s) in the same batch)\n",
      "✅ Flushed 13 items (total=5290)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  96%|█████████▌| 5664/5908 [01:03<00:01, 138.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=5306)\n",
      "✅ Flushed 16 items (total=5322)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  96%|█████████▋| 5696/5908 [01:04<00:01, 133.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 14 items (total=5336)\n",
      "✅ Flushed 16 items (total=5352)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  97%|█████████▋| 5728/5908 [01:04<00:01, 115.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=5368)\n",
      "✅ Flushed 16 items (total=5384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  97%|█████████▋| 5760/5908 [01:04<00:01, 107.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=5400)\n",
      "✅ Flushed 16 items (total=5416)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  98%|█████████▊| 5792/5908 [01:05<00:01, 99.25it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=5432)\n",
      "✅ Flushed 16 items (total=5448)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  99%|█████████▊| 5824/5908 [01:05<00:00, 108.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "✅ Flushed 15 items (total=5463)\n",
      "✅ Flushed 16 items (total=5479)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=5495)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  99%|█████████▉| 5872/5908 [01:05<00:00, 88.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=5511)\n",
      "✅ Flushed 16 items (total=5527)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL: 100%|██████████| 5908/5908 [01:06<00:00, 89.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flushed 16 items (total=5543)\n",
      "✅ Flushed 16 items (total=5559)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "✅ Flushed 3 items (total=5562)\n",
      "\n",
      "🎉 Done. Inserted 5562 chunks into 'guideline_chunks' at './chroma_store'.\n",
      "Model: BAAI/bge-large-zh-v1.5 | Device: cuda | Start batch: 16 | max_seq_len: 384\n",
      "[VRAM] end used=2.49 GB / total=11.99 GB (free=9.50 GB)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "===============================================================================\n",
    "Embed Chinese guideline chunks → ChromaDB (OOM-safe, idempotent, metadata-robust)\n",
    "===============================================================================\n",
    "\n",
    "What this script does (high-level)\n",
    "----------------------------------\n",
    "1) Streams a JSONL file that contains one chunk per line:\n",
    "     {\n",
    "       \"content\": \"分章文本……\",\n",
    "       \"meta\": {\n",
    "         \"source\": \"中国高血压防治指南(2024年修订版).pdf\",\n",
    "         \"year\": 2024,\n",
    "         \"section\": \"3.2 降压目标\",\n",
    "         \"chunk_id\": 57,\n",
    "         \"authors\": [\"秦煜\", \"张三\"]    # lists allowed; we sanitize below\n",
    "       }\n",
    "     }\n",
    "\n",
    "2) Uses a Chinese-capable SentenceTransformer (defaults to BAAI/bge-small-zh)\n",
    "   to embed \"content\" with **GPU if available**.\n",
    "\n",
    "3) Writes embeddings, documents, and **sanitized metadata** into a persistent\n",
    "   **Chroma** collection on disk, in an **idempotent** way:\n",
    "   - Uses **upsert** when available (Chromadb>=0.5) to overwrite duplicates.\n",
    "   - Strengthens IDs to minimize collisions.\n",
    "   - De-duplicates IDs **inside each batch**.\n",
    "   - Final safety net: per-item upsert/repair if a batch raises an error.\n",
    "\n",
    "Inputs (files & environment variables)\n",
    "-------------------------------------\n",
    "• JSONL file (default `data/guidelines.parsed.jsonl`)\n",
    "  - Set via env var `CAREMIND_DATA`.\n",
    "\n",
    "• Environment variables (optional, with sensible defaults):\n",
    "  CHROMA_PERSIST_DIR  : Directory for Chroma persistence (default './chroma_store')\n",
    "  CHROMA_COLLECTION   : Collection name (default 'guideline_chunks')\n",
    "  CAREMIND_DATA       : Input JSONL path (default 'data/guidelines.parsed.jsonl')\n",
    "  EMBEDDING_MODEL     : Model id (default 'BAAI/bge-small-zh')\n",
    "  EMBED_BATCH_SIZE    : Starting batch size (default '16')\n",
    "  EMBED_FP16          : '1' to allow fp16 autocast on CUDA (default '1')\n",
    "  EMBED_PROGRESS      : '1' to show progress bars (default '1')\n",
    "  EMBED_MAX_LEN       : Max sequence length for encoder (default '384')\n",
    "  OOM_CPU_FALLBACK    : '1' to allow CPU fallback when BS=1 still OOM (default '1')\n",
    "\n",
    "Outputs\n",
    "-------\n",
    "• A persistent Chroma database on disk containing:\n",
    "    - ids: stable, low-collision per-chunk IDs\n",
    "    - embeddings: float vectors\n",
    "    - documents: original text chunks\n",
    "    - metadatas: scalar/JSON-safe metadata\n",
    "  Location and collection are controlled by CHROMA_PERSIST_DIR / CHROMA_COLLECTION.\n",
    "\n",
    "Run\n",
    "---\n",
    "$ python embed_to_chroma.py\n",
    "(or set envs first, e.g., EMBED_MAX_LEN=256 EMBED_BATCH_SIZE=8 for tighter VRAM)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List, Dict, Any\n",
    "\n",
    "# NOTE: We intentionally do NOT set PYTORCH_CUDA_ALLOC_CONF here because some\n",
    "# PyTorch builds require a specific format and may crash. All OOM tactics below\n",
    "# (batch backoff, fp16, truncation, cache clears, CPU fallback) work without it.\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from chromadb import PersistentClient\n",
    "from chromadb import errors as chroma_errors  # for DuplicateIDError handling\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------------------------\n",
    "# Config (safe defaults; override via env vars)\n",
    "# ---------------------------\n",
    "PERSIST_DIR      = os.getenv(\"CHROMA_PERSIST_DIR\", \"./chroma_store\")\n",
    "COLLECTION_NAME  = os.getenv(\"CHROMA_COLLECTION\", \"guideline_chunks\")\n",
    "DATA_PATH        = os.getenv(\"CAREMIND_DATA\", \"data/guidelines.parsed.jsonl\")\n",
    "\n",
    "# Chinese-capable model; bge-* are strong + efficient. Start small on 12GB VRAM.\n",
    "EMBEDDING_MODEL  = os.getenv(\"EMBEDDING_MODEL\", \"BAAI/bge-small-zh\")\n",
    "\n",
    "# Start conservatively; dynamic backoff will reduce further on OOM.\n",
    "START_BATCH_SIZE = int(os.getenv(\"EMBED_BATCH_SIZE\", \"16\"))\n",
    "\n",
    "# fp16 autocast reduces memory on CUDA and is safe for sentence-transformers inference\n",
    "USE_FP16         = os.getenv(\"EMBED_FP16\", \"1\") == \"1\"\n",
    "\n",
    "# Lower seq length cuts memory on long Chinese paragraphs; 384 or even 256 works well\n",
    "MAX_SEQ_LEN      = int(os.getenv(\"EMBED_MAX_LEN\", \"384\"))\n",
    "\n",
    "# nice progress bars\n",
    "SHOW_PROGRESS    = os.getenv(\"EMBED_PROGRESS\", \"1\") == \"1\"\n",
    "\n",
    "# If bs=1 still OOMs (rare), push that batch to CPU to finish and keep going\n",
    "CPU_FALLBACK     = os.getenv(\"OOM_CPU_FALLBACK\", \"1\") == \"1\"\n",
    "\n",
    "# Ensure output dir exists early\n",
    "Path(PERSIST_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------------------------\n",
    "# Device and model load\n",
    "# ---------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    # Not a memory tactic, but improves throughput on Ada/RTX40\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "embed_model = SentenceTransformer(EMBEDDING_MODEL, device=device)\n",
    "# Reduce max sequence length to save memory; long texts will be truncated\n",
    "embed_model.max_seq_length = MAX_SEQ_LEN\n",
    "\n",
    "# ---------------------------\n",
    "# ID generation (stable + low collision)\n",
    "# ---------------------------\n",
    "def stable_id(meta: Dict[str, Any], content: str) -> str:\n",
    "    \"\"\"\n",
    "    Stable, low-collision ID:\n",
    "    • Prefer source|chunk_id|sha12(content) when source+chunk_id exist.\n",
    "    • Fallback binds to source-hash + content-hash so identical text in different\n",
    "      files remains separate.\n",
    "    This keeps re-runs idempotent and minimizes accidental collisions.\n",
    "    \"\"\"\n",
    "    src = str(meta.get(\"source\", \"\")).strip()\n",
    "    cid = meta.get(\"chunk_id\", None)\n",
    "    ch  = hashlib.sha1(content.encode(\"utf-8\")).hexdigest()[:12]\n",
    "    if src and cid is not None:\n",
    "        return f\"{src}|{cid}|{ch}\"\n",
    "    sh  = hashlib.sha1(src.encode(\"utf-8\")).hexdigest()[:8]\n",
    "    return f\"g_{sh}_{hashlib.sha1(content.encode('utf-8')).hexdigest()[:16]}\"\n",
    "\n",
    "# ---------------------------\n",
    "# JSONL streaming (robust to bad lines)\n",
    "# ---------------------------\n",
    "def jsonl_iter(path: Path) -> Iterable[Dict[str, Any]]:\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for ln, line in enumerate(f, 1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                yield json.loads(line)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"[WARN] Skipping bad JSON at line {ln}: {e}\")\n",
    "                continue\n",
    "\n",
    "# ---------------------------\n",
    "# Metadata sanitizer (prevents list/dict errors in Chroma)\n",
    "# ---------------------------\n",
    "def sanitize_meta(meta: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Chroma requires scalar metadata values: str, int, float, bool, or None.\n",
    "    This function converts:\n",
    "        • List/Tuple/Set of scalars → \"a, b, c\"\n",
    "        • Complex lists (mixed/nested) → JSON string\n",
    "        • Dicts → JSON string (preserve Chinese with ensure_ascii=False)\n",
    "        • Other types → str(v)\n",
    "    Example fix:\n",
    "        [\"秦煜\"]  → \"秦煜\"\n",
    "    \"\"\"\n",
    "    clean: Dict[str, Any] = {}\n",
    "    for k, v in meta.items():\n",
    "        if isinstance(v, (str, int, float, bool)) or v is None:\n",
    "            clean[k] = v\n",
    "        elif isinstance(v, (list, tuple, set)):\n",
    "            seq = list(v)\n",
    "            if all(isinstance(x, (str, int, float, bool)) or x is None for x in seq):\n",
    "                clean[k] = \", \".join(\"\" if x is None else str(x) for x in seq)\n",
    "            else:\n",
    "                clean[k] = json.dumps(seq, ensure_ascii=False)\n",
    "        elif isinstance(v, dict):\n",
    "            clean[k] = json.dumps(v, ensure_ascii=False, sort_keys=True)\n",
    "        else:\n",
    "            clean[k] = str(v)\n",
    "    return clean\n",
    "\n",
    "# ---------------------------\n",
    "# Optional VRAM stats (debug)\n",
    "# ---------------------------\n",
    "def cuda_mem_summary(prefix: str = \"\"):\n",
    "    if not torch.cuda.is_available():\n",
    "        return\n",
    "    try:\n",
    "        free, total = torch.cuda.mem_get_info()  # bytes\n",
    "        used = total - free\n",
    "        gb = 1024**3\n",
    "        print(f\"[VRAM] {prefix} used={used/gb:.2f} GB / total={total/gb:.2f} GB (free={free/gb:.2f} GB)\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def clear_cuda_cache():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "# ---------------------------\n",
    "# OOM-resilient encoder with dynamic batch backoff + optional CPU fallback\n",
    "# ---------------------------\n",
    "@torch.no_grad()\n",
    "def encode_with_backoff(texts: List[str], start_bs: int, use_fp16: bool, model: SentenceTransformer, cpu_fallback: bool):\n",
    "    \"\"\"\n",
    "    Try encoding with the given batch size. On CUDA OOM:\n",
    "      - Halve batch size and retry.\n",
    "      - If batch size is 1 and still OOM, optionally move to CPU for that batch.\n",
    "    Returns a List[List[float]].\n",
    "    \"\"\"\n",
    "    bs = max(1, start_bs)\n",
    "    current_device = model.device  # torch.device('cuda') or ('cpu')\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            if use_fp16 and current_device.type == \"cuda\":\n",
    "                # AMP saves memory; sentence-transformers is autocast-aware\n",
    "                with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "                    vecs = model.encode(\n",
    "                        texts,\n",
    "                        batch_size=bs,\n",
    "                        normalize_embeddings=True,\n",
    "                        convert_to_numpy=True,\n",
    "                        show_progress_bar=False\n",
    "                    )\n",
    "            else:\n",
    "                vecs = model.encode(\n",
    "                    texts,\n",
    "                    batch_size=bs,\n",
    "                    normalize_embeddings=True,\n",
    "                    convert_to_numpy=True,\n",
    "                    show_progress_bar=False\n",
    "                )\n",
    "            return vecs.tolist()\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            msg = str(e)\n",
    "            if \"CUDA out of memory\" in msg or \"CUBLAS\" in msg:\n",
    "                print(f\"[OOM] CUDA OOM at batch_size={bs} (len={len(texts)}). Backing off…\")\n",
    "                clear_cuda_cache()\n",
    "                if bs > 1:\n",
    "                    bs = max(1, bs // 2)\n",
    "                    continue\n",
    "                # bs==1 and still OOM → optional CPU fallback\n",
    "                if cpu_fallback and current_device.type == \"cuda\":\n",
    "                    print(\"[OOM] Switching this batch to CPU to complete.\")\n",
    "                    model.to(\"cpu\")\n",
    "                    current_device = torch.device(\"cpu\")\n",
    "                    vecs = model.encode(\n",
    "                        texts,\n",
    "                        batch_size=1,\n",
    "                        normalize_embeddings=True,\n",
    "                        convert_to_numpy=True,\n",
    "                        show_progress_bar=False\n",
    "                    )\n",
    "                    # Move back to CUDA for subsequent batches if available\n",
    "                    if torch.cuda.is_available():\n",
    "                        model.to(\"cuda\")\n",
    "                    return vecs.tolist()\n",
    "                else:\n",
    "                    raise\n",
    "            else:\n",
    "                # Not an OOM error; surface it\n",
    "                raise\n",
    "\n",
    "# ---------------------------\n",
    "# Main pipeline\n",
    "# ---------------------------\n",
    "def main():\n",
    "    # Resolve & validate input\n",
    "    data_path = Path(DATA_PATH).expanduser().resolve()\n",
    "    if not data_path.exists():\n",
    "        raise SystemExit(f\"❌ JSONL not found: {data_path}\")\n",
    "\n",
    "    # Prepare Chroma\n",
    "    client = PersistentClient(path=PERSIST_DIR)\n",
    "    collection = client.get_or_create_collection(COLLECTION_NAME)\n",
    "\n",
    "    # Count lines for progress bar (optional; ok to skip on huge files)\n",
    "    try:\n",
    "        num_lines = sum(1 for _ in data_path.open(\"r\", encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        num_lines = None\n",
    "\n",
    "    iterator = jsonl_iter(data_path)\n",
    "    if SHOW_PROGRESS:\n",
    "        iterator = tqdm(iterator, total=num_lines, desc=\"Reading JSONL\")\n",
    "\n",
    "    BUFFER: List[Dict[str, Any]] = []\n",
    "    total = 0\n",
    "    current_bs = START_BATCH_SIZE\n",
    "\n",
    "    print(f\"📦 Persist dir: {PERSIST_DIR}\")\n",
    "    print(f\"🗃️  Collection: {COLLECTION_NAME}\")\n",
    "    print(f\"🧠 Model: {EMBEDDING_MODEL} | Device: {device} | fp16: {USE_FP16} | max_seq_len: {MAX_SEQ_LEN}\")\n",
    "    print(f\"📑 Input: {data_path}\")\n",
    "    print(f\"⚙️  Start batch size: {START_BATCH_SIZE} | CPU fallback: {CPU_FALLBACK}\")\n",
    "    if torch.cuda.is_available():\n",
    "        cuda_mem_summary(\"start\")\n",
    "\n",
    "    # Inner helper: flush a batch to Chroma robustly\n",
    "    def flush(batch: List[Dict[str, Any]]):\n",
    "        nonlocal total, current_bs\n",
    "        if not batch:\n",
    "            return\n",
    "\n",
    "        # Extract and sanitize\n",
    "        docs  = [b[\"content\"] for b in batch]\n",
    "        metas = [sanitize_meta(b[\"meta\"]) for b in batch]\n",
    "        ids   = [stable_id(b[\"meta\"], b[\"content\"]) for b in batch]\n",
    "\n",
    "        # Embed with OOM backoff\n",
    "        try:\n",
    "            vecs = encode_with_backoff(\n",
    "                docs,\n",
    "                start_bs=current_bs,\n",
    "                use_fp16=USE_FP16,\n",
    "                model=embed_model,\n",
    "                cpu_fallback=CPU_FALLBACK\n",
    "            )\n",
    "        except RuntimeError as e:\n",
    "            raise RuntimeError(f\"Embedding failed for a batch of size {len(docs)}: {e}\") from e\n",
    "        finally:\n",
    "            clear_cuda_cache()\n",
    "\n",
    "        # Intra-batch de-duplication (keeps only first occurrence of each ID)\n",
    "        seen = set()\n",
    "        keep = []\n",
    "        for i, _id in enumerate(ids):\n",
    "            if _id in seen:\n",
    "                continue\n",
    "            seen.add(_id)\n",
    "            keep.append(i)\n",
    "        if len(keep) != len(ids):\n",
    "            print(f\"[dedupe] removed {len(ids) - len(keep)} duplicate id(s) in the same batch)\")\n",
    "\n",
    "        ids   = [ids[i] for i in keep]\n",
    "        docs  = [docs[i] for i in keep]\n",
    "        vecs  = [vecs[i] for i in keep]\n",
    "        metas = [metas[i] for i in keep]\n",
    "\n",
    "        # Write to Chroma, preferring upsert for idempotency\n",
    "        try:\n",
    "            if hasattr(collection, \"upsert\"):\n",
    "                collection.upsert(ids=ids, embeddings=vecs, documents=docs, metadatas=metas)\n",
    "            else:\n",
    "                # Older Chroma: emulate upsert with add+update split\n",
    "                existing = set(collection.get(ids=ids).get(\"ids\", []) or [])\n",
    "                new_idx = [i for i, _id in enumerate(ids) if _id not in existing]\n",
    "                upd_idx = [i for i, _id in enumerate(ids) if _id in existing]\n",
    "                if new_idx:\n",
    "                    collection.add(\n",
    "                        ids=[ids[i] for i in new_idx],\n",
    "                        embeddings=[vecs[i] for i in new_idx],\n",
    "                        documents=[docs[i] for i in new_idx],\n",
    "                        metadatas=[metas[i] for i in new_idx],\n",
    "                    )\n",
    "                if upd_idx:\n",
    "                    collection.update(\n",
    "                        ids=[ids[i] for i in upd_idx],\n",
    "                        embeddings=[vecs[i] for i in upd_idx],\n",
    "                        documents=[docs[i] for i in upd_idx],\n",
    "                        metadatas=[metas[i] for i in upd_idx],\n",
    "                    )\n",
    "\n",
    "        except (chroma_errors.DuplicateIDError, ValueError) as e:\n",
    "            # Final safety net: heal per item (rare edge cases)\n",
    "            print(f\"[warn] batch write hit {type(e).__name__}: {e}\\n[repair] trying per-item upsert/repair\")\n",
    "            for i in range(len(ids)):\n",
    "                try:\n",
    "                    if hasattr(collection, \"upsert\"):\n",
    "                        collection.upsert(\n",
    "                            ids=[ids[i]],\n",
    "                            embeddings=[vecs[i]],\n",
    "                            documents=[docs[i]],\n",
    "                            metadatas=[metas[i]],\n",
    "                        )\n",
    "                    else:\n",
    "                        if collection.get(ids=[ids[i]]).get(\"ids\"):\n",
    "                            collection.update(\n",
    "                                ids=[ids[i]],\n",
    "                                embeddings=[vecs[i]],\n",
    "                                documents=[docs[i]],\n",
    "                                metadatas=[metas[i]],\n",
    "                            )\n",
    "                        else:\n",
    "                            collection.add(\n",
    "                                ids=[ids[i]],\n",
    "                                embeddings=[vecs[i]],\n",
    "                                documents=[docs[i]],\n",
    "                                metadatas=[metas[i]],\n",
    "                            )\n",
    "                except Exception as inner:\n",
    "                    # As a last resort, stringify all non-scalar meta fields and retry once\n",
    "                    fallback_meta = {\n",
    "                        k: (v if isinstance(v, (str, int, float, bool)) or v is None else json.dumps(v, ensure_ascii=False))\n",
    "                        for k, v in metas[i].items()\n",
    "                    }\n",
    "                    if hasattr(collection, \"upsert\"):\n",
    "                        collection.upsert(\n",
    "                            ids=[ids[i]],\n",
    "                            embeddings=[vecs[i]],\n",
    "                            documents=[docs[i]],\n",
    "                            metadatas=[fallback_meta],\n",
    "                        )\n",
    "                    else:\n",
    "                        if collection.get(ids=[ids[i]]).get(\"ids\"):\n",
    "                            collection.update(\n",
    "                                ids=[ids[i]],\n",
    "                                embeddings=[vecs[i]],\n",
    "                                documents=[docs[i]],\n",
    "                                metadatas=[fallback_meta],\n",
    "                            )\n",
    "                        else:\n",
    "                            collection.add(\n",
    "                                ids=[ids[i]],\n",
    "                                embeddings=[vecs[i]],\n",
    "                                documents=[docs[i]],\n",
    "                                metadatas=[fallback_meta],\n",
    "                            )\n",
    "\n",
    "        total += len(ids)\n",
    "        if SHOW_PROGRESS:\n",
    "            tqdm.write(f\"✅ Flushed {len(ids)} items (total={total})\")\n",
    "        if torch.cuda.is_available() and total % max(64, START_BATCH_SIZE) == 0:\n",
    "            cuda_mem_summary(f\"after {total}\")\n",
    "\n",
    "    # Stream → buffer → flush\n",
    "    for obj in iterator:\n",
    "        # Minimal schema validation\n",
    "        if not isinstance(obj, dict) or \"content\" not in obj or \"meta\" not in obj:\n",
    "            continue\n",
    "        BUFFER.append(obj)\n",
    "        if len(BUFFER) >= current_bs:\n",
    "            flush(BUFFER)\n",
    "            BUFFER.clear()\n",
    "\n",
    "    # Flush remainder\n",
    "    if BUFFER:\n",
    "        flush(BUFFER)\n",
    "        BUFFER.clear()\n",
    "\n",
    "    print(f\"\\n🎉 Done. Inserted {total} chunks into '{COLLECTION_NAME}' at '{PERSIST_DIR}'.\")\n",
    "    print(f\"Model: {EMBEDDING_MODEL} | Device: {device} | Start batch: {START_BATCH_SIZE} | max_seq_len: {MAX_SEQ_LEN}\")\n",
    "    if torch.cuda.is_available():\n",
    "        cuda_mem_summary(\"end\")\n",
    "\n",
    "# ---------------------------\n",
    "# Entry point\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a14076f",
   "metadata": {},
   "source": [
    "The above is the updated, production-ready, and teaching-oriented script with all the fixes you asked for:\n",
    "\n",
    "* OOM-safe on RTX 4070: dynamic batch backoff, FP16 autocast, shorter max sequence length, cache clears, and optional CPU fallback.\n",
    "\n",
    "* Duplicate-ID proof: upsert (with add+update fallback), stronger IDs, intra-batch de-duplication, and a final “per-item repair” safety net.\n",
    "\n",
    "* Metadata-safe: robust sanitize_meta(...) to prevent ValueError … got ['秦煜'] by converting lists/dicts to scalars/JSON.\n",
    "\n",
    "* Rich comments explaining inputs, outputs, and why each tactic exists."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0dd41f",
   "metadata": {},
   "source": [
    "### 3.3 药品数据（Excel → SQLite）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e12e585",
   "metadata": {},
   "source": [
    "可直接在本地运行的 Python 脚本（单文件），用于从 DrugBank（可选，需 API key）与 NMPA 数据查询站抓取/解析药品信息，生成 caremind/data/drugs.xlsx。脚本在没有 DrugBank API的情况下也能工作（只抓 NMPA），并支持离线解析本地 NMPA 说明书 PDF/HTML作为兜底。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdf6f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --in IN_FILE --out OUT_FILE\n",
      "                             [--nmpa-offline-dir NMPA_OFFLINE_DIR]\n",
      "                             [--no-nmpa-online] [--no-drugbank]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --in, --out\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/myunix/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "caremind_drugs.py\n",
    "\n",
    "双用方式\n",
    "--------\n",
    "1) Notebook 导入：\n",
    "   from caremind_drugs import run_from_notebook\n",
    "   run_from_notebook(\n",
    "       names=[\"氨氯地平\",\"二甲双胍\",\"阿托伐他汀\"],\n",
    "       out_file=\"~/caremind/data/drugs.xlsx\",\n",
    "       nmpa_offline_dir=\"~/caremind/data/nmpa_labels\",\n",
    "       use_nmpa_online=False,\n",
    "       use_drugbank=False\n",
    "   )\n",
    "\n",
    "   或使用底层 API：\n",
    "   from caremind_drugs import build_records, save_to_excel\n",
    "   recs = build_records([...], use_nmpa_online=False, nmpa_offline_dir=\"...\", use_drugbank=False)\n",
    "   save_to_excel(recs, \"~/caremind/data/drugs.xlsx\")\n",
    "\n",
    "2) 命令行：\n",
    "   python caremind_drugs.py \\\n",
    "     --in ~/caremind/data/drug_list.txt \\\n",
    "     --out ~/caremind/data/drugs.xlsx \\\n",
    "     --nmpa-offline-dir ~/caremind/data/nmpa_labels \\\n",
    "     --no-nmpa-online \\\n",
    "     --no-drugbank\n",
    "\n",
    "环境\n",
    "----\n",
    "- Python 3.10+\n",
    "- pip install: requests beautifulsoup4 lxml pandas openpyxl tenacity pdfminer.six pypdf2 chardet python-dotenv\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations # all type hints are treated as strings, meaning they are not evaluated until later. This is called \"deferred evaluation\" of annotations.\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import argparse # a powerful and flexible module used for parsing command-line arguments\n",
    "from dataclasses import dataclass # a way to simplify the creation of classes that are primarily used to store data\n",
    "from typing import Optional, Dict, List # used for type hinting to specify expected data types. Optional means a value can be of a type or None.\n",
    "\n",
    "import requests # a popular library for making HTTP requests in Python\n",
    "from bs4 import BeautifulSoup # a library for parsing HTML and XML documents\n",
    "import pandas as pd # a powerful data manipulation and analysis library\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type # a library for retrying operations that can fail\n",
    "from pdfminer.high_level import extract_text as pdf_extract_text # a library for extracting text from PDF files\n",
    "from PyPDF2 import PdfReader # a library for reading and manipulating PDF files\n",
    "import chardet # a library for detecting the encoding of text data\n",
    "from dotenv import load_dotenv # a library for loading environment variables from a .env file\n",
    "\n",
    "# ---------------------------\n",
    "# 基础配置\n",
    "# ---------------------------\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (compatible; CareMindDrugBot/1.0; +https://example.org)\"\n",
    "}\n",
    "NEXT_SECTION_RE = re.compile(r\"【[^】]{1,20}】\")\n",
    "SECTION_PATTERNS = [\n",
    "    (\"适应症\", r\"(?:【适应症】|适\\s*应\\s*症|适应证|适应症状)[：:\\s]*\"),\n",
    "    (\"禁忌症\", r\"(?:【禁忌】|禁\\s*忌|禁忌症)[：:\\s]*\"),\n",
    "    (\"药物相互作用\", r\"(?:【药物相互作用】|相互作用|药物-药物相互作用)[：:\\s]*\"),\n",
    "    (\"妊娠分级\", r\"(?:【孕妇及哺乳期用药】|孕妇及哺乳期用药|孕期用药|妊娠用药)[：:\\s]*\"),\n",
    "]\n",
    "\n",
    "@dataclass\n",
    "class DrugRecord:\n",
    "    name: str\n",
    "    indications: Optional[str] = None\n",
    "    contraindications: Optional[str] = None\n",
    "    interactions: Optional[str] = None\n",
    "    pregnancy_category: Optional[str] = None\n",
    "    source: Optional[str] = None\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 工具函数\n",
    "# ---------------------------\n",
    "def _ensure_utf8(text_bytes) -> str:\n",
    "    if not isinstance(text_bytes, (bytes, bytearray)):\n",
    "        return str(text_bytes)\n",
    "    det = chardet.detect(text_bytes)\n",
    "    enc = det.get(\"encoding\") or \"utf-8\"\n",
    "    try:\n",
    "        return text_bytes.decode(enc, errors=\"ignore\")\n",
    "    except Exception:\n",
    "        return text_bytes.decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "\n",
    "def _slice_section(text: str, start_pat: str) -> Optional[str]:\n",
    "    m = re.search(start_pat, text, flags=re.IGNORECASE)\n",
    "    if not m:\n",
    "        return None\n",
    "    start = m.end()\n",
    "    tail = text[start:]\n",
    "    nxt = NEXT_SECTION_RE.search(tail)\n",
    "    body = tail[:nxt.start()] if nxt else tail\n",
    "    return re.sub(r\"\\s+\", \" \", body).strip() or None\n",
    "\n",
    "\n",
    "def _parse_cn_label_text(full_text: str) -> Dict[str, Optional[str]]:\n",
    "    out = {\"适应症\": None, \"禁忌症\": None, \"药物相互作用\": None, \"妊娠分级\": None}\n",
    "    text = re.sub(r\"\\s+\", \" \", full_text)\n",
    "    for key, pat in SECTION_PATTERNS:\n",
    "        if key != \"妊娠分级\":\n",
    "            out[key] = _slice_section(text, pat) or out[key]\n",
    "    preg_txt = _slice_section(text, SECTION_PATTERNS[-1][1])\n",
    "    if preg_txt:\n",
    "        if re.search(r\"(禁用|绝对禁用|禁止使用)\", preg_txt):\n",
    "            out[\"妊娠分级\"] = \"禁用（未标注分级）\"\n",
    "        elif re.search(r\"(慎用|权衡利弊|风险.*收益)\", preg_txt):\n",
    "            out[\"妊娠分级\"] = \"慎用（未标注分级）\"\n",
    "        else:\n",
    "            out[\"妊娠分级\"] = \"未标注\"\n",
    "    else:\n",
    "        out[\"妊娠分级\"] = \"未标注\"\n",
    "    return out\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# NMPA 在线（占位：需按实际接口定制）\n",
    "# ---------------------------\n",
    "class NMPAClient:\n",
    "    def __init__(self, rate_sec: float = 1.2):\n",
    "        self.sess = requests.Session()\n",
    "        self.rate_sec = rate_sec\n",
    "        self.base = \"https://www.nmpa.gov.cn/datasearch/\"\n",
    "\n",
    "    @retry(reraise=True, stop=stop_after_attempt(3),\n",
    "           wait=wait_exponential(multiplier=1, min=1, max=8),\n",
    "           retry=retry_if_exception_type((requests.RequestException,)))\n",
    "    def _get(self, url: str, params: Optional[dict] = None) -> requests.Response:\n",
    "        resp = self.sess.get(url, headers=HEADERS, params=params, timeout=15)\n",
    "        if resp.status_code != 200:\n",
    "            raise requests.RequestException(f\"HTTP {resp.status_code}\")\n",
    "        return resp\n",
    "\n",
    "    def search_label_urls(self, drug_name: str) -> List[str]:\n",
    "        # 提醒：需抓站点真实搜索接口后替换此实现\n",
    "        return []\n",
    "\n",
    "    def fetch_label_text(self, url: str) -> Optional[str]:\n",
    "        try:\n",
    "            r = self._get(url)\n",
    "            time.sleep(self.rate_sec)\n",
    "            html = _ensure_utf8(r.content)\n",
    "            soup = BeautifulSoup(html, \"lxml\")\n",
    "            container = soup.find(\"div\", class_=\"article\") or soup.find(id=\"article\") or soup\n",
    "            text = container.get_text(separator=\"\\n\")\n",
    "            return re.sub(r\"\\n+\", \"\\n\", text).strip()\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# DrugBank API（可选）\n",
    "# ---------------------------\n",
    "class DrugBankClient:\n",
    "    def __init__(self):\n",
    "        load_dotenv()\n",
    "        self.api_key = os.getenv(\"DRUGBANK_API_KEY\")\n",
    "        self.base = os.getenv(\"DRUGBANK_BASE\", \"https://api.drugbank.com/v1\")\n",
    "        self.sess = requests.Session()\n",
    "        if self.api_key:\n",
    "            self.sess.headers.update({\n",
    "                \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "                \"Accept\": \"application/json\",\n",
    "                \"User-Agent\": HEADERS[\"User-Agent\"],\n",
    "            })\n",
    "\n",
    "    def available(self) -> bool:\n",
    "        return bool(self.api_key)\n",
    "\n",
    "    @retry(reraise=True, stop=stop_after_attempt(3),\n",
    "           wait=wait_exponential(multiplier=1, min=1, max=8),\n",
    "           retry=retry_if_exception_type((requests.RequestException,)))\n",
    "    def _get(self, path: str, params: Optional[dict] = None) -> dict:\n",
    "        url = self.base.rstrip(\"/\") + \"/\" + path.lstrip(\"/\")\n",
    "        resp = self.sess.get(url, params=params, timeout=20)\n",
    "        resp.raise_for_status()\n",
    "        return resp.json()\n",
    "\n",
    "    def query_by_name(self, name: str) -> Optional[dict]:\n",
    "        try:\n",
    "            j = self._get(\"drugs\", params={\"name\": name})\n",
    "            first = (j[0] if isinstance(j, list) and j else j) or None\n",
    "            if not first:\n",
    "                return None\n",
    "            did = first.get(\"id\") or first.get(\"drugbank_id\")\n",
    "            return self._get(f\"drugs/{did}\") if did else first\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def pick_fields(detail: dict) -> Dict[str, Optional[str]]:\n",
    "        def g(d, *ks, default=None):\n",
    "            cur = d\n",
    "            for k in ks:\n",
    "                if cur is None:\n",
    "                    return default\n",
    "                cur = cur.get(k)\n",
    "            return cur if cur is not None else default\n",
    "\n",
    "        indications = g(detail, \"indication\") or g(detail, \"indications\")\n",
    "        contraindications = g(detail, \"contraindications\")\n",
    "        interactions = None\n",
    "        intr = g(detail, \"drug_interactions\")\n",
    "        if isinstance(intr, list) and intr:\n",
    "            parts = []\n",
    "            for it in intr[:30]:\n",
    "                desc = it.get(\"description\") or it.get(\"text\")\n",
    "                partner = it.get(\"name\") or it.get(\"drug\") or \"\"\n",
    "                if desc:\n",
    "                    parts.append(f\"{partner}: {desc}\")\n",
    "            if parts:\n",
    "                interactions = \"；\".join(parts)\n",
    "        preg = g(detail, \"pregnancy_category\") or g(detail, \"fda_pregnancy_category\")\n",
    "        return {\"适应症\": indications, \"禁忌症\": contraindications,\n",
    "                \"药物相互作用\": interactions, \"妊娠分级\": preg}\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 离线说明书解析\n",
    "# ---------------------------\n",
    "def _read_pdf_text(path: str) -> str:\n",
    "    try:\n",
    "        txt = pdf_extract_text(path) or \"\"\n",
    "        if not txt:\n",
    "            reader = PdfReader(path)\n",
    "            buf = [(p.extract_text() or \"\") for p in reader.pages]\n",
    "            txt = \"\\n\".join(buf)\n",
    "        return txt\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def _read_html_text(path: str) -> str:\n",
    "    try:\n",
    "        with open(path, \"rb\") as f:\n",
    "            raw = f.read()\n",
    "        soup = BeautifulSoup(_ensure_utf8(raw), \"lxml\")\n",
    "        return re.sub(r\"\\n+\", \"\\n\", soup.get_text(separator=\"\\n\")).strip()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def _scan_offline_label(drug_name: str, folder: Optional[str]) -> Optional[str]:\n",
    "    if not folder or not os.path.isdir(os.path.expanduser(folder)):\n",
    "        return None\n",
    "    folder = os.path.expanduser(folder)\n",
    "    cands = [os.path.join(folder, fn) for fn in os.listdir(folder)\n",
    "             if re.search(re.escape(drug_name), fn, flags=re.IGNORECASE)]\n",
    "    cands = sorted(cands, key=lambda p: (0 if p.lower().endswith((\".html\", \".htm\")) else 1, p))\n",
    "    for p in cands:\n",
    "        if p.lower().endswith((\".html\", \".htm\")):\n",
    "            txt = _read_html_text(p)\n",
    "        elif p.lower().endswith(\".pdf\"):\n",
    "            txt = _read_pdf_text(p)\n",
    "        else:\n",
    "            continue\n",
    "        if txt and len(txt) > 50:\n",
    "            return txt\n",
    "    return None\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 核心构建与输出\n",
    "# ---------------------------\n",
    "def build_records(\n",
    "    names: List[str],\n",
    "    use_nmpa_online: bool = True,\n",
    "    nmpa_offline_dir: Optional[str] = None,\n",
    "    use_drugbank: bool = True\n",
    ") -> List[DrugRecord]:\n",
    "\n",
    "    nmpa = NMPAClient()\n",
    "    db = DrugBankClient()\n",
    "    if use_drugbank and not db.available():\n",
    "        use_drugbank = False\n",
    "\n",
    "    out: List[DrugRecord] = []\n",
    "    for name in names:\n",
    "        rec = DrugRecord(name=name)\n",
    "\n",
    "        # NMPA 在线（当前为占位；返回空即跳过）\n",
    "        if use_nmpa_online:\n",
    "            urls = nmpa.search_label_urls(name)\n",
    "            text = None\n",
    "            for u in urls:\n",
    "                text = nmpa.fetch_label_text(u)\n",
    "                if text:\n",
    "                    break\n",
    "            if text:\n",
    "                parsed = _parse_cn_label_text(text)\n",
    "                rec.indications = parsed[\"适应症\"]\n",
    "                rec.contraindications = parsed[\"禁忌症\"]\n",
    "                rec.interactions = parsed[\"药物相互作用\"]\n",
    "                rec.pregnancy_category = parsed[\"妊娠分级\"]\n",
    "                rec.source = \"NMPA说明书（在线）\"\n",
    "\n",
    "        # DrugBank（可选）\n",
    "        if use_drugbank and not all([rec.indications, rec.contraindications, rec.interactions, rec.pregnancy_category]):\n",
    "            detail = db.query_by_name(name)\n",
    "            if detail:\n",
    "                picked = DrugBankClient.pick_fields(detail)\n",
    "                rec.indications = rec.indications or picked[\"适应症\"]\n",
    "                rec.contraindications = rec.contraindications or picked[\"禁忌症\"]\n",
    "                rec.interactions = rec.interactions or picked[\"药物相互作用\"]\n",
    "                rec.pregnancy_category = rec.pregnancy_category or picked[\"妊娠分级\"]\n",
    "                rec.source = (rec.source + \" + DrugBank\") if rec.source else \"DrugBank\"\n",
    "\n",
    "        # 离线兜底\n",
    "        if not any([rec.indications, rec.contraindications, rec.interactions, rec.pregnancy_category]):\n",
    "            text = _scan_offline_label(name, nmpa_offline_dir)\n",
    "            if text:\n",
    "                parsed = _parse_cn_label_text(text)\n",
    "                rec.indications = parsed[\"适应症\"]\n",
    "                rec.contraindications = parsed[\"禁忌症\"]\n",
    "                rec.interactions = parsed[\"药物相互作用\"]\n",
    "                rec.pregnancy_category = parsed[\"妊娠分级\"]\n",
    "                rec.source = \"NMPA说明书（离线）\"\n",
    "\n",
    "        # 填补未标注\n",
    "        rec.indications = rec.indications or \"未标注\"\n",
    "        rec.contraindications = rec.contraindications or \"未标注\"\n",
    "        rec.interactions = rec.interactions or \"未标注\"\n",
    "        rec.pregnancy_category = rec.pregnancy_category or \"未标注\"\n",
    "        rec.source = rec.source or \"未获取（请补充源）\"\n",
    "\n",
    "        out.append(rec)\n",
    "    return out\n",
    "\n",
    "\n",
    "def save_to_excel(records: List[DrugRecord], out_path: str):\n",
    "    os.makedirs(os.path.dirname(os.path.expanduser(out_path)), exist_ok=True)\n",
    "    rows = [{\n",
    "        \"药品名称\": r.name,\n",
    "        \"适应症\": r.indications,\n",
    "        \"禁忌症\": r.contraindications,\n",
    "        \"药物相互作用\": r.interactions,\n",
    "        \"妊娠分级\": r.pregnancy_category,\n",
    "        \"来源\": r.source,\n",
    "    } for r in records]\n",
    "    df = pd.DataFrame(rows, columns=[\"药品名称\",\"适应症\",\"禁忌症\",\"药物相互作用\",\"妊娠分级\",\"来源\"])\n",
    "    df.to_excel(os.path.expanduser(out_path), index=False)\n",
    "    print(f\"✅ 已写出 {len(df)} 条 → {os.path.expanduser(out_path)}\")\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Notebook 友好入口\n",
    "# ---------------------------\n",
    "def run_from_notebook(\n",
    "    names: Optional[List[str]] = None,\n",
    "    in_file: Optional[str] = None,\n",
    "    out_file: str = \"~/caremind/data/drugs.xlsx\",\n",
    "    nmpa_offline_dir: Optional[str] = None,\n",
    "    use_nmpa_online: bool = False,\n",
    "    use_drugbank: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    - names 和 in_file 二选一；都提供时以 names 优先。\n",
    "    - 默认关闭在线检索与 DrugBank，利于先跑通离线流程。\n",
    "    \"\"\"\n",
    "    if not names:\n",
    "        if not in_file:\n",
    "            raise ValueError(\"请提供 names 列表或 in_file 路径\")\n",
    "        in_file = os.path.expanduser(in_file)\n",
    "        if not os.path.exists(in_file):\n",
    "            raise FileNotFoundError(f\"未找到药品清单：{in_file}\")\n",
    "        with open(in_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            names = [ln.strip() for ln in f if ln.strip()]\n",
    "\n",
    "    recs = build_records(\n",
    "        names=names,\n",
    "        use_nmpa_online=use_nmpa_online,\n",
    "        nmpa_offline_dir=os.path.expanduser(nmpa_offline_dir) if nmpa_offline_dir else None,\n",
    "        use_drugbank=use_drugbank\n",
    "    )\n",
    "    save_to_excel(recs, out_file)\n",
    "    return recs\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# CLI 入口\n",
    "# ---------------------------\n",
    "def _load_names(path: str) -> List[str]:\n",
    "    with open(os.path.expanduser(path), \"r\", encoding=\"utf-8\") as f:\n",
    "        return [ln.strip() for ln in f if ln.strip()]\n",
    "\n",
    "def main():\n",
    "    ap = argparse.ArgumentParser(description=\"Build drugs.xlsx from NMPA/DrugBank/local labels\")\n",
    "    ap.add_argument(\"--in\", dest=\"in_file\", required=True, help=\"药品清单 txt（每行一个药名）\")\n",
    "    ap.add_argument(\"--out\", dest=\"out_file\", required=True, help=\"输出 Excel 路径\")\n",
    "    ap.add_argument(\"--nmpa-offline-dir\", dest=\"nmpa_offline_dir\", default=None, help=\"离线 NMPA 说明书目录（可选）\")\n",
    "    ap.add_argument(\"--no-nmpa-online\", action=\"store_true\", help=\"禁用 NMPA 在线检索\")\n",
    "    ap.add_argument(\"--no-drugbank\", action=\"store_true\", help=\"禁用 DrugBank API\")\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    names = _load_names(args.in_file)\n",
    "    recs = build_records(\n",
    "        names=names,\n",
    "        use_nmpa_online=not args.no_nmpa_online,\n",
    "        nmpa_offline_dir=args.nmpa_offline_dir,\n",
    "        use_drugbank=not args.no_drugbank\n",
    "    )\n",
    "    save_to_excel(recs, args.out_file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3cc22e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'run_from_notebook' from 'caremind_drugs' (/home/myunix/caremind/caremind_drugs.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      3\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m~/caremind\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcaremind_drugs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m run_from_notebook\n\u001b[1;32m      7\u001b[0m recs \u001b[38;5;241m=\u001b[39m run_from_notebook(\n\u001b[1;32m      8\u001b[0m     names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m氨氯地平\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m二甲双胍\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m阿托伐他汀\u001b[39m\u001b[38;5;124m\"\u001b[39m],        \u001b[38;5;66;03m# 或用 in_file=\"~/caremind/data/drug_list.txt\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     out_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m~/caremind/data/drugs.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     use_drugbank\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m       \u001b[38;5;66;03m# 若配置了 DrugBank API 再改 True\u001b[39;00m\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mlen\u001b[39m(recs)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'run_from_notebook' from 'caremind_drugs' (/home/myunix/caremind/caremind_drugs.py)"
     ]
    }
   ],
   "source": [
    "# 假设 caremind_drugs.py 与 Notebook 同在 ~/caremind/\n",
    "import sys, os\n",
    "sys.path.append(os.path.expanduser(\"~/caremind\"))\n",
    "\n",
    "from caremind_drugs import run_from_notebook\n",
    "\n",
    "recs = run_from_notebook(\n",
    "    names=[\"氨氯地平\",\"二甲双胍\",\"阿托伐他汀\"],        # 或用 in_file=\"~/caremind/data/drug_list.txt\"\n",
    "    out_file=\"~/caremind/data/drugs.xlsx\",\n",
    "    nmpa_offline_dir=\"~/caremind/data/nmpa_labels\",\n",
    "    use_nmpa_online=False,   # 你稍后完善 NMPA 在线接口后可改 True\n",
    "    use_drugbank=False       # 若配置了 DrugBank API 再改 True\n",
    ")\n",
    "len(recs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321db1f6",
   "metadata": {},
   "source": [
    "手工整理 data/drugs.xlsx（字段：name、indications、contraindications、interactions、dosage、pregnancy_category、source）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a34132c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RangeIndex(start=0, stop=0, step=1)\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "near \")\": syntax error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDrugs loaded into SQLite.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 35\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 29\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mhead())\n\u001b[0;32m---> 29\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdrugs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_exists\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreplace\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m con\u001b[38;5;241m.\u001b[39mcommit()\n\u001b[1;32m     31\u001b[0m con\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/caremind/lib/python3.10/site-packages/pandas/util/_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    332\u001b[0m     )\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/caremind/lib/python3.10/site-packages/pandas/core/generic.py:3106\u001b[0m, in \u001b[0;36mNDFrame.to_sql\u001b[0;34m(self, name, con, schema, if_exists, index, index_label, chunksize, dtype, method)\u001b[0m\n\u001b[1;32m   2908\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2909\u001b[0m \u001b[38;5;124;03mWrite records stored in a DataFrame to a SQL database.\u001b[39;00m\n\u001b[1;32m   2910\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3102\u001b[0m \u001b[38;5;124;03m[(1,), (None,), (2,)]\u001b[39;00m\n\u001b[1;32m   3103\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m   3104\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sql\n\u001b[0;32m-> 3106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_sql\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3107\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mif_exists\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_exists\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3117\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/caremind/lib/python3.10/site-packages/pandas/io/sql.py:844\u001b[0m, in \u001b[0;36mto_sql\u001b[0;34m(frame, name, con, schema, if_exists, index, index_label, chunksize, dtype, method, engine, **engine_kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mframe\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m argument should be either a Series or a DataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    841\u001b[0m     )\n\u001b[1;32m    843\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pandasSQL_builder(con, schema\u001b[38;5;241m=\u001b[39mschema, need_transaction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[0;32m--> 844\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpandas_sql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_sql\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_exists\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_exists\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/caremind/lib/python3.10/site-packages/pandas/io/sql.py:2840\u001b[0m, in \u001b[0;36mSQLiteDatabase.to_sql\u001b[0;34m(self, frame, name, if_exists, index, index_label, schema, chunksize, dtype, method, engine, **engine_kwargs)\u001b[0m\n\u001b[1;32m   2829\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmy_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) not a string\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2831\u001b[0m table \u001b[38;5;241m=\u001b[39m SQLiteTable(\n\u001b[1;32m   2832\u001b[0m     name,\n\u001b[1;32m   2833\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2838\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   2839\u001b[0m )\n\u001b[0;32m-> 2840\u001b[0m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2841\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m table\u001b[38;5;241m.\u001b[39minsert(chunksize, method)\n",
      "File \u001b[0;32m~/miniconda3/envs/caremind/lib/python3.10/site-packages/pandas/io/sql.py:991\u001b[0m, in \u001b[0;36mSQLTable.create\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mif_exists \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    990\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpd_sql\u001b[38;5;241m.\u001b[39mdrop_table(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema)\n\u001b[0;32m--> 991\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_create\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mif_exists \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    993\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/caremind/lib/python3.10/site-packages/pandas/io/sql.py:2514\u001b[0m, in \u001b[0;36mSQLiteTable._execute_create\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2512\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpd_sql\u001b[38;5;241m.\u001b[39mrun_transaction() \u001b[38;5;28;01mas\u001b[39;00m conn:\n\u001b[1;32m   2513\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stmt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtable:\n\u001b[0;32m-> 2514\u001b[0m         \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstmt\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOperationalError\u001b[0m: near \")\": syntax error"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3, os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "db_path = os.getenv(\"SQLITE_PATH\", \"./db/drugs.sqlite\")\n",
    "os.makedirs(os.path.dirname(db_path), exist_ok=True)\n",
    "\n",
    "schema = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS drugs (\n",
    "  id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "  name TEXT UNIQUE,\n",
    "  indications TEXT,\n",
    "  contraindications TEXT,\n",
    "  interactions TEXT,\n",
    "  dosage TEXT,\n",
    "  pregnancy_category TEXT,\n",
    "  source TEXT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "def main():\n",
    "    df = pd.read_excel(\"data/drugs.xlsx\")\n",
    "    con = sqlite3.connect(db_path)\n",
    "    cur = con.cursor()\n",
    "    cur.executescript(schema)\n",
    "    print(df.columns)\n",
    "    print(df.head())\n",
    "    df.to_sql(\"drugs\", con, if_exists=\"replace\", index=False)\n",
    "    con.commit()\n",
    "    con.close()\n",
    "    print(\"Drugs loaded into SQLite.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "caremind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
