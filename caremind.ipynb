{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da58641a",
   "metadata": {},
   "source": [
    "# CareMind: a MVP CDSS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473557e2",
   "metadata": {},
   "source": [
    "## 0 ç›®æ ‡ä¸èŒƒå›´ï¼ˆMVP è¾¹ç•Œï¼‰\n",
    "\n",
    "è¦†ç›– 1â€“2 ä¸ªä¼˜å…ˆç–¾ç—…åœºæ™¯ï¼ˆå¦‚é«˜è¡€å‹ã€2 å‹ç³–å°¿ç—…ï¼‰ã€‚\n",
    "\n",
    "çŸ¥è¯†æºï¼š3â€“5 ä»½å«å¥å§”æŒ‡å— + 1â€“2 ä»½æŠ¤ç†å…±è¯† + 10 ç§å¸¸ç”¨è¯ï¼ˆé˜¿å¸åŒ¹æ—ã€äºŒç”²åŒèƒã€æ°¨æ°¯åœ°å¹³ç­‰ï¼‰ã€‚\n",
    "\n",
    "èƒ½åŠ›ï¼šè‡ªç„¶è¯­è¨€æé—® â†’ï¼ˆå‘é‡æ£€ç´¢æŒ‡å—/å…±è¯† + SQL æŸ¥è¯¢è¯å“è¡¨ï¼‰â†’ ç”Ÿæˆå¸¦å¼•ç”¨ä¸åˆè§„æç¤ºçš„å›ç­”ã€‚\n",
    "\n",
    "UIï¼šç®€å•çš„ Streamlit å•é¡µï¼ˆè¾“å…¥é—®é¢˜ã€å±•ç¤ºä¾æ®ç‰‡æ®µä¸æ¥æºã€è¯å“è¡¨ç»“æ„åŒ–ä¿¡æ¯ã€å…è´£å£°æ˜ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636d9ab8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51b8d710",
   "metadata": {},
   "source": [
    "ç°æœ‰æœ¬åœ°ç¯å¢ƒï¼ˆWin11 + WSL + RTX 4070 SUPER + VS Code/Jupyterï¼‰ï¼Œå¹¶ä»¥**RAGï¼ˆå‘é‡åº“ï¼‰+ ç»“æ„åŒ–è¯å“åº“ï¼ˆSQLiteï¼‰**çš„æ··åˆæ£€ç´¢ä¸ºæ ¸å¿ƒã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66414576",
   "metadata": {},
   "source": [
    "## 1. ç¯å¢ƒå‡†å¤‡ï¼ˆWSL å†…æ‰§è¡Œï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdbea64",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6923260d",
   "metadata": {},
   "source": [
    "### 1.1 CUDA/é©±åŠ¨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1add5400",
   "metadata": {},
   "source": [
    "Windows ä¾§å·²è£…å¥½ NVIDIA é©±åŠ¨ä¸ CUDA/cuDNNï¼›WSL2 ä¸­å»ºè®®ä½¿ç”¨ nvidia-smi ä¸ torch.cuda.is_available() éªŒè¯ GPU å¯ç”¨ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edf5e03",
   "metadata": {},
   "source": [
    "(base) myunix@40VFO2U:~$ mkdir caremind\n",
    "\n",
    "(base) myunix@40VFO2U:~$ cd caremind\n",
    "\n",
    "(base) myunix@40VFO2U:~/caremind$\n",
    "\n",
    "(base) myunix@40VFO2U:~/caremind$ nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ed62a4",
   "metadata": {},
   "source": [
    "### 1.2 æœ¬åœ°æ¨ç†æœåŠ¡ï¼ˆæ¨è Ollama + Qwen2ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5276a91b",
   "metadata": {},
   "source": [
    "æˆ‘ç”¨ 4070 SUPERï¼Œæœ¬åœ° 7Bâ€“14B æ¨ç†å¾ˆåˆé€‚ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62969a7c",
   "metadata": {},
   "source": [
    "(base) myunix@40VFO2U:~/caremind$ curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    ">>> Cleaning up old version at /usr/local/lib/ollama\n",
    "\n",
    "[sudo] password for myunix:\n",
    "\n",
    ">>> Installing ollama to /usr/local\n",
    "\n",
    ">>> Downloading Linux amd64 bundle\n",
    "\n",
    "######################################################################## 100.0%\n",
    "\n",
    ">>> Creating ollama user...\n",
    "\n",
    ">>> Adding ollama user to render group...\n",
    "\n",
    ">>> Adding ollama user to video group...\n",
    "\n",
    ">>> Adding current user to ollama group...\n",
    "\n",
    ">>> Creating ollama systemd service...\n",
    "\n",
    ">>> Enabling and starting ollama service...\n",
    "\n",
    "Created symlink /etc/systemd/system/default.target.wants/ollama.service â†’ /etc/systemd/system/ollama.service.\n",
    "\n",
    ">>> Nvidia GPU detected.\n",
    "\n",
    ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
    "\n",
    ">>> Install complete. Run \"ollama\" from the command line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e27f398",
   "metadata": {},
   "source": [
    "## 2 é¡¹ç›®ç»“æ„ä¸é…ç½®"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951903e2",
   "metadata": {},
   "source": [
    "caremind/\n",
    "  â”œâ”€ .env                           # ç¯å¢ƒå˜é‡\n",
    "  â”œâ”€ data/\n",
    "  â”‚   â”œâ”€ guidelines/                # å«å¥å§”/æŠ¤ç†å…±è¯† PDF/HTML/Word\n",
    "  â”‚   â””â”€ drugs.xlsx                 # 10ç§è¯çš„Excelï¼ˆåˆæœŸï¼‰\n",
    "  â”œâ”€ db/\n",
    "  â”‚   â””â”€ drugs.sqlite               # SQLite å®ä¾‹\n",
    "  â”œâ”€ embeddings/\n",
    "  â”‚   â””â”€ bge-large-zh/              # å¯é€‰ï¼šç¼“å­˜æ¨¡å‹æƒé‡\n",
    "  â”œâ”€ ingest/\n",
    "  â”‚   â”œâ”€ parse_docs.py              # æ–‡æ¡£æŠ½å–ä¸åˆ‡ç‰‡\n",
    "  â”‚   â”œâ”€ build_vectors.py           # åµŒå…¥ä¸å†™å…¥ Chroma\n",
    "  â”‚   â””â”€ load_drugs.py              # Excelâ†’SQLite\n",
    "  â”œâ”€ rag/\n",
    "  â”‚   â”œâ”€ retriever.py               # æ··åˆæ£€ç´¢ï¼ˆChroma+SQLiteï¼‰\n",
    "  â”‚   â”œâ”€ prompt.py                  # æç¤ºè¯æ¨¡æ¿ï¼ˆå«åˆè§„è¦æ±‚ä¸å¼•ç”¨æ ¼å¼ï¼‰\n",
    "  â”‚   â””â”€ pipeline.py                # ç«¯åˆ°ç«¯ RAG ç®¡çº¿\n",
    "  â”œâ”€ app.py                         # Streamlit å‰ç«¯\n",
    "  â””â”€ README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f15568",
   "metadata": {},
   "source": [
    "(base) myunix@40VFO2U:~/caremind$ mkdir data\n",
    "\n",
    "(base) myunix@40VFO2U:~/caremind$ cd data\n",
    "\n",
    "(base) myunix@40VFO2U:~/caremind/data$ mkdir guidlines\n",
    "\n",
    "(base) myunix@40VFO2U:~/caremind/data$ cd ..\n",
    "\n",
    "(base) myunix@40VFO2U:~/caremind$ mkdir db\n",
    "\n",
    "(base) myunix@40VFO2U:~/caremind$ mkdir embeddings\n",
    "\n",
    "(base) myunix@40VFO2U:~/caremind$ cd embeddings\n",
    "\n",
    "(base) myunix@40VFO2U:~/caremind/embeddings$ mkdir bge-large-zh\n",
    "\n",
    "(base) myunix@40VFO2U:~/caremind/embeddings$ cd ..\n",
    "\n",
    "(base) myunix@40VFO2U:~/caremind$ mkdir ingest\n",
    "\n",
    "(base) myunix@40VFO2U:~/caremind$ mkdir rag\n",
    "\n",
    "(base) myunix@40VFO2U:~/caremind$ ls\n",
    "\n",
    "data  db  embeddings  ingest  rag\n",
    "\n",
    "(base) myunix@40VFO2U:~/caremind$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30358c55",
   "metadata": {},
   "source": [
    "#### .env ç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf3358a",
   "metadata": {},
   "source": [
    "##### # LLM/Ollama\n",
    "OLLAMA_BASE_URL=http://localhost:11434\n",
    "LLM_MODEL=qwen2:7b-instruct\n",
    "\n",
    "##### # Embedding æ¨¡å‹ï¼ˆä¸­æ–‡ä¼˜å…ˆï¼‰\n",
    "EMBEDDING_MODEL=BAAI/bge-large-zh-v1.5\n",
    "\n",
    "##### # Chroma å‘é‡åº“å­˜å‚¨è·¯å¾„\n",
    "CHROMA_PERSIST_DIR=./chroma_store\n",
    "\n",
    "##### # SQLite\n",
    "SQLITE_PATH=./db/drugs.sqlite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7b4baf",
   "metadata": {},
   "source": [
    "### 1.4 Conda ç¯å¢ƒä¸ä¾èµ–"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12bc2e1",
   "metadata": {},
   "source": [
    "(base) myunix@40VFO2U:~/caremind$conda create -n caremind python=3.10.18 -y\n",
    "\n",
    "(base) myunix@40VFO2U:~/caremind$conda activate caremind\n",
    "\n",
    "(caremind) myunix@40VFO2U:~/caremind$ pip install --upgrade pip\n",
    "\n",
    "#### æ ¸å¿ƒï¼šRAG + åˆ†è¯/åµŒå…¥ + æœ¬åœ°LLM(èµ°Ollama) + UI + PDFæŠ½å– + ä¸­æ–‡å¤„ç†\n",
    "(caremind) myunix@40VFO2U:~/caremind$ pip install langchain langchain-community chromadb pydantic-settings\n",
    "\n",
    "(caremind) myunix@40VFO2U:~/caremind$ pip install sentence-transformers # ç”¨äºä¸­æ–‡ bge\n",
    "\n",
    "(caremind) myunix@40VFO2U:~/caremind$ pip install transformers accelerate torch --extra-index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "(caremind) myunix@40VFO2U:~/caremind$ pip install pdfplumber pypdf pdf2image pytesseract pillow beautifulsoup4 lxml\n",
    "\n",
    "(caremind) myunix@40VFO2U:~/caremind$ pip install jieba nltk\n",
    "\n",
    "(caremind) myunix@40VFO2U:~/caremind$ pip install streamlit\n",
    "\n",
    "(caremind) myunix@40VFO2U:~/caremind$ pip install python-dotenv\n",
    "\n",
    "(caremind) myunix@40VFO2U:~/caremind$ pip install sqlalchemy aiosqlite\n",
    "\n",
    "Newly added:\n",
    "\n",
    "(caremind) myunix@40VFO2U:~/caremind$ pip install -U \"chromadb>=0.5\"\n",
    "\n",
    "pip install requests pandas openpyxl tenacity pdfminer.six pypdf2 chardet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf94c176",
   "metadata": {},
   "source": [
    "### 1.5 Open VS Code from WSL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b77ec79",
   "metadata": {},
   "source": [
    "(caremind) myunix@40VFO2U:~/caremind$ code ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7fbb45",
   "metadata": {},
   "source": [
    "create or open file: caremind.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d95589",
   "metadata": {},
   "source": [
    "use ctrl+shit+p to select the Python Environment: caremind(Python3.10.18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70851a4",
   "metadata": {},
   "source": [
    "## 3 æ•°æ®å‡†å¤‡ä¸å…¥åº“"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6990110",
   "metadata": {},
   "source": [
    "### 3.1 æ–‡æ¡£æŠ½å–ä¸åˆ‡ç‰‡ï¼ˆæŒ‡å—/æŠ¤ç†å…±è¯†ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d4f7c1",
   "metadata": {},
   "source": [
    "ç­–ç•¥ï¼šæŒ‰æ ‡é¢˜å±‚çº§ + è¯­ä¹‰åˆ‡ç‰‡ï¼Œæ¯å— 500â€“1000 å­—ï¼Œä¿ç•™å…ƒæ•°æ®ï¼ˆæ¥æºã€å¹´ä»½/ç‰ˆæœ¬ã€è¯æ®ç­‰çº§ã€äººç¾¤é€‚ç”¨ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12578ec9",
   "metadata": {},
   "source": [
    "Working dir assumption (you can change paths if needed): /home/myunix/caremind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bd34d80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/myunix/caremind'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dc6f80",
   "metadata": {},
   "source": [
    "#### Pipeline\n",
    "1. Extract year from filename (Chinese guideline naming patterns).\n",
    "2. Extract text (pdfplumber â†’ OCR fallback with pdf2image + pytesseract).\n",
    "3. Heuristically split into titled chunks.\n",
    "4. Write JSONL with metadata (content, meta)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be660df3",
   "metadata": {},
   "source": [
    "ğŸ§± SECTION BREAKDOWN\n",
    "SECTION PURPOSE\n",
    "1. Metadata Extraction Utils\n",
    "Self-contained functions to extract: year,title,authors,typefrom filename â€” easily testable and reusable.\n",
    "2. Text Extraction\n",
    "pdfplumber, wrapper + optional OCR fallback (commented out).\n",
    "3. Chunking Logic\n",
    "Enhanced heuristic-based chunking tuned for Chinese medical text â€” outputs structured chunks with full metadata.\n",
    "4. Main Pipeline\n",
    "Orchestrates file discovery â†’ metadata extraction â†’ text extraction â†’ chunking â†’ JSONL output. Includes debug prints.\n",
    "5. Run\n",
    "Standard Python entry point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f44a3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Input directory: /home/myunix/caremind/data/guidelines\n",
      "ğŸ“„ Found 16 PDF files.\n",
      "\n",
      "--- ğŸ“„ Processing: å¦Šå¨ æœŸç³–å°¿ç—…æ‚£è€…äº§å‰è¡€ç³–ç®¡ç†çš„è¯æ®æ€»ç»“_ç§¦ç…œ(2023).pdf ---\n",
      "  ğŸ“… Year (from filename): 2023\n",
      "  ğŸ·ï¸ Title (from filename): å¦Šå¨ æœŸç³–å°¿ç—…æ‚£è€…äº§å‰è¡€ç³–ç®¡ç†çš„è¯æ®æ€»ç»“\n",
      "  ğŸ‘¥ Authors (from filename): ['ç§¦ç…œ']\n",
      "  ğŸ“‘ Type (from filename): evidence_summary\n",
      "  ğŸ”¤ Extracted 11758 characters.\n",
      "  âœï¸  Authors (final): ['ç§¦ç…œ']\n",
      "  ğŸ“š Journal:  (), , \n",
      "  ğŸ”— DOI: \n",
      "  ğŸ·ï¸ Keywords: []\n",
      "  ğŸ§­ Original Guideline: \n",
      "  ğŸ“‘ Doc Type (final): evidence_summary\n",
      "  ğŸ§© Generated 34 chunks.\n",
      "\n",
      "--- ğŸ“„ Processing: å† å¿ƒç—…åˆå¹¶2 å‹ç³–å°¿ç—…æ‚£è€…çš„è¡€ç³–ç®¡ç†ä¸“å®¶å…±è¯†(2024)_ä¸­å›½åŒ»ç–—ä¿å¥å›½é™…äº¤æµä¿ƒè¿›ä¼šå¿ƒè¡€ç®¡ç—…å­¦åˆ†ä¼š.pdf ---\n",
      "  ğŸ“… Year (from filename): 2024\n",
      "  ğŸ·ï¸ Title (from filename): å† å¿ƒç—…åˆå¹¶2 å‹ç³–å°¿ç—…æ‚£è€…çš„è¡€ç³–ç®¡ç†ä¸“å®¶å…±è¯†\n",
      "  ğŸ‘¥ Authors (from filename): ['ä¸­å›½åŒ»ç–—ä¿å¥å›½é™…äº¤æµä¿ƒè¿›ä¼šå¿ƒè¡€ç®¡ç—…å­¦åˆ†ä¼š']\n",
      "  ğŸ“‘ Type (from filename): consensus\n",
      "  ğŸ”¤ Extracted 29204 characters.\n",
      "  âœï¸  Authors (final): ['ä¸­å›½åŒ»ç–—ä¿å¥å›½é™…äº¤æµä¿ƒè¿›ä¼šå¿ƒè¡€ç®¡ç—…å­¦åˆ†ä¼š']\n",
      "  ğŸ“š Journal:  (), , \n",
      "  ğŸ”— DOI: 10.3969/j.issn.1000-3614.2024.04.003ChaoXing\n",
      "  ğŸ·ï¸ Keywords: []\n",
      "  ğŸ§­ Original Guideline: \n",
      "  ğŸ“‘ Doc Type (final): consensus\n",
      "  ğŸ§© Generated 103 chunks.\n",
      "\n",
      "--- ğŸ“„ Processing: ä¸­å›½é«˜è¡€å‹æ‚£è€…å¿ƒç‡ç®¡ç†å¤šå­¦ç§‘ä¸“å®¶å…±è¯†ï¼ˆ2021å¹´ç‰ˆï¼‰_é«˜è¡€å‹å¿ƒç‡ç®¡ç†å¤šå­¦ç§‘å…±è¯†ç»„.pdf ---\n",
      "  ğŸ“… Year (from filename): 2021\n",
      "  ğŸ·ï¸ Title (from filename): ä¸­å›½é«˜è¡€å‹æ‚£è€…å¿ƒç‡ç®¡ç†å¤šå­¦ç§‘ä¸“å®¶å…±è¯†\n",
      "  ğŸ‘¥ Authors (from filename): ['é«˜è¡€å‹å¿ƒç‡ç®¡ç†å¤šå­¦ç§‘å…±è¯†ç»„']\n",
      "  ğŸ“‘ Type (from filename): consensus\n",
      "  ğŸ”¤ Extracted 18607 characters.\n",
      "  âœï¸  Authors (final): ['é«˜è¡€å‹å¿ƒç‡ç®¡ç†å¤šå­¦ç§‘å…±è¯†ç»„']\n",
      "  ğŸ“š Journal:  (), 2501, \n",
      "  ğŸ”— DOI: 10.12114/j.issn.1007-9572.2021.00.595\n",
      "  ğŸ·ï¸ Keywords: ['é«˜è¡€å‹', 'å¿ƒç‡', 'ç–¾ç—…ç®¡ç†', 'å¤šå­¦ç§‘è¯Šç–—æ¨¡å¼', 'ä¸“å®¶å…±è¯†']\n",
      "  ğŸ§­ Original Guideline: \n",
      "  ğŸ“‘ Doc Type (final): consensus\n",
      "  ğŸ§© Generated 107 chunks.\n",
      "\n",
      "--- ğŸ“„ Processing: æˆäºº2å‹ç³–å°¿ç—…çš„é«˜è¡€å‹ç®¡ç†ä¸­å›½ä¸“å®¶å…±è¯†(2025).pdf ---\n",
      "  ğŸ“… Year (from filename): 2025\n",
      "  ğŸ·ï¸ Title (from filename): æˆäºº2å‹ç³–å°¿ç—…çš„é«˜è¡€å‹ç®¡ç†ä¸­å›½ä¸“å®¶å…±è¯†\n",
      "  ğŸ‘¥ Authors (from filename): []\n",
      "  ğŸ“‘ Type (from filename): consensus\n",
      "  ğŸ”¤ Extracted 39421 characters.\n",
      "  âœï¸  Authors (final): ['é‡åº†', 'ï¼š ï¼›æèˆäºˆ', 'å››å·å¤§å­¦åè¥¿åŒ»é™¢å†…åˆ†æ³Œä»£è°¢ç§‘', 'æˆéƒ½']\n",
      "  ğŸ“š Journal:  (), , \n",
      "  ğŸ”— DOI: 10.1111/\n",
      "  ğŸ·ï¸ Keywords: ['ç³–å°¿ç—…', 'å‹', 'é«˜è¡€å‹', 'é«˜è¡€å‹ç­›æŸ¥', 'è¡€å‹ç›‘æµ‹', 'é™å‹æ²»ç–—']\n",
      "  ğŸ§­ Original Guideline: \n",
      "  ğŸ“‘ Doc Type (final): consensus\n",
      "  ğŸ§© Generated 279 chunks.\n",
      "\n",
      "--- ğŸ“„ Processing: ã€Šä¸­å›½é«˜è¡€å‹é˜²æ²»æŒ‡å—(2024å¹´ä¿®è®¢ç‰ˆ)ã€‹æ–°å¢å†…å®¹è§£è¯» â€”â€”ä»¥æ”¹å–„è¡€å‹å˜å¼‚å’Œé™å‹ç›®æ ‡èŒƒå›´å†…æ—¶é—´ä¸ºæ ¸å¿ƒçš„é«˜è´¨é‡é™å‹ç­–ç•¥æµ…æ_å¼ æ–°å†›.pdf ---\n",
      "  ğŸ“… Year (from filename): 2024\n",
      "  ğŸ·ï¸ Title (from filename): ã€Šä¸­å›½é«˜è¡€å‹é˜²æ²»æŒ‡å—ã€‹æ–°å¢å†…å®¹è§£è¯» ä»¥æ”¹å–„è¡€å‹å˜å¼‚å’Œé™å‹ç›®æ ‡èŒƒå›´å†…æ—¶é—´ä¸ºæ ¸å¿ƒçš„é«˜è´¨é‡é™å‹ç­–ç•¥æµ…æ\n",
      "  ğŸ‘¥ Authors (from filename): ['å¼ æ–°å†›']\n",
      "  ğŸ“‘ Type (from filename): guideline_interpretation\n",
      "  ğŸ”¤ Extracted 12125 characters.\n",
      "  âœï¸  Authors (final): ['å¼ æ–°å†›']\n",
      "  ğŸ“š Journal:  (), , \n",
      "  ğŸ”— DOI: 10.3969/j.issn.1007-5410.2024.05.002\n",
      "  ğŸ·ï¸ Keywords: ['é«˜è¡€å‹ è¡€å‹ç®¡ç† æŒ‡å— è§£è¯» è¡€å‹å˜å¼‚ ç›®æ ‡èŒƒå›´å†…æ—¶é—´']\n",
      "  ğŸ§­ Original Guideline: ä¸­å›½é«˜è¡€å‹é˜²æ²»æŒ‡å— (2024 å¹´ä¿®è®¢ç‰ˆ)\n",
      "  ğŸ“‘ Doc Type (final): guideline_interpretation\n",
      "  ğŸ§© Generated 81 chunks.\n",
      "\n",
      "--- ğŸ“„ Processing: ä¸­å›½2å‹ç³–å°¿ç—…é˜²æ²»æŒ‡å—ï¼ˆ2020å¹´ç‰ˆï¼‰.pdf ---\n",
      "  ğŸ“… Year (from filename): 2020\n",
      "  ğŸ·ï¸ Title (from filename): ä¸­å›½2å‹ç³–å°¿ç—…é˜²æ²»æŒ‡å—\n",
      "  ğŸ‘¥ Authors (from filename): []\n",
      "  ğŸ“‘ Type (from filename): guideline\n",
      "  ğŸ”¤ Extracted 329233 characters.\n",
      "  âœï¸  Authors (final): ['é€šä¿¡ä½œè€…ï¼šæœ±å¤§é¾™', 'å—äº¬å¤§å­¦åŒ»å­¦é™¢é™„å±é¼“æ¥¼åŒ»é™¢å†…åˆ†æ³Œç§‘']\n",
      "  ğŸ“š Journal:  (), , \n",
      "  ğŸ”— DOI: 10.3760/cma.j.cn115791-20210221-00095\n",
      "  ğŸ·ï¸ Keywords: ['ç³–å°¿ç—…', 'å‹', 'æŒ‡å—']\n",
      "  ğŸ§­ Original Guideline: ä¸­å›½ å‹ç³–å°¿ç—…é˜²æ²»æŒ‡å—ï¼ˆ å¹´ç‰ˆï¼‰\n",
      "  ğŸ“‘ Doc Type (final): guideline\n",
      "  ğŸ§© Generated 1631 chunks.\n",
      "\n",
      "--- ğŸ“„ Processing: å›½å®¶åŸºå±‚é«˜è¡€å‹é˜²æ²»ç®¡ç†æŒ‡å— 2020ç‰ˆ.pdf ---\n",
      "  ğŸ“… Year (from filename): 2020\n",
      "  ğŸ·ï¸ Title (from filename): å›½å®¶åŸºå±‚é«˜è¡€å‹é˜²æ²»ç®¡ç†æŒ‡å— 2020ç‰ˆ\n",
      "  ğŸ‘¥ Authors (from filename): []\n",
      "  ğŸ“‘ Type (from filename): guideline\n",
      "  ğŸ”¤ Extracted 24188 characters.\n",
      "  âœï¸  Authors (final): []\n",
      "  ğŸ“š Journal:  (), , \n",
      "  ğŸ”— DOI: \n",
      "  ğŸ·ï¸ Keywords: []\n",
      "  ğŸ§­ Original Guideline: \n",
      "  ğŸ“‘ Doc Type (final): guideline\n",
      "  ğŸ§© Generated 148 chunks.\n",
      "\n",
      "--- ğŸ“„ Processing: ä¸­å›½æ…¢æ€§è‚¾è„ç—…æ—©æœŸè¯„ä»·ä¸ç®¡ç†æŒ‡å—(2023).pdf ---\n",
      "  ğŸ“… Year (from filename): 2023\n",
      "  ğŸ·ï¸ Title (from filename): ä¸­å›½æ…¢æ€§è‚¾è„ç—…æ—©æœŸè¯„ä»·ä¸ç®¡ç†æŒ‡å—\n",
      "  ğŸ‘¥ Authors (from filename): []\n",
      "  ğŸ“‘ Type (from filename): guideline\n",
      "  ğŸ”¤ Extracted 98010 characters.\n",
      "  âœï¸  Authors (final): ['é€šä¿¡ä½œè€…ï¼šå¼ è·¯éœ', 'åŒ—äº¬å¤§å­¦å¥åº·åŒ»ç–—å¤§æ•°æ®å›½å®¶ç ”ç©¶é™¢', 'åŒ—äº¬']\n",
      "  ğŸ“š Journal:  (), , \n",
      "  ğŸ”— DOI: 10.3760/cma.j.cn112138-20221013-00755\n",
      "  ğŸ·ï¸ Keywords: ['æ…¢æ€§è‚¾è„ç—…', 'æŒ‡å—', 'è¯„ä»·', 'ç®¡ç†', 'é¢„é˜²']\n",
      "  ğŸ§­ Original Guideline: \n",
      "  ğŸ“‘ Doc Type (final): guideline\n",
      "  ğŸ§© Generated 412 chunks.\n",
      "\n",
      "--- ğŸ“„ Processing: æˆäººç³–å°¿ç—…æ‚£è€…è¡€å‹ç®¡ç†ä¸“å®¶å…±è¯†(2021).pdf ---\n",
      "  ğŸ“… Year (from filename): 2021\n",
      "  ğŸ·ï¸ Title (from filename): æˆäººç³–å°¿ç—…æ‚£è€…è¡€å‹ç®¡ç†ä¸“å®¶å…±è¯†\n",
      "  ğŸ‘¥ Authors (from filename): []\n",
      "  ğŸ“‘ Type (from filename): consensus\n",
      "  ğŸ”¤ Extracted 26954 characters.\n",
      "  âœï¸  Authors (final): []\n",
      "  ğŸ“š Journal:  (), ï¼˜, \n",
      "  ğŸ”— DOI: \n",
      "  ğŸ·ï¸ Keywords: ['ç³–å°¿ç—…', 'é«˜è¡€å‹', 'è¡€å‹', 'æ§åˆ¶ç›®æ ‡', 'æŠ—é«˜è¡€å‹è¯']\n",
      "  ğŸ§­ Original Guideline: \n",
      "  ğŸ“‘ Doc Type (final): consensus\n",
      "  ğŸ§© Generated 94 chunks.\n",
      "\n",
      "--- ğŸ“„ Processing: å›½å®¶åŸºå±‚ç³–å°¿ç—…é˜²æ²»ç®¡ç†æŒ‡å—ï¼ˆ2022ï¼‰.pdf ---\n",
      "  ğŸ“… Year (from filename): 2022\n",
      "  ğŸ·ï¸ Title (from filename): å›½å®¶åŸºå±‚ç³–å°¿ç—…é˜²æ²»ç®¡ç†æŒ‡å—\n",
      "  ğŸ‘¥ Authors (from filename): []\n",
      "  ğŸ“‘ Type (from filename): guideline\n",
      "  ğŸ”¤ Extracted 26924 characters.\n",
      "  âœï¸  Authors (final): ['åŒ»å­¦ä¸­å¿ƒä¸Šæµ·å¸‚ç³–å°¿ç—…é‡ç‚¹å®éªŒå®¤å›½å®¶åŸºå±‚ç³–å°¿ç—…é˜²æ²»ç®¡ç†åŠå…¬å®¤', 'ä¸Šæµ·']\n",
      "  ğŸ“š Journal:  (), , \n",
      "  ğŸ”— DOI: 10.3760/cma.j.cn112138-20220120-000063\n",
      "  ğŸ·ï¸ Keywords: ['ç³–å°¿ç—…', 'å‹', 'é˜²æ²»', 'åŸºå±‚å«ç”Ÿæœºæ„']\n",
      "  ğŸ§­ Original Guideline: å›½å®¶åŸºå±‚ç³–å°¿ç—…é˜²æ²»ç®¡ç†æŒ‡å—ï¼ˆ ï¼‰\n",
      "  ğŸ“‘ Doc Type (final): guideline\n",
      "  ğŸ§© Generated 203 chunks.\n",
      "\n",
      "--- ğŸ“„ Processing: ç³–å°¿ç—…æ‚£è€…ç”²ç—…ç®¡ç†çš„æœ€ä½³è¯æ®æ€»ç»“_é™ˆæ¬¢(2022).pdf ---\n",
      "  ğŸ“… Year (from filename): 2022\n",
      "  ğŸ·ï¸ Title (from filename): ç³–å°¿ç—…æ‚£è€…ç”²ç—…ç®¡ç†çš„æœ€ä½³è¯æ®æ€»ç»“\n",
      "  ğŸ‘¥ Authors (from filename): ['é™ˆæ¬¢']\n",
      "  ğŸ“‘ Type (from filename): evidence_summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-07 22:16:28,210 | WARNING  | Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ”¤ Extracted 22961 characters.\n",
      "  âœï¸  Authors (final): ['é™ˆæ¬¢']\n",
      "  ğŸ“š Journal:  (), 3984, \n",
      "  ğŸ”— DOI: 10.12114/j.issn.1007-9572.2022.0362\n",
      "  ğŸ·ï¸ Keywords: ['ç³–å°¿ç—…è¶³', 'ç³–å°¿ç—…', 'ç”²ç—…', 'æŒ‡ï¼ˆè¶¾ï¼‰ç”²', 'å¾ªè¯åŒ»å­¦', 'è¯æ®æ€»ç»“']\n",
      "  ğŸ§­ Original Guideline: \n",
      "  ğŸ“‘ Doc Type (final): evidence_summary\n",
      "  ğŸ§© Generated 65 chunks.\n",
      "\n",
      "--- ğŸ“„ Processing: 2å‹ç³–å°¿ç—…æ‚£è€…è¿åŠ¨æ–¹æ¡ˆçš„æœ€ä½³è¯æ®æ€»ç»“(2019).pdf ---\n",
      "  ğŸ“… Year (from filename): 2019\n",
      "  ğŸ·ï¸ Title (from filename): 2å‹ç³–å°¿ç—…æ‚£è€…è¿åŠ¨æ–¹æ¡ˆçš„æœ€ä½³è¯æ®æ€»ç»“\n",
      "  ğŸ‘¥ Authors (from filename): []\n",
      "  ğŸ“‘ Type (from filename): evidence_summary\n",
      "  ğŸ”¤ Extracted 15099 characters.\n",
      "  âœï¸  Authors (final): []\n",
      "  ğŸ“š Journal:  (), , \n",
      "  ğŸ”— DOI: 10\n",
      "  ğŸ·ï¸ Keywords: ['ç³–å°¿ç—…', '2å‹', 'è¿åŠ¨æ²»ç–—', 'ç”Ÿæ´»æ–¹å¼', 'å¾ªè¯æŠ¤ç†å­¦']\n",
      "  ğŸ§­ Original Guideline: \n",
      "  ğŸ“‘ Doc Type (final): evidence_summary\n",
      "  ğŸ§© Generated 47 chunks.\n",
      "\n",
      "--- ğŸ“„ Processing: ã€Šå¦Šå¨ æœŸç³–å°¿ç—…ä¸´åºŠæŠ¤ç†å®è·µæŒ‡å—ã€‹æ¨èæ„è§ä¸“å®¶å…±è¯†_å‘¨è‹±å‡¤(2020).pdf ---\n",
      "  ğŸ“… Year (from filename): 2020\n",
      "  ğŸ·ï¸ Title (from filename): ã€Šå¦Šå¨ æœŸç³–å°¿ç—…ä¸´åºŠæŠ¤ç†å®è·µæŒ‡å—ã€‹æ¨èæ„è§ä¸“å®¶å…±è¯†\n",
      "  ğŸ‘¥ Authors (from filename): ['å‘¨è‹±å‡¤']\n",
      "  ğŸ“‘ Type (from filename): guideline\n",
      "  ğŸ”¤ Extracted 16024 characters.\n",
      "  âœï¸  Authors (final): ['å‘¨è‹±å‡¤']\n",
      "  ğŸ“š Journal:  (), , \n",
      "  ğŸ”— DOI: R473.71A10.12102/j.issn.1009-6493.2020.24.001\n",
      "  ğŸ·ï¸ Keywords: []\n",
      "  ğŸ§­ Original Guideline: å¦Šå¨ æœŸç³–å°¿ç—…ä¸´åºŠæŠ¤ç†å®è·µæŒ‡å—\n",
      "  ğŸ“‘ Doc Type (final): guideline\n",
      "  ğŸ§© Generated 40 chunks.\n",
      "\n",
      "--- ğŸ“„ Processing: ä¸­å›½é«˜è¡€å‹é˜²æ²»æŒ‡å—(2024å¹´ä¿®è®¢ç‰ˆ).pdf ---\n",
      "  ğŸ“… Year (from filename): 2024\n",
      "  ğŸ·ï¸ Title (from filename): ä¸­å›½é«˜è¡€å‹é˜²æ²»æŒ‡å—\n",
      "  ğŸ‘¥ Authors (from filename): []\n",
      "  ğŸ“‘ Type (from filename): guideline\n",
      "  ğŸ”¤ Extracted 300858 characters.\n",
      "  âœï¸  Authors (final): []\n",
      "  ğŸ“š Journal:  (), , \n",
      "  ğŸ”— DOI: 10.16439/j.issn.1673-7245.2024.07.002\n",
      "  ğŸ·ï¸ Keywords: []\n",
      "  ğŸ§­ Original Guideline: \n",
      "  ğŸ“‘ Doc Type (final): guideline\n",
      "  ğŸ§© Generated 2238 chunks.\n",
      "\n",
      "--- ğŸ“„ Processing: ç³–å°¿ç—…æ‚£è€…ä½“é‡ç®¡ç†ä¸“å®¶å…±è¯†(2024ç‰ˆ).pdf ---\n",
      "  ğŸ“… Year (from filename): 2024\n",
      "  ğŸ·ï¸ Title (from filename): ç³–å°¿ç—…æ‚£è€…ä½“é‡ç®¡ç†ä¸“å®¶å…±è¯†\n",
      "  ğŸ‘¥ Authors (from filename): []\n",
      "  ğŸ“‘ Type (from filename): consensus\n",
      "  ğŸ”¤ Extracted 38020 characters.\n",
      "  âœï¸  Authors (final): []\n",
      "  ğŸ“š Journal:  (), , \n",
      "  ğŸ”— DOI: 10.3760/cma.j.cn115791-20240731-00396\n",
      "  ğŸ·ï¸ Keywords: ['ç³–å°¿ç—…', 'ä½“é‡ç®¡ç†', 'è¯„ä¼°', 'ç®¡ç†ç­–ç•¥', 'ä¸“å®¶å…±è¯†']\n",
      "  ğŸ§­ Original Guideline: \n",
      "  ğŸ“‘ Doc Type (final): consensus\n",
      "  ğŸ§© Generated 244 chunks.\n",
      "\n",
      "--- ğŸ“„ Processing: ç³–å°¿ç—…æ‚£è€…è¡€è„‚ç®¡ç†ä¸­å›½ä¸“å®¶å…±è¯†ï¼ˆ2024ç‰ˆï¼‰.pdf ---\n",
      "  ğŸ“… Year (from filename): 2024\n",
      "  ğŸ·ï¸ Title (from filename): ç³–å°¿ç—…æ‚£è€…è¡€è„‚ç®¡ç†ä¸­å›½ä¸“å®¶å…±è¯†\n",
      "  ğŸ‘¥ Authors (from filename): []\n",
      "  ğŸ“‘ Type (from filename): consensus\n",
      "  ğŸ”¤ Extracted 72407 characters.\n",
      "  âœï¸  Authors (final): []\n",
      "  ğŸ“š Journal:  (), , \n",
      "  ğŸ”— DOI: 10.3969/j.issn.1000-3614.2024.04.002\n",
      "  ğŸ·ï¸ Keywords: []\n",
      "  ğŸ§­ Original Guideline: \n",
      "  ğŸ“‘ Doc Type (final): consensus\n",
      "  ğŸ§© Generated 182 chunks.\n",
      "\n",
      "âœ… Output written to: /home/myunix/caremind/data/guidelines.parsed.jsonl\n",
      "ğŸ’¾ File size: 4531135 bytes\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # ğŸ“„ Medical Guideline PDF Parser v1.1\n",
    "# Enhanced to extract rich metadata from Chinese clinical guidelines and interpretation articles.\n",
    "# Distinguishes between official guidelines and expert interpretations.\n",
    "# Outputs: `guidelines.parsed.jsonl` with full bibliographic & structural metadata.\n",
    "\n",
    "# %%\n",
    "from pathlib import Path\n",
    "import pdfplumber \n",
    "import re\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# %%\n",
    "# =============================\n",
    "# ğŸ§© 1. METADATA EXTRACTION UTILS (from filename)\n",
    "# =============================\n",
    "\n",
    "def extract_year_from_filename(filename: str) -> str:\n",
    "    \"\"\"Extracts 4-digit year from Chinese medical guideline filenames.\"\"\"\n",
    "    patterns = [\n",
    "        r'[ï¼ˆ\\(]([12]\\d{3})[ï¼‰\\)]',\n",
    "        r'[ï¼ˆ\\(]([12]\\d{3})[å¹´\\s]*(?:ä¿®è®¢ç‰ˆ|ç‰ˆ|å¹´ç‰ˆ|å¹´)?[ï¼‰\\)]?',\n",
    "        r'([12]\\d{3})[å¹´\\s]*(?:ä¿®è®¢ç‰ˆ|ç‰ˆ|å¹´ç‰ˆ|å¹´)?(?=[\\s_ï¼‰\\)ã€‚\\.\\-]|$)',\n",
    "        r'[ï¼ˆ\\(]?([12]\\d{3})[ï¼‰\\)]?\\s*\\.pdf$',\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, filename)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    return \"unknown\"\n",
    "\n",
    "def extract_doc_title(filename: str) -> str:\n",
    "    \"\"\"Extract clean document title from filename.\"\"\"\n",
    "    base = re.sub(r\"_[^_]*\\.pdf$\", \"\", filename)\n",
    "    base = re.sub(r\"\\.pdf$\", \"\", base)\n",
    "    base = re.sub(r\"[ï¼ˆ\\(][^ï¼‰\\)]*[ï¼‰\\)]\", \"\", base)\n",
    "    base = re.sub(r\"\\s*â€”+\\s*\", \" \", base)\n",
    "    base = re.sub(r\"\\s+\", \" \", base).strip()\n",
    "    return base or \"æœªå‘½åæ–‡æ¡£\"\n",
    "\n",
    "def extract_authors_from_filename(filename: str) -> List[str]:\n",
    "    \"\"\"Extract author names from filename (after last underscore).\"\"\"\n",
    "    match = re.search(r\"_([^_\\(]+?)(?:\\([12]\\d{3}\\))?\\.pdf$\", filename)\n",
    "    if match:\n",
    "        author_str = match.group(1).strip()\n",
    "        authors = re.split(r\"[,ï¼Œã€]\", author_str)\n",
    "        return [a.strip() for a in authors if a.strip()]\n",
    "    return []\n",
    "\n",
    "def extract_doc_type_from_filename(filename: str) -> str:\n",
    "    \"\"\"Classify document type from filename.\"\"\"\n",
    "    if \"æŒ‡å—\" in filename and \"è§£è¯»\" not in filename:\n",
    "        return \"guideline\"\n",
    "    elif \"è§£è¯»\" in filename or \"æµ…æ\" in filename or \"è§£æ\" in filename:\n",
    "        return \"guideline_interpretation\"\n",
    "    elif \"å…±è¯†\" in filename:\n",
    "        return \"consensus\"\n",
    "    elif \"è¯æ®æ€»ç»“\" in filename:\n",
    "        return \"evidence_summary\"\n",
    "    else:\n",
    "        return \"other\"\n",
    "\n",
    "# %%\n",
    "# =============================\n",
    "# ğŸ“š 2. METADATA EXTRACTION UTILS (from document text â€” PREFERRED)\n",
    "# =============================\n",
    "\n",
    "def extract_metadata_from_text(text: str) -> Dict[str, Any]:\n",
    "    \"\"\"Extract rich metadata from first page of document text.\"\"\"\n",
    "    meta = {\n",
    "        \"authors\": [],\n",
    "        \"corresponding_author\": \"\",\n",
    "        \"affiliations\": [],\n",
    "        \"journal_name\": \"\",\n",
    "        \"volume\": \"\",\n",
    "        \"issue\": \"\",\n",
    "        \"pages\": \"\",\n",
    "        \"doi\": \"\",\n",
    "        \"keywords\": [],\n",
    "        \"publish_date\": \"\",\n",
    "        \"original_guideline_title\": \"\",\n",
    "        \"doc_type\": \"other\"  # Will be overridden if detected\n",
    "    }\n",
    "\n",
    "    # Extract author (first non-empty line after title, before postal code or affiliation)\n",
    "    lines = [ln.strip() for ln in text.splitlines() if ln.strip()] #\n",
    "    for i, line in enumerate(lines[:10]):  # Look in first 10 lines\n",
    "        if re.match(r\"^\\d{6}\", line):  # Postal code â†’ previous line is likely author\n",
    "            if i > 0:\n",
    "                author_line = lines[i-1]\n",
    "                authors = re.split(r\"[,ï¼Œã€]\", author_line)\n",
    "                meta[\"authors\"] = [a.strip() for a in authors if len(a.strip()) >= 2]\n",
    "            break\n",
    "        if \"é€šä¿¡ä½œè€…\" in line:\n",
    "            author_match = re.search(r\"é€šä¿¡ä½œè€…[:ï¼š]\\s*([^\\sï¼Œ,ã€]+)\", line)\n",
    "            if author_match:\n",
    "                meta[\"corresponding_author\"] = author_match.group(1).strip()\n",
    "                if not meta[\"authors\"]:\n",
    "                    meta[\"authors\"] = [meta[\"corresponding_author\"]]\n",
    "\n",
    "    # Extract affiliation (lines with postal code or university)\n",
    "    for line in lines[:15]:\n",
    "        if re.search(r\"\\d{6}|å¤§å­¦|åŒ»é™¢|ä¸­å¿ƒ\", line) and len(line) > 10:\n",
    "            meta[\"affiliations\"].append(line)\n",
    "\n",
    "    # Extract DOI\n",
    "    doi_match = re.search(r\"DOI\\s*[:ï¼š]?\\s*([0-9\\.\\s\\/a-z-]+)\", text, re.IGNORECASE)\n",
    "    if doi_match:\n",
    "        meta[\"doi\"] = re.sub(r\"\\s+\", \"\", doi_match.group(1)).strip()\n",
    "\n",
    "    # Extract journal, volume, issue, pages from footer pattern\n",
    "    # e.g., \"Â·396Â· ä¸­å›½å¿ƒè¡€ç®¡æ‚å¿— 2024å¹´ 10æœˆç¬¬ 29å·ç¬¬ 5æœŸ\"\n",
    "    journal_match = re.search(r\"Â·\\d+Â·\\s*([^\\s]+?æ‚å¿—|å­¦æŠ¥)\\s*(\\d{4})å¹´\\s*\\d+æœˆç¬¬\\s*(\\d+)å·ç¬¬\\s*(\\d+)æœŸ\", text)\n",
    "    if journal_match:\n",
    "        meta[\"journal_name\"] = journal_match.group(1).strip()\n",
    "        meta[\"publish_date\"] = journal_match.group(2).strip()  # e.g., \"2024\"\n",
    "        meta[\"volume\"] = journal_match.group(3).strip()\n",
    "        meta[\"issue\"] = journal_match.group(4).strip()\n",
    "\n",
    "    # Extract pages from header/footer (e.g., \"Â·396Â·\")\n",
    "    page_match = re.search(r\"Â·(\\d+)Â·\", text.splitlines()[0] if text.splitlines() else \"\")\n",
    "    if page_match:\n",
    "        start_page = page_match.group(1)\n",
    "        # Try to find end page (often not available, so leave as single page)\n",
    "        meta[\"pages\"] = start_page\n",
    "\n",
    "    # Extract keywords\n",
    "    kw_match = re.search(r\"ã€å…³é”®è¯ã€‘\\s*([^\\nã€ã€‘]+)\", text)\n",
    "    if kw_match:\n",
    "        kw_str = kw_match.group(1).strip()\n",
    "        meta[\"keywords\"] = [k.strip() for k in re.split(r\"[,ï¼Œ;ï¼›ã€]\", kw_str) if k.strip()]\n",
    "\n",
    "    # Detect if this is an interpretation of a guideline\n",
    "    guideline_ref_match = re.search(r\"ã€Š([^ã€‹]+?æŒ‡å—[^ã€‹]*)ã€‹\", text[:500])\n",
    "    if guideline_ref_match:\n",
    "        meta[\"original_guideline_title\"] = guideline_ref_match.group(1).strip()\n",
    "        if \"è§£è¯»\" in text[:200] or \"æµ…æ\" in text[:200]:\n",
    "            meta[\"doc_type\"] = \"guideline_interpretation\"\n",
    "\n",
    "    # If no authors found but filename has them, fallback\n",
    "    # (Handled in main function)\n",
    "\n",
    "    return meta\n",
    "\n",
    "# %%\n",
    "# =============================\n",
    "# ğŸ“„ 3. TEXT EXTRACTION\n",
    "# =============================\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: Path) -> str:\n",
    "    \"\"\"Extract text from PDF using pdfplumber.\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(str(pdf_path)) as pdf: \n",
    "            pages = [p.extract_text() or \"\" for p in pdf.pages] \n",
    "        return \"\\n\".join(pages)\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  PDF extraction error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# %%\n",
    "# =============================\n",
    "# ğŸ§± 4. CHUNKING LOGIC\n",
    "# =============================\n",
    "\n",
    "def chunk_by_rules(\n",
    "    text: str,\n",
    "    source_filename: str,\n",
    "    year: str,\n",
    "    doc_title: str,\n",
    "    authors: List[str],\n",
    "    doc_type: str,\n",
    "    original_guideline_title: str = \"\",\n",
    "    journal_name: str = \"\",\n",
    "    volume: str = \"\",\n",
    "    issue: str = \"\",\n",
    "    pages: str = \"\",\n",
    "    doi: str = \"\",\n",
    "    keywords: List[str] = [],\n",
    "    publish_date: str = \"\"\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Split text into chunks by section titles, with rich metadata.\"\"\"\n",
    "    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n",
    "    if not lines:\n",
    "        return []\n",
    "\n",
    "    chunks, buf = [], []\n",
    "    current_title = \"æœªå‘½åç« èŠ‚\"\n",
    "\n",
    "    TITLE_KEYWORDS = [\n",
    "        \"ç« \", \"èŠ‚\", \"ç¯‡\", \"éƒ¨åˆ†\", \"æ¦‚è¿°\", \"èƒŒæ™¯\", \"ç›®çš„\", \"æ–¹æ³•\", \"ç»“æœ\", \"ç»“è®º\",\n",
    "        \"æ¨è\", \"å»ºè®®\", \"ç®¡ç†\", \"æ²»ç–—\", \"è¯Šæ–­\", \"è¯„ä¼°\", \"å®šä¹‰\", \"ç›®æ ‡\",\n",
    "        \"ä¸€ã€\", \"äºŒã€\", \"ä¸‰ã€\", \"å››ã€\", \"äº”ã€\", \"å…­ã€\", \"ä¸ƒã€\", \"å…«ã€\", \"ä¹ã€\", \"åã€\",\n",
    "        \"1.\", \"2.\", \"3.\", \"4.\", \"5.\", \"6.\", \"7.\", \"8.\", \"9.\", \"10.\",\n",
    "        \"ç¬¬ä¸€\", \"ç¬¬äºŒ\", \"ç¬¬ä¸‰\", \"ç¬¬å››\", \"ç¬¬äº”\",\n",
    "        \"ã€\", \"ã€‘\", \"ï¼ˆ\", \"ï¼‰\", \"(\", \")\", \"ï¼š\", \":\"\n",
    "    ]\n",
    "\n",
    "    for ln in lines:\n",
    "        is_title = False\n",
    "\n",
    "        if any(kw in ln for kw in TITLE_KEYWORDS) and 3 <= len(ln) <= 100:\n",
    "            is_title = True\n",
    "        elif ln.endswith(\"ï¼š\") or ln.endswith(\":\") or \\\n",
    "             (len(ln) <= 50 and (ln.startswith(\"ã€\") and ln.endswith(\"ã€‘\"))):\n",
    "            is_title = True\n",
    "        elif 3 <= len(ln) <= 25 and not ln.endswith(\"ã€‚\") and not ln.endswith(\".\"):\n",
    "            is_title = True\n",
    "        elif re.match(r\"^[0-9]+[\\.ã€]\\s*\\S{3,}\", ln):\n",
    "            is_title = True\n",
    "\n",
    "        if is_title:\n",
    "            if buf:\n",
    "                chunk_id = f\"{re.sub(r'[^a-zA-Z0-9]', '_', doc_title)}_{year}_{len(chunks):03d}\"\n",
    "                chunk_meta = {\n",
    "                    \"source_filename\": source_filename,\n",
    "                    \"doc_title\": doc_title,\n",
    "                    \"section_title\": current_title,\n",
    "                    \"authors\": authors,\n",
    "                    \"year\": year,\n",
    "                    \"doc_type\": doc_type,\n",
    "                    \"original_guideline_title\": original_guideline_title,\n",
    "                    \"journal_name\": journal_name,\n",
    "                    \"volume\": volume,\n",
    "                    \"issue\": issue,\n",
    "                    \"pages\": pages,\n",
    "                    \"doi\": doi,\n",
    "                    \"keywords\": keywords,\n",
    "                    \"publish_date\": publish_date,\n",
    "                    \"chunk_id\": chunk_id,\n",
    "                    \"extraction_method\": \"pdfplumber + rule-based + metadata extraction\"\n",
    "                }\n",
    "                chunks.append({\n",
    "                    \"content\": \"\\n\".join(buf),\n",
    "                    \"meta\": chunk_meta\n",
    "                })\n",
    "                buf = []\n",
    "            current_title = ln\n",
    "        else:\n",
    "            buf.append(ln)\n",
    "\n",
    "    # Flush final buffer\n",
    "    if buf:\n",
    "        chunk_id = f\"{re.sub(r'[^a-zA-Z0-9]', '_', doc_title)}_{year}_{len(chunks):03d}\"\n",
    "        chunk_meta = {\n",
    "            \"source_filename\": source_filename,\n",
    "            \"doc_title\": doc_title,\n",
    "            \"section_title\": current_title,\n",
    "            \"authors\": authors,\n",
    "            \"year\": year,\n",
    "            \"doc_type\": doc_type,\n",
    "            \"original_guideline_title\": original_guideline_title,\n",
    "            \"journal_name\": journal_name,\n",
    "            \"volume\": volume,\n",
    "            \"issue\": issue,\n",
    "            \"pages\": pages,\n",
    "            \"doi\": doi,\n",
    "            \"keywords\": keywords,\n",
    "            \"publish_date\": publish_date,\n",
    "            \"chunk_id\": chunk_id,\n",
    "            \"extraction_method\": \"pdfplumber + rule-based + metadata extraction\"\n",
    "        }\n",
    "        chunks.append({\n",
    "            \"content\": \"\\n\".join(buf),\n",
    "            \"meta\": chunk_meta\n",
    "        })\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# %%\n",
    "# =============================\n",
    "# ğŸš€ 5. MAIN PROCESSING PIPELINE\n",
    "# =============================\n",
    "\n",
    "def main():\n",
    "    in_dir = Path(\"data/guidelines\")\n",
    "    out_path = Path(\"data/guidelines.parsed.jsonl\")\n",
    "\n",
    "    print(f\"ğŸ“‚ Input directory: {in_dir.absolute()}\")\n",
    "    pdf_files = list(in_dir.glob(\"*.pdf\"))\n",
    "    print(f\"ğŸ“„ Found {len(pdf_files)} PDF files.\")\n",
    "\n",
    "    if not pdf_files:\n",
    "        print(\"âŒ No PDFs found. Check directory path.\")\n",
    "        return\n",
    "\n",
    "    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for pdf in pdf_files:\n",
    "            print(f\"\\n--- ğŸ“„ Processing: {pdf.name} ---\")\n",
    "\n",
    "            # Step 1: Extract preliminary metadata from filename\n",
    "            year = extract_year_from_filename(pdf.name)\n",
    "            doc_title = extract_doc_title(pdf.name)\n",
    "            authors_filename = extract_authors_from_filename(pdf.name)\n",
    "            doc_type = extract_doc_type_from_filename(pdf.name)\n",
    "\n",
    "            print(f\"  ğŸ“… Year (from filename): {year}\")\n",
    "            print(f\"  ğŸ·ï¸ Title (from filename): {doc_title}\")\n",
    "            print(f\"  ğŸ‘¥ Authors (from filename): {authors_filename}\")\n",
    "            print(f\"  ğŸ“‘ Type (from filename): {doc_type}\")\n",
    "\n",
    "            # Step 2: Extract text\n",
    "            text = extract_text_from_pdf(pdf)\n",
    "            char_count = len(text.strip())\n",
    "            print(f\"  ğŸ”¤ Extracted {char_count} characters.\")\n",
    "\n",
    "            if char_count == 0:\n",
    "                print(\"  âš ï¸  WARNING: No text extracted. File may be scanned.\")\n",
    "                continue\n",
    "\n",
    "            # Step 3: Extract rich metadata from text (overrides filename where possible)\n",
    "            text_meta = extract_metadata_from_text(text)\n",
    "\n",
    "            # Merge: Prefer text-extracted metadata, fallback to filename\n",
    "            authors = text_meta[\"authors\"] if text_meta[\"authors\"] else authors_filename\n",
    "            doc_type_final = text_meta[\"doc_type\"] if text_meta[\"doc_type\"] != \"other\" else doc_type\n",
    "            original_guideline_title = text_meta[\"original_guideline_title\"]\n",
    "            journal_name = text_meta[\"journal_name\"]\n",
    "            volume = text_meta[\"volume\"]\n",
    "            issue = text_meta[\"issue\"]\n",
    "            pages = text_meta[\"pages\"]\n",
    "            doi = text_meta[\"doi\"]\n",
    "            keywords = text_meta[\"keywords\"]\n",
    "            publish_date = text_meta[\"publish_date\"]\n",
    "\n",
    "            print(f\"  âœï¸  Authors (final): {authors}\")\n",
    "            print(f\"  ğŸ“š Journal: {journal_name} {volume}({issue}), {pages}, {publish_date}\")\n",
    "            print(f\"  ğŸ”— DOI: {doi}\")\n",
    "            print(f\"  ğŸ·ï¸ Keywords: {keywords}\")\n",
    "            print(f\"  ğŸ§­ Original Guideline: {original_guideline_title}\")\n",
    "            print(f\"  ğŸ“‘ Doc Type (final): {doc_type_final}\")\n",
    "\n",
    "            # Step 4: Chunk text with full metadata\n",
    "            chunks = chunk_by_rules(\n",
    "                text=text,\n",
    "                source_filename=pdf.name,\n",
    "                year=year,\n",
    "                doc_title=doc_title,\n",
    "                authors=authors,\n",
    "                doc_type=doc_type_final,\n",
    "                original_guideline_title=original_guideline_title,\n",
    "                journal_name=journal_name,\n",
    "                volume=volume,\n",
    "                issue=issue,\n",
    "                pages=pages,\n",
    "                doi=doi,\n",
    "                keywords=keywords,\n",
    "                publish_date=publish_date\n",
    "            )\n",
    "            print(f\"  ğŸ§© Generated {len(chunks)} chunks.\")\n",
    "\n",
    "            if not chunks:\n",
    "                print(\"  âŒ No chunks generated. Check chunking logic.\")\n",
    "\n",
    "            # Step 5: Write to JSONL\n",
    "            for chunk in chunks:\n",
    "                f.write(json.dumps(chunk, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"\\nâœ… Output written to: {out_path.absolute()}\")\n",
    "    print(f\"ğŸ’¾ File size: {out_path.stat().st_size} bytes\")\n",
    "\n",
    "# %%\n",
    "# =============================\n",
    "# â–¶ï¸ 6. RUN\n",
    "# =============================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e194928",
   "metadata": {},
   "source": [
    "### å‘é‡åŒ–ä¸å†™å…¥ Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e950af01",
   "metadata": {},
   "source": [
    "#### ğŸ“‘ Embedding Chinese Guideline Chunks into ChromaDB\n",
    "\n",
    "This notebook cell runs a **robust embedding pipeline** designed to convert parsed\n",
    "Chinese medical guideline chunks into vector representations and store them in a\n",
    "**persistent ChromaDB collection** for downstream **RAG (Retrieval-Augmented Generation)**\n",
    "applications.\n",
    "\n",
    "##### ğŸ” What this program does\n",
    "- **Stream input**: Reads a JSONL file (`guidelines.parsed.jsonl`), where each line\n",
    "  contains a `content` field (text chunk) and a `meta` field (metadata such as source,\n",
    "  year, section, authors, etc.).\n",
    "- **Embed text**: Uses a **Chinese-capable SentenceTransformer** model\n",
    "  (default: `BAAI/bge-small-zh`) to convert text into dense embeddings.\n",
    "- **Store vectors**: Saves embeddings, documents, and sanitized metadata into a\n",
    "  **ChromaDB collection** on disk for fast similarity search and retrieval.\n",
    "\n",
    "##### âš™ï¸ Key features and safeguards\n",
    "- **OOM resilience (RTX 4070 friendly)**:\n",
    "  - Dynamic batch-size backoff (halves batch size if VRAM runs out).\n",
    "  - FP16 autocast and lowered max sequence length to reduce memory use.\n",
    "  - Optional CPU fallback if GPU memory is completely exhausted.\n",
    "- **Metadata safety**:\n",
    "  - Converts lists (e.g., `[\"ç§¦ç…œ\"]`) and dicts into scalar or JSON-safe forms.\n",
    "  - Prevents `ValueError: Expected metadata value to be a str, int, float, bool, or None`.\n",
    "- **Duplicate ID protection**:\n",
    "  - Generates **stable, low-collision IDs** from source + chunk_id + content hash.\n",
    "  - Removes duplicate IDs **inside each batch**.\n",
    "  - Uses **upsert** (or add+update fallback) so re-runs are **idempotent**.\n",
    "- **Progress and monitoring**:\n",
    "  - Optional progress bars with `tqdm`.\n",
    "  - Periodic VRAM usage reports (on CUDA).\n",
    "\n",
    "##### ğŸ“¥ Input format (one JSON object per line)\n",
    "```json\n",
    "{\"content\": \"æŸæ®µä¸­æ–‡æ–‡æœ¬â€¦\", \"meta\": {\n",
    "  \"source\": \"ä¸­å›½é«˜è¡€å‹é˜²æ²»æŒ‡å—(2024å¹´ä¿®è®¢ç‰ˆ).pdf\",\n",
    "  \"year\": 2024,\n",
    "  \"section\": \"2.1 å®šä¹‰\",\n",
    "  \"chunk_id\": 12,\n",
    "  \"authors\": [\"ç§¦ç…œ\",\"å¼ ä¸‰\"]\n",
    "}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96d6c839",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:   0%|          | 0/5908 [00:00<?, ?it/s]/tmp/ipykernel_25240/2454140094.py:224: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.float16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ Persist dir: ./chroma_store\n",
      "ğŸ—ƒï¸  Collection: guideline_chunks\n",
      "ğŸ§  Model: BAAI/bge-large-zh-v1.5 | Device: cuda | fp16: True | max_seq_len: 384\n",
      "ğŸ“‘ Input: /home/myunix/caremind/data/guidelines.parsed.jsonl\n",
      "âš™ï¸  Start batch size: 16 | CPU fallback: True\n",
      "[VRAM] start used=2.42 GB / total=11.99 GB (free=9.57 GB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:   1%|          | 32/5908 [00:01<02:42, 36.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=16)\n",
      "âœ… Flushed 16 items (total=32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:   1%|          | 64/5908 [00:01<01:33, 62.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 15 items (total=47)\n",
      "[dedupe] removed 4 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 12 items (total=59)\n",
      "[dedupe] removed 3 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:   2%|â–         | 96/5908 [00:01<01:04, 89.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 13 items (total=72)\n",
      "[dedupe] removed 3 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 13 items (total=85)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:   2%|â–         | 112/5908 [00:01<00:57, 100.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=101)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:   2%|â–         | 144/5908 [00:02<01:01, 93.71it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=117)\n",
      "âœ… Flushed 16 items (total=133)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:   3%|â–         | 176/5908 [00:02<00:52, 109.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 14 items (total=147)\n",
      "âœ… Flushed 16 items (total=163)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:   4%|â–         | 208/5908 [00:02<00:49, 115.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=179)\n",
      "âœ… Flushed 16 items (total=195)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:   4%|â–         | 240/5908 [00:02<00:51, 110.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 14 items (total=209)\n",
      "âœ… Flushed 16 items (total=225)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:   5%|â–         | 272/5908 [00:03<00:50, 112.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 15 items (total=240)\n",
      "âœ… Flushed 16 items (total=256)\n",
      "[VRAM] after 256 used=2.49 GB / total=11.99 GB (free=9.50 GB)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:   5%|â–Œ         | 304/5908 [00:03<00:47, 119.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 15 items (total=271)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 15 items (total=286)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:   6%|â–Œ         | 336/5908 [00:03<00:45, 122.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 14 items (total=300)\n",
      "[dedupe] removed 7 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 9 items (total=309)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:   6%|â–Œ         | 368/5908 [00:03<00:47, 117.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 15 items (total=324)\n",
      "âœ… Flushed 16 items (total=340)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:   7%|â–‹         | 400/5908 [00:04<00:46, 117.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 15 items (total=355)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 15 items (total=370)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:   7%|â–‹         | 432/5908 [00:04<00:46, 116.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=386)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 15 items (total=401)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:   8%|â–Š         | 464/5908 [00:04<00:47, 114.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=417)\n",
      "âœ… Flushed 16 items (total=433)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:   8%|â–Š         | 496/5908 [00:05<00:51, 105.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=449)\n",
      "âœ… Flushed 16 items (total=465)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:   9%|â–‰         | 528/5908 [00:05<00:56, 95.67it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=481)\n",
      "âœ… Flushed 16 items (total=497)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:   9%|â–‰         | 560/5908 [00:05<00:52, 101.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 15 items (total=512)\n",
      "[VRAM] after 512 used=2.49 GB / total=11.99 GB (free=9.50 GB)\n",
      "âœ… Flushed 16 items (total=528)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  10%|â–‰         | 576/5908 [00:05<00:52, 101.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 15 items (total=543)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  10%|â–ˆ         | 608/5908 [00:06<00:59, 89.38it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 15 items (total=558)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 15 items (total=573)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  11%|â–ˆ         | 640/5908 [00:06<01:02, 83.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 14 items (total=587)\n",
      "[dedupe] removed 3 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 13 items (total=600)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  11%|â–ˆ         | 656/5908 [00:06<00:59, 87.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 4 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 12 items (total=612)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  12%|â–ˆâ–        | 688/5908 [00:07<01:02, 83.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 14 items (total=626)\n",
      "âœ… Flushed 16 items (total=642)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  12%|â–ˆâ–        | 720/5908 [00:07<01:04, 80.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 14 items (total=656)\n",
      "âœ… Flushed 16 items (total=672)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  13%|â–ˆâ–        | 752/5908 [00:08<01:03, 81.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=688)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 15 items (total=703)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  13%|â–ˆâ–        | 784/5908 [00:08<01:03, 80.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=719)\n",
      "[dedupe] removed 3 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 13 items (total=732)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  14%|â–ˆâ–        | 800/5908 [00:08<01:03, 81.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=748)\n",
      "[dedupe] removed 5 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  14%|â–ˆâ–        | 832/5908 [00:09<01:02, 80.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 11 items (total=759)\n",
      "âœ… Flushed 16 items (total=775)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  15%|â–ˆâ–        | 864/5908 [00:09<01:04, 78.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=791)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 15 items (total=806)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  15%|â–ˆâ–        | 880/5908 [00:09<01:05, 77.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=822)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  15%|â–ˆâ–Œ        | 896/5908 [00:10<01:07, 73.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 14 items (total=836)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  15%|â–ˆâ–Œ        | 912/5908 [00:10<01:10, 70.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 15 items (total=851)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  16%|â–ˆâ–Œ        | 944/5908 [00:10<01:08, 72.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 15 items (total=866)\n",
      "âœ… Flushed 16 items (total=882)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  17%|â–ˆâ–‹        | 976/5908 [00:11<01:06, 73.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 3 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 13 items (total=895)\n",
      "âœ… Flushed 16 items (total=911)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  17%|â–ˆâ–‹        | 992/5908 [00:11<01:04, 76.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=927)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  17%|â–ˆâ–‹        | 1008/5908 [00:11<01:03, 77.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=943)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  17%|â–ˆâ–‹        | 1024/5908 [00:11<01:04, 75.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 15 items (total=958)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  18%|â–ˆâ–Š        | 1056/5908 [00:12<01:03, 76.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 14 items (total=972)\n",
      "[dedupe] removed 4 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 12 items (total=984)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  18%|â–ˆâ–Š        | 1088/5908 [00:12<00:52, 92.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=1000)\n",
      "âœ… Flushed 16 items (total=1016)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  19%|â–ˆâ–‰        | 1120/5908 [00:12<00:47, 101.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=1032)\n",
      "[dedupe] removed 4 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 12 items (total=1044)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  19%|â–ˆâ–‰        | 1152/5908 [00:13<00:43, 108.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=1060)\n",
      "âœ… Flushed 16 items (total=1076)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  20%|â–ˆâ–ˆ        | 1184/5908 [00:13<00:48, 98.04it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=1092)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 14 items (total=1106)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  21%|â–ˆâ–ˆ        | 1216/5908 [00:13<00:50, 93.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 4 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 12 items (total=1118)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 15 items (total=1133)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  21%|â–ˆâ–ˆ        | 1248/5908 [00:14<00:46, 100.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 14 items (total=1147)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 15 items (total=1162)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  22%|â–ˆâ–ˆâ–       | 1280/5908 [00:14<00:45, 101.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 15 items (total=1177)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 15 items (total=1192)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  22%|â–ˆâ–ˆâ–       | 1312/5908 [00:14<00:43, 105.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 14 items (total=1206)\n",
      "[dedupe] removed 4 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 12 items (total=1218)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  23%|â–ˆâ–ˆâ–       | 1344/5908 [00:15<00:43, 104.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 6 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 10 items (total=1228)\n",
      "âœ… Flushed 16 items (total=1244)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  23%|â–ˆâ–ˆâ–       | 1376/5908 [00:15<00:43, 103.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 14 items (total=1258)\n",
      "âœ… Flushed 16 items (total=1274)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  24%|â–ˆâ–ˆâ–       | 1408/5908 [00:15<00:45, 99.19it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 14 items (total=1288)\n",
      "âœ… Flushed 16 items (total=1304)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  24%|â–ˆâ–ˆâ–       | 1440/5908 [00:16<00:42, 104.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 3 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 13 items (total=1317)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 14 items (total=1331)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  25%|â–ˆâ–ˆâ–       | 1472/5908 [00:16<00:44, 98.78it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 5 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 11 items (total=1342)\n",
      "[dedupe] removed 3 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 13 items (total=1355)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  25%|â–ˆâ–ˆâ–Œ       | 1488/5908 [00:16<00:41, 105.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=1371)\n",
      "[dedupe] removed 3 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  26%|â–ˆâ–ˆâ–Œ       | 1520/5908 [00:16<00:43, 101.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 13 items (total=1384)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 15 items (total=1399)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  26%|â–ˆâ–ˆâ–‹       | 1552/5908 [00:17<00:45, 95.78it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=1415)\n",
      "[dedupe] removed 3 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 13 items (total=1428)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  27%|â–ˆâ–ˆâ–‹       | 1584/5908 [00:17<00:48, 89.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 3 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 13 items (total=1441)\n",
      "âœ… Flushed 16 items (total=1457)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  27%|â–ˆâ–ˆâ–‹       | 1616/5908 [00:17<00:48, 88.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=1473)\n",
      "âœ… Flushed 16 items (total=1489)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  28%|â–ˆâ–ˆâ–Š       | 1648/5908 [00:18<00:45, 93.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 15 items (total=1504)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 15 items (total=1519)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  28%|â–ˆâ–ˆâ–Š       | 1664/5908 [00:18<00:42, 99.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 14 items (total=1533)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  29%|â–ˆâ–ˆâ–Š       | 1696/5908 [00:18<00:44, 94.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 15 items (total=1548)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 14 items (total=1562)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  29%|â–ˆâ–ˆâ–‰       | 1712/5908 [00:18<00:44, 93.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 15 items (total=1577)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  30%|â–ˆâ–ˆâ–‰       | 1744/5908 [00:19<00:46, 90.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=1593)\n",
      "âœ… Flushed 16 items (total=1609)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  30%|â–ˆâ–ˆâ–ˆ       | 1776/5908 [00:19<00:42, 97.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=1625)\n",
      "âœ… Flushed 16 items (total=1641)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  31%|â–ˆâ–ˆâ–ˆ       | 1808/5908 [00:19<00:41, 99.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=1657)\n",
      "âœ… Flushed 16 items (total=1673)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  31%|â–ˆâ–ˆâ–ˆ       | 1840/5908 [00:20<00:41, 96.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=1689)\n",
      "âœ… Flushed 16 items (total=1705)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  32%|â–ˆâ–ˆâ–ˆâ–      | 1872/5908 [00:20<00:44, 90.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=1721)\n",
      "âœ… Flushed 16 items (total=1737)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  32%|â–ˆâ–ˆâ–ˆâ–      | 1888/5908 [00:20<00:44, 90.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=1753)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  32%|â–ˆâ–ˆâ–ˆâ–      | 1904/5908 [00:21<00:49, 81.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=1769)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  32%|â–ˆâ–ˆâ–ˆâ–      | 1920/5908 [00:21<00:49, 80.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=1785)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  32%|â–ˆâ–ˆâ–ˆâ–      | 1920/5908 [00:20<00:49, 80.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=1801)\n",
      "âœ… Flushed 16 items (total=1817)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  32%|â–ˆâ–ˆâ–ˆâ–      | 1920/5908 [00:21<00:49, 80.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=1833)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  34%|â–ˆâ–ˆâ–ˆâ–      | 2000/5908 [00:21<00:23, 163.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=1849)\n",
      "âœ… Flushed 16 items (total=1865)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  34%|â–ˆâ–ˆâ–ˆâ–      | 2032/5908 [00:21<00:32, 120.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=1881)\n",
      "âœ… Flushed 16 items (total=1897)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  35%|â–ˆâ–ˆâ–ˆâ–      | 2048/5908 [00:22<00:37, 103.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=1913)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 2080/5908 [00:22<00:43, 88.80it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=1929)\n",
      "âœ… Flushed 16 items (total=1945)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 2096/5908 [00:22<00:43, 86.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=1961)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 2128/5908 [00:23<00:44, 85.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=1977)\n",
      "âœ… Flushed 16 items (total=1993)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 2160/5908 [00:23<00:34, 107.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=2009)\n",
      "âœ… Flushed 16 items (total=2025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 2192/5908 [00:23<00:30, 120.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=2041)\n",
      "âœ… Flushed 16 items (total=2057)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 2224/5908 [00:23<00:30, 119.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=2073)\n",
      "âœ… Flushed 16 items (total=2089)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 2256/5908 [00:24<00:35, 103.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=2105)\n",
      "âœ… Flushed 16 items (total=2121)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 2288/5908 [00:24<00:30, 120.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=2137)\n",
      "âœ… Flushed 16 items (total=2153)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 2320/5908 [00:24<00:28, 126.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=2169)\n",
      "âœ… Flushed 16 items (total=2185)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 2352/5908 [00:24<00:28, 124.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=2201)\n",
      "âœ… Flushed 16 items (total=2217)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2384/5908 [00:25<00:26, 131.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=2233)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 14 items (total=2247)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2416/5908 [00:25<00:27, 127.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=2263)\n",
      "âœ… Flushed 16 items (total=2279)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2448/5908 [00:25<00:27, 127.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 14 items (total=2293)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 15 items (total=2308)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2480/5908 [00:25<00:26, 128.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=2324)\n",
      "âœ… Flushed 16 items (total=2340)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2512/5908 [00:26<00:27, 125.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=2356)\n",
      "âœ… Flushed 16 items (total=2372)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2544/5908 [00:26<00:27, 123.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=2388)\n",
      "âœ… Flushed 16 items (total=2404)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2576/5908 [00:26<00:27, 121.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=2420)\n",
      "âœ… Flushed 16 items (total=2436)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2608/5908 [00:27<00:27, 119.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=2452)\n",
      "âœ… Flushed 16 items (total=2468)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2640/5908 [00:27<00:27, 117.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=2484)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 15 items (total=2499)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2672/5908 [00:27<00:29, 110.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=2515)\n",
      "âœ… Flushed 16 items (total=2531)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2688/5908 [00:27<00:30, 105.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=2547)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2720/5908 [00:28<00:35, 91.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=2563)\n",
      "âœ… Flushed 16 items (total=2579)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2736/5908 [00:28<00:35, 88.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=2595)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2752/5908 [00:28<00:36, 85.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=2611)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2784/5908 [00:28<00:37, 82.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=2627)\n",
      "âœ… Flushed 16 items (total=2643)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2816/5908 [00:29<00:37, 82.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=2659)\n",
      "âœ… Flushed 16 items (total=2675)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2832/5908 [00:29<00:37, 82.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=2691)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2848/5908 [00:29<00:37, 80.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=2707)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2864/5908 [00:30<00:46, 64.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=2723)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2880/5908 [00:30<00:49, 61.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=2739)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2896/5908 [00:30<00:52, 57.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 15 items (total=2754)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2912/5908 [00:31<00:54, 54.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=2770)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2928/5908 [00:31<00:53, 55.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=2786)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2944/5908 [00:31<00:52, 56.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=2802)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2960/5908 [00:31<00:49, 58.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 14 items (total=2816)\n",
      "[VRAM] after 2816 used=2.49 GB / total=11.99 GB (free=9.50 GB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2976/5908 [00:32<00:49, 59.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=2832)\n",
      "[dedupe] removed 3 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2992/5908 [00:32<00:46, 62.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 13 items (total=2845)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3008/5908 [00:32<00:46, 62.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 15 items (total=2860)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3024/5908 [00:32<00:47, 61.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 15 items (total=2875)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 3040/5908 [00:33<00:47, 59.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 14 items (total=2889)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 3056/5908 [00:33<00:50, 57.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 14 items (total=2903)\n",
      "[dedupe] removed 6 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 3072/5908 [00:33<00:49, 57.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 10 items (total=2913)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 3088/5908 [00:34<00:49, 56.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 15 items (total=2928)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 3104/5908 [00:34<01:01, 45.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=2944)\n",
      "[VRAM] after 2944 used=2.49 GB / total=11.99 GB (free=9.50 GB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 3120/5908 [00:34<00:58, 48.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=2960)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 3136/5908 [00:35<00:56, 49.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=2976)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 3152/5908 [00:35<01:02, 44.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=2992)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 3168/5908 [00:35<00:57, 47.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=3008)\n",
      "[VRAM] after 3008 used=2.49 GB / total=11.99 GB (free=9.50 GB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 3184/5908 [00:36<00:51, 52.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=3024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 3200/5908 [00:36<00:46, 58.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=3040)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 3232/5908 [00:36<00:40, 66.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=3056)\n",
      "âœ… Flushed 16 items (total=3072)\n",
      "[VRAM] after 3072 used=2.49 GB / total=11.99 GB (free=9.50 GB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 3248/5908 [00:36<00:40, 65.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=3088)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 3264/5908 [00:37<00:38, 67.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=3104)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 3296/5908 [00:37<00:35, 72.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 14 items (total=3118)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 15 items (total=3133)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 3312/5908 [00:37<00:36, 70.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 15 items (total=3148)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 3344/5908 [00:38<00:34, 73.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=3164)\n",
      "âœ… Flushed 16 items (total=3180)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 3360/5908 [00:38<00:34, 72.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 3 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 13 items (total=3193)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 3376/5908 [00:38<00:33, 74.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=3209)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 3392/5908 [00:38<00:33, 74.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=3225)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3408/5908 [00:39<00:33, 73.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=3241)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3424/5908 [00:39<00:35, 70.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=3257)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3440/5908 [00:39<00:37, 66.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 15 items (total=3272)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3456/5908 [00:39<00:37, 64.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=3288)\n",
      "[dedupe] removed 3 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3472/5908 [00:40<00:35, 67.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 13 items (total=3301)\n",
      "[dedupe] removed 3 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3488/5908 [00:40<00:34, 70.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 13 items (total=3314)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3504/5908 [00:40<00:34, 70.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=3330)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3520/5908 [00:40<00:33, 71.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 15 items (total=3345)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3536/5908 [00:41<00:33, 69.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=3361)\n",
      "[dedupe] removed 3 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3552/5908 [00:41<00:33, 69.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 13 items (total=3374)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3568/5908 [00:41<00:34, 68.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 14 items (total=3388)\n",
      "[dedupe] removed 7 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3584/5908 [00:41<00:32, 70.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 9 items (total=3397)\n",
      "[dedupe] removed 8 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3600/5908 [00:41<00:31, 73.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 8 items (total=3405)\n",
      "[dedupe] removed 4 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3616/5908 [00:42<00:30, 74.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 12 items (total=3417)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3632/5908 [00:42<00:31, 72.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 15 items (total=3432)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3648/5908 [00:42<00:33, 68.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=3448)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3664/5908 [00:42<00:33, 66.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 15 items (total=3463)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3680/5908 [00:43<00:32, 67.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 14 items (total=3477)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3696/5908 [00:43<00:32, 68.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=3493)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3712/5908 [00:43<00:31, 69.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=3509)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3728/5908 [00:43<00:31, 69.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 14 items (total=3523)\n",
      "[dedupe] removed 5 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3744/5908 [00:43<00:31, 69.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 11 items (total=3534)\n",
      "[dedupe] removed 4 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3760/5908 [00:44<00:31, 69.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 12 items (total=3546)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3776/5908 [00:44<00:31, 68.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=3562)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3792/5908 [00:44<00:31, 67.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=3578)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3808/5908 [00:44<00:33, 63.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=3594)\n",
      "[dedupe] removed 3 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3840/5908 [00:45<00:27, 74.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 13 items (total=3607)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 15 items (total=3622)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3872/5908 [00:45<00:24, 83.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 15 items (total=3637)\n",
      "âœ… Flushed 16 items (total=3653)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3904/5908 [00:46<00:22, 89.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=3669)\n",
      "âœ… Flushed 16 items (total=3685)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3920/5908 [00:46<00:22, 89.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=3701)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3936/5908 [00:46<00:36, 54.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=3717)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3952/5908 [00:47<00:34, 56.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 14 items (total=3731)\n",
      "[dedupe] removed 3 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3968/5908 [00:47<00:32, 60.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 13 items (total=3744)\n",
      "[dedupe] removed 3 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3984/5908 [00:47<00:33, 57.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 13 items (total=3757)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 4000/5908 [00:47<00:32, 58.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 14 items (total=3771)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 4016/5908 [00:48<00:32, 58.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 15 items (total=3786)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 4032/5908 [00:48<00:30, 62.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 15 items (total=3801)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 4048/5908 [00:48<00:28, 64.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=3817)\n",
      "[dedupe] removed 4 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 4064/5908 [00:48<00:27, 67.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 12 items (total=3829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 4080/5908 [00:49<00:27, 65.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=3845)\n",
      "[dedupe] removed 5 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 4096/5908 [00:49<00:27, 66.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 11 items (total=3856)\n",
      "[dedupe] removed 6 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 4112/5908 [00:49<00:28, 63.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 10 items (total=3866)\n",
      "[dedupe] removed 7 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 4128/5908 [00:49<00:27, 64.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 9 items (total=3875)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 4144/5908 [00:49<00:26, 66.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 14 items (total=3889)\n",
      "[dedupe] removed 5 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 4160/5908 [00:50<00:25, 68.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 11 items (total=3900)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 4176/5908 [00:50<00:24, 70.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 14 items (total=3914)\n",
      "[dedupe] removed 3 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 4192/5908 [00:50<00:24, 69.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 13 items (total=3927)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 4208/5908 [00:50<00:25, 67.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 15 items (total=3942)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 4224/5908 [00:51<00:24, 70.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 14 items (total=3956)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 4240/5908 [00:51<00:24, 69.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=3972)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 4256/5908 [00:51<00:23, 68.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 15 items (total=3987)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 4272/5908 [00:52<00:29, 55.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 14 items (total=4001)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 4304/5908 [00:52<00:23, 69.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 14 items (total=4015)\n",
      "âœ… Flushed 16 items (total=4031)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 4336/5908 [00:52<00:19, 82.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 15 items (total=4046)\n",
      "âœ… Flushed 16 items (total=4062)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 4352/5908 [00:52<00:18, 85.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 14 items (total=4076)\n",
      "âœ… Flushed 16 items (total=4092)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 4352/5908 [00:52<00:18, 85.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 15 items (total=4107)\n",
      "[dedupe] removed 5 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 11 items (total=4118)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 4352/5908 [00:52<00:18, 85.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 14 items (total=4132)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 14 items (total=4146)\n",
      "[dedupe] removed 6 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 4464/5908 [00:53<00:05, 249.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 10 items (total=4156)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 15 items (total=4171)\n",
      "[dedupe] removed 4 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 4496/5908 [00:53<00:06, 215.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 12 items (total=4183)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 15 items (total=4198)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4543/5908 [00:53<00:06, 210.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=4214)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 15 items (total=4229)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4565/5908 [00:53<00:08, 162.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=4245)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 14 items (total=4259)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4602/5908 [00:53<00:08, 159.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 15 items (total=4274)\n",
      "[dedupe] removed 5 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 11 items (total=4285)\n",
      "[dedupe] removed 4 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4636/5908 [00:54<00:08, 151.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 12 items (total=4297)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 15 items (total=4312)\n",
      "[dedupe] removed 4 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4668/5908 [00:54<00:08, 146.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 12 items (total=4324)\n",
      "[dedupe] removed 3 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 13 items (total=4337)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4697/5908 [00:54<00:09, 123.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 14 items (total=4351)\n",
      "âœ… Flushed 16 items (total=4367)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4722/5908 [00:54<00:11, 103.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=4383)\n",
      "âœ… Flushed 16 items (total=4399)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4752/5908 [00:55<00:10, 106.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=4415)\n",
      "âœ… Flushed 16 items (total=4431)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4784/5908 [00:55<00:10, 107.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=4447)\n",
      "âœ… Flushed 16 items (total=4463)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4816/5908 [00:55<00:09, 112.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=4479)\n",
      "âœ… Flushed 16 items (total=4495)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4848/5908 [00:56<00:09, 116.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=4511)\n",
      "âœ… Flushed 16 items (total=4527)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4880/5908 [00:56<00:09, 107.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=4543)\n",
      "âœ… Flushed 16 items (total=4559)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4912/5908 [00:56<00:10, 95.11it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=4575)\n",
      "âœ… Flushed 16 items (total=4591)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4944/5908 [00:57<00:09, 99.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=4607)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 15 items (total=4622)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4976/5908 [00:57<00:08, 105.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 3 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 13 items (total=4635)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 15 items (total=4650)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 5008/5908 [00:57<00:08, 110.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=4666)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 15 items (total=4681)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 5040/5908 [00:57<00:07, 112.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=4697)\n",
      "âœ… Flushed 16 items (total=4713)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 5072/5908 [00:58<00:08, 103.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=4729)\n",
      "âœ… Flushed 16 items (total=4745)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 5104/5908 [00:58<00:08, 94.34it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=4761)\n",
      "âœ… Flushed 16 items (total=4777)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 5136/5908 [00:59<00:08, 86.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=4793)\n",
      "âœ… Flushed 16 items (total=4809)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 5152/5908 [00:59<00:08, 88.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=4825)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 5184/5908 [00:59<00:08, 83.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=4841)\n",
      "âœ… Flushed 16 items (total=4857)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 5216/5908 [00:59<00:07, 87.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=4873)\n",
      "âœ… Flushed 16 items (total=4889)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 5248/5908 [01:00<00:07, 86.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=4905)\n",
      "âœ… Flushed 16 items (total=4921)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 5264/5908 [01:00<00:07, 87.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=4937)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 5296/5908 [01:00<00:07, 81.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=4953)\n",
      "âœ… Flushed 16 items (total=4969)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 5312/5908 [01:01<00:07, 85.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=4985)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 5344/5908 [01:01<00:06, 87.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=5001)\n",
      "âœ… Flushed 16 items (total=5017)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 5376/5908 [01:01<00:05, 104.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=5033)\n",
      "âœ… Flushed 16 items (total=5049)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 5408/5908 [01:01<00:04, 120.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=5065)\n",
      "âœ… Flushed 16 items (total=5081)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 5440/5908 [01:02<00:03, 120.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=5097)\n",
      "âœ… Flushed 16 items (total=5113)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 5472/5908 [01:02<00:03, 124.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=5129)\n",
      "âœ… Flushed 16 items (total=5145)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 5504/5908 [01:02<00:03, 115.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=5161)\n",
      "âœ… Flushed 16 items (total=5177)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 5536/5908 [01:02<00:02, 128.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 14 items (total=5191)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 14 items (total=5205)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 5568/5908 [01:03<00:02, 138.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 14 items (total=5219)\n",
      "âœ… Flushed 16 items (total=5235)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 5600/5908 [01:03<00:02, 141.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=5251)\n",
      "[dedupe] removed 5 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 11 items (total=5262)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 5632/5908 [01:03<00:01, 141.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 15 items (total=5277)\n",
      "[dedupe] removed 3 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 13 items (total=5290)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 5664/5908 [01:03<00:01, 138.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=5306)\n",
      "âœ… Flushed 16 items (total=5322)\n",
      "[dedupe] removed 2 duplicate id(s) in the same batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5696/5908 [01:04<00:01, 133.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 14 items (total=5336)\n",
      "âœ… Flushed 16 items (total=5352)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5728/5908 [01:04<00:01, 115.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=5368)\n",
      "âœ… Flushed 16 items (total=5384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5760/5908 [01:04<00:01, 107.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=5400)\n",
      "âœ… Flushed 16 items (total=5416)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5792/5908 [01:05<00:01, 99.25it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=5432)\n",
      "âœ… Flushed 16 items (total=5448)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5824/5908 [01:05<00:00, 108.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 15 items (total=5463)\n",
      "âœ… Flushed 16 items (total=5479)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=5495)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5872/5908 [01:05<00:00, 88.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=5511)\n",
      "âœ… Flushed 16 items (total=5527)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5908/5908 [01:06<00:00, 89.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flushed 16 items (total=5543)\n",
      "âœ… Flushed 16 items (total=5559)\n",
      "[dedupe] removed 1 duplicate id(s) in the same batch)\n",
      "âœ… Flushed 3 items (total=5562)\n",
      "\n",
      "ğŸ‰ Done. Inserted 5562 chunks into 'guideline_chunks' at './chroma_store'.\n",
      "Model: BAAI/bge-large-zh-v1.5 | Device: cuda | Start batch: 16 | max_seq_len: 384\n",
      "[VRAM] end used=2.49 GB / total=11.99 GB (free=9.50 GB)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "===============================================================================\n",
    "Embed Chinese guideline chunks â†’ ChromaDB (OOM-safe, idempotent, metadata-robust)\n",
    "===============================================================================\n",
    "\n",
    "What this script does (high-level)\n",
    "----------------------------------\n",
    "1) Streams a JSONL file that contains one chunk per line:\n",
    "     {\n",
    "       \"content\": \"åˆ†ç« æ–‡æœ¬â€¦â€¦\",\n",
    "       \"meta\": {\n",
    "         \"source\": \"ä¸­å›½é«˜è¡€å‹é˜²æ²»æŒ‡å—(2024å¹´ä¿®è®¢ç‰ˆ).pdf\",\n",
    "         \"year\": 2024,\n",
    "         \"section\": \"3.2 é™å‹ç›®æ ‡\",\n",
    "         \"chunk_id\": 57,\n",
    "         \"authors\": [\"ç§¦ç…œ\", \"å¼ ä¸‰\"]    # lists allowed; we sanitize below\n",
    "       }\n",
    "     }\n",
    "\n",
    "2) Uses a Chinese-capable SentenceTransformer (defaults to BAAI/bge-small-zh)\n",
    "   to embed \"content\" with **GPU if available**.\n",
    "\n",
    "3) Writes embeddings, documents, and **sanitized metadata** into a persistent\n",
    "   **Chroma** collection on disk, in an **idempotent** way:\n",
    "   - Uses **upsert** when available (Chromadb>=0.5) to overwrite duplicates.\n",
    "   - Strengthens IDs to minimize collisions.\n",
    "   - De-duplicates IDs **inside each batch**.\n",
    "   - Final safety net: per-item upsert/repair if a batch raises an error.\n",
    "\n",
    "Inputs (files & environment variables)\n",
    "-------------------------------------\n",
    "â€¢ JSONL file (default `data/guidelines.parsed.jsonl`)\n",
    "  - Set via env var `CAREMIND_DATA`.\n",
    "\n",
    "â€¢ Environment variables (optional, with sensible defaults):\n",
    "  CHROMA_PERSIST_DIR  : Directory for Chroma persistence (default './chroma_store')\n",
    "  CHROMA_COLLECTION   : Collection name (default 'guideline_chunks')\n",
    "  CAREMIND_DATA       : Input JSONL path (default 'data/guidelines.parsed.jsonl')\n",
    "  EMBEDDING_MODEL     : Model id (default 'BAAI/bge-small-zh')\n",
    "  EMBED_BATCH_SIZE    : Starting batch size (default '16')\n",
    "  EMBED_FP16          : '1' to allow fp16 autocast on CUDA (default '1')\n",
    "  EMBED_PROGRESS      : '1' to show progress bars (default '1')\n",
    "  EMBED_MAX_LEN       : Max sequence length for encoder (default '384')\n",
    "  OOM_CPU_FALLBACK    : '1' to allow CPU fallback when BS=1 still OOM (default '1')\n",
    "\n",
    "Outputs\n",
    "-------\n",
    "â€¢ A persistent Chroma database on disk containing:\n",
    "    - ids: stable, low-collision per-chunk IDs\n",
    "    - embeddings: float vectors\n",
    "    - documents: original text chunks\n",
    "    - metadatas: scalar/JSON-safe metadata\n",
    "  Location and collection are controlled by CHROMA_PERSIST_DIR / CHROMA_COLLECTION.\n",
    "\n",
    "Run\n",
    "---\n",
    "$ python embed_to_chroma.py\n",
    "(or set envs first, e.g., EMBED_MAX_LEN=256 EMBED_BATCH_SIZE=8 for tighter VRAM)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List, Dict, Any\n",
    "\n",
    "# NOTE: We intentionally do NOT set PYTORCH_CUDA_ALLOC_CONF here because some\n",
    "# PyTorch builds require a specific format and may crash. All OOM tactics below\n",
    "# (batch backoff, fp16, truncation, cache clears, CPU fallback) work without it.\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from chromadb import PersistentClient\n",
    "from chromadb import errors as chroma_errors  # for DuplicateIDError handling\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------------------------\n",
    "# Config (safe defaults; override via env vars)\n",
    "# ---------------------------\n",
    "PERSIST_DIR      = os.getenv(\"CHROMA_PERSIST_DIR\", \"./chroma_store\")\n",
    "COLLECTION_NAME  = os.getenv(\"CHROMA_COLLECTION\", \"guideline_chunks\")\n",
    "DATA_PATH        = os.getenv(\"CAREMIND_DATA\", \"data/guidelines.parsed.jsonl\")\n",
    "\n",
    "# Chinese-capable model; bge-* are strong + efficient. Start small on 12GB VRAM.\n",
    "EMBEDDING_MODEL  = os.getenv(\"EMBEDDING_MODEL\", \"BAAI/bge-small-zh\")\n",
    "\n",
    "# Start conservatively; dynamic backoff will reduce further on OOM.\n",
    "START_BATCH_SIZE = int(os.getenv(\"EMBED_BATCH_SIZE\", \"16\"))\n",
    "\n",
    "# fp16 autocast reduces memory on CUDA and is safe for sentence-transformers inference\n",
    "USE_FP16         = os.getenv(\"EMBED_FP16\", \"1\") == \"1\"\n",
    "\n",
    "# Lower seq length cuts memory on long Chinese paragraphs; 384 or even 256 works well\n",
    "MAX_SEQ_LEN      = int(os.getenv(\"EMBED_MAX_LEN\", \"384\"))\n",
    "\n",
    "# nice progress bars\n",
    "SHOW_PROGRESS    = os.getenv(\"EMBED_PROGRESS\", \"1\") == \"1\"\n",
    "\n",
    "# If bs=1 still OOMs (rare), push that batch to CPU to finish and keep going\n",
    "CPU_FALLBACK     = os.getenv(\"OOM_CPU_FALLBACK\", \"1\") == \"1\"\n",
    "\n",
    "# Ensure output dir exists early\n",
    "Path(PERSIST_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------------------------\n",
    "# Device and model load\n",
    "# ---------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    # Not a memory tactic, but improves throughput on Ada/RTX40\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "embed_model = SentenceTransformer(EMBEDDING_MODEL, device=device)\n",
    "# Reduce max sequence length to save memory; long texts will be truncated\n",
    "embed_model.max_seq_length = MAX_SEQ_LEN\n",
    "\n",
    "# ---------------------------\n",
    "# ID generation (stable + low collision)\n",
    "# ---------------------------\n",
    "def stable_id(meta: Dict[str, Any], content: str) -> str:\n",
    "    \"\"\"\n",
    "    Stable, low-collision ID:\n",
    "    â€¢ Prefer source|chunk_id|sha12(content) when source+chunk_id exist.\n",
    "    â€¢ Fallback binds to source-hash + content-hash so identical text in different\n",
    "      files remains separate.\n",
    "    This keeps re-runs idempotent and minimizes accidental collisions.\n",
    "    \"\"\"\n",
    "    src = str(meta.get(\"source\", \"\")).strip()\n",
    "    cid = meta.get(\"chunk_id\", None)\n",
    "    ch  = hashlib.sha1(content.encode(\"utf-8\")).hexdigest()[:12]\n",
    "    if src and cid is not None:\n",
    "        return f\"{src}|{cid}|{ch}\"\n",
    "    sh  = hashlib.sha1(src.encode(\"utf-8\")).hexdigest()[:8]\n",
    "    return f\"g_{sh}_{hashlib.sha1(content.encode('utf-8')).hexdigest()[:16]}\"\n",
    "\n",
    "# ---------------------------\n",
    "# JSONL streaming (robust to bad lines)\n",
    "# ---------------------------\n",
    "def jsonl_iter(path: Path) -> Iterable[Dict[str, Any]]:\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for ln, line in enumerate(f, 1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                yield json.loads(line)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"[WARN] Skipping bad JSON at line {ln}: {e}\")\n",
    "                continue\n",
    "\n",
    "# ---------------------------\n",
    "# Metadata sanitizer (prevents list/dict errors in Chroma)\n",
    "# ---------------------------\n",
    "def sanitize_meta(meta: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Chroma requires scalar metadata values: str, int, float, bool, or None.\n",
    "    This function converts:\n",
    "        â€¢ List/Tuple/Set of scalars â†’ \"a, b, c\"\n",
    "        â€¢ Complex lists (mixed/nested) â†’ JSON string\n",
    "        â€¢ Dicts â†’ JSON string (preserve Chinese with ensure_ascii=False)\n",
    "        â€¢ Other types â†’ str(v)\n",
    "    Example fix:\n",
    "        [\"ç§¦ç…œ\"]  â†’ \"ç§¦ç…œ\"\n",
    "    \"\"\"\n",
    "    clean: Dict[str, Any] = {}\n",
    "    for k, v in meta.items():\n",
    "        if isinstance(v, (str, int, float, bool)) or v is None:\n",
    "            clean[k] = v\n",
    "        elif isinstance(v, (list, tuple, set)):\n",
    "            seq = list(v)\n",
    "            if all(isinstance(x, (str, int, float, bool)) or x is None for x in seq):\n",
    "                clean[k] = \", \".join(\"\" if x is None else str(x) for x in seq)\n",
    "            else:\n",
    "                clean[k] = json.dumps(seq, ensure_ascii=False)\n",
    "        elif isinstance(v, dict):\n",
    "            clean[k] = json.dumps(v, ensure_ascii=False, sort_keys=True)\n",
    "        else:\n",
    "            clean[k] = str(v)\n",
    "    return clean\n",
    "\n",
    "# ---------------------------\n",
    "# Optional VRAM stats (debug)\n",
    "# ---------------------------\n",
    "def cuda_mem_summary(prefix: str = \"\"):\n",
    "    if not torch.cuda.is_available():\n",
    "        return\n",
    "    try:\n",
    "        free, total = torch.cuda.mem_get_info()  # bytes\n",
    "        used = total - free\n",
    "        gb = 1024**3\n",
    "        print(f\"[VRAM] {prefix} used={used/gb:.2f} GB / total={total/gb:.2f} GB (free={free/gb:.2f} GB)\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def clear_cuda_cache():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "# ---------------------------\n",
    "# OOM-resilient encoder with dynamic batch backoff + optional CPU fallback\n",
    "# ---------------------------\n",
    "@torch.no_grad()\n",
    "def encode_with_backoff(texts: List[str], start_bs: int, use_fp16: bool, model: SentenceTransformer, cpu_fallback: bool):\n",
    "    \"\"\"\n",
    "    Try encoding with the given batch size. On CUDA OOM:\n",
    "      - Halve batch size and retry.\n",
    "      - If batch size is 1 and still OOM, optionally move to CPU for that batch.\n",
    "    Returns a List[List[float]].\n",
    "    \"\"\"\n",
    "    bs = max(1, start_bs)\n",
    "    current_device = model.device  # torch.device('cuda') or ('cpu')\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            if use_fp16 and current_device.type == \"cuda\":\n",
    "                # AMP saves memory; sentence-transformers is autocast-aware\n",
    "                with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "                    vecs = model.encode(\n",
    "                        texts,\n",
    "                        batch_size=bs,\n",
    "                        normalize_embeddings=True,\n",
    "                        convert_to_numpy=True,\n",
    "                        show_progress_bar=False\n",
    "                    )\n",
    "            else:\n",
    "                vecs = model.encode(\n",
    "                    texts,\n",
    "                    batch_size=bs,\n",
    "                    normalize_embeddings=True,\n",
    "                    convert_to_numpy=True,\n",
    "                    show_progress_bar=False\n",
    "                )\n",
    "            return vecs.tolist()\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            msg = str(e)\n",
    "            if \"CUDA out of memory\" in msg or \"CUBLAS\" in msg:\n",
    "                print(f\"[OOM] CUDA OOM at batch_size={bs} (len={len(texts)}). Backing offâ€¦\")\n",
    "                clear_cuda_cache()\n",
    "                if bs > 1:\n",
    "                    bs = max(1, bs // 2)\n",
    "                    continue\n",
    "                # bs==1 and still OOM â†’ optional CPU fallback\n",
    "                if cpu_fallback and current_device.type == \"cuda\":\n",
    "                    print(\"[OOM] Switching this batch to CPU to complete.\")\n",
    "                    model.to(\"cpu\")\n",
    "                    current_device = torch.device(\"cpu\")\n",
    "                    vecs = model.encode(\n",
    "                        texts,\n",
    "                        batch_size=1,\n",
    "                        normalize_embeddings=True,\n",
    "                        convert_to_numpy=True,\n",
    "                        show_progress_bar=False\n",
    "                    )\n",
    "                    # Move back to CUDA for subsequent batches if available\n",
    "                    if torch.cuda.is_available():\n",
    "                        model.to(\"cuda\")\n",
    "                    return vecs.tolist()\n",
    "                else:\n",
    "                    raise\n",
    "            else:\n",
    "                # Not an OOM error; surface it\n",
    "                raise\n",
    "\n",
    "# ---------------------------\n",
    "# Main pipeline\n",
    "# ---------------------------\n",
    "def main():\n",
    "    # Resolve & validate input\n",
    "    data_path = Path(DATA_PATH).expanduser().resolve()\n",
    "    if not data_path.exists():\n",
    "        raise SystemExit(f\"âŒ JSONL not found: {data_path}\")\n",
    "\n",
    "    # Prepare Chroma\n",
    "    client = PersistentClient(path=PERSIST_DIR)\n",
    "    collection = client.get_or_create_collection(COLLECTION_NAME)\n",
    "\n",
    "    # Count lines for progress bar (optional; ok to skip on huge files)\n",
    "    try:\n",
    "        num_lines = sum(1 for _ in data_path.open(\"r\", encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        num_lines = None\n",
    "\n",
    "    iterator = jsonl_iter(data_path)\n",
    "    if SHOW_PROGRESS:\n",
    "        iterator = tqdm(iterator, total=num_lines, desc=\"Reading JSONL\")\n",
    "\n",
    "    BUFFER: List[Dict[str, Any]] = []\n",
    "    total = 0\n",
    "    current_bs = START_BATCH_SIZE\n",
    "\n",
    "    print(f\"ğŸ“¦ Persist dir: {PERSIST_DIR}\")\n",
    "    print(f\"ğŸ—ƒï¸  Collection: {COLLECTION_NAME}\")\n",
    "    print(f\"ğŸ§  Model: {EMBEDDING_MODEL} | Device: {device} | fp16: {USE_FP16} | max_seq_len: {MAX_SEQ_LEN}\")\n",
    "    print(f\"ğŸ“‘ Input: {data_path}\")\n",
    "    print(f\"âš™ï¸  Start batch size: {START_BATCH_SIZE} | CPU fallback: {CPU_FALLBACK}\")\n",
    "    if torch.cuda.is_available():\n",
    "        cuda_mem_summary(\"start\")\n",
    "\n",
    "    # Inner helper: flush a batch to Chroma robustly\n",
    "    def flush(batch: List[Dict[str, Any]]):\n",
    "        nonlocal total, current_bs\n",
    "        if not batch:\n",
    "            return\n",
    "\n",
    "        # Extract and sanitize\n",
    "        docs  = [b[\"content\"] for b in batch]\n",
    "        metas = [sanitize_meta(b[\"meta\"]) for b in batch]\n",
    "        ids   = [stable_id(b[\"meta\"], b[\"content\"]) for b in batch]\n",
    "\n",
    "        # Embed with OOM backoff\n",
    "        try:\n",
    "            vecs = encode_with_backoff(\n",
    "                docs,\n",
    "                start_bs=current_bs,\n",
    "                use_fp16=USE_FP16,\n",
    "                model=embed_model,\n",
    "                cpu_fallback=CPU_FALLBACK\n",
    "            )\n",
    "        except RuntimeError as e:\n",
    "            raise RuntimeError(f\"Embedding failed for a batch of size {len(docs)}: {e}\") from e\n",
    "        finally:\n",
    "            clear_cuda_cache()\n",
    "\n",
    "        # Intra-batch de-duplication (keeps only first occurrence of each ID)\n",
    "        seen = set()\n",
    "        keep = []\n",
    "        for i, _id in enumerate(ids):\n",
    "            if _id in seen:\n",
    "                continue\n",
    "            seen.add(_id)\n",
    "            keep.append(i)\n",
    "        if len(keep) != len(ids):\n",
    "            print(f\"[dedupe] removed {len(ids) - len(keep)} duplicate id(s) in the same batch)\")\n",
    "\n",
    "        ids   = [ids[i] for i in keep]\n",
    "        docs  = [docs[i] for i in keep]\n",
    "        vecs  = [vecs[i] for i in keep]\n",
    "        metas = [metas[i] for i in keep]\n",
    "\n",
    "        # Write to Chroma, preferring upsert for idempotency\n",
    "        try:\n",
    "            if hasattr(collection, \"upsert\"):\n",
    "                collection.upsert(ids=ids, embeddings=vecs, documents=docs, metadatas=metas)\n",
    "            else:\n",
    "                # Older Chroma: emulate upsert with add+update split\n",
    "                existing = set(collection.get(ids=ids).get(\"ids\", []) or [])\n",
    "                new_idx = [i for i, _id in enumerate(ids) if _id not in existing]\n",
    "                upd_idx = [i for i, _id in enumerate(ids) if _id in existing]\n",
    "                if new_idx:\n",
    "                    collection.add(\n",
    "                        ids=[ids[i] for i in new_idx],\n",
    "                        embeddings=[vecs[i] for i in new_idx],\n",
    "                        documents=[docs[i] for i in new_idx],\n",
    "                        metadatas=[metas[i] for i in new_idx],\n",
    "                    )\n",
    "                if upd_idx:\n",
    "                    collection.update(\n",
    "                        ids=[ids[i] for i in upd_idx],\n",
    "                        embeddings=[vecs[i] for i in upd_idx],\n",
    "                        documents=[docs[i] for i in upd_idx],\n",
    "                        metadatas=[metas[i] for i in upd_idx],\n",
    "                    )\n",
    "\n",
    "        except (chroma_errors.DuplicateIDError, ValueError) as e:\n",
    "            # Final safety net: heal per item (rare edge cases)\n",
    "            print(f\"[warn] batch write hit {type(e).__name__}: {e}\\n[repair] trying per-item upsert/repair\")\n",
    "            for i in range(len(ids)):\n",
    "                try:\n",
    "                    if hasattr(collection, \"upsert\"):\n",
    "                        collection.upsert(\n",
    "                            ids=[ids[i]],\n",
    "                            embeddings=[vecs[i]],\n",
    "                            documents=[docs[i]],\n",
    "                            metadatas=[metas[i]],\n",
    "                        )\n",
    "                    else:\n",
    "                        if collection.get(ids=[ids[i]]).get(\"ids\"):\n",
    "                            collection.update(\n",
    "                                ids=[ids[i]],\n",
    "                                embeddings=[vecs[i]],\n",
    "                                documents=[docs[i]],\n",
    "                                metadatas=[metas[i]],\n",
    "                            )\n",
    "                        else:\n",
    "                            collection.add(\n",
    "                                ids=[ids[i]],\n",
    "                                embeddings=[vecs[i]],\n",
    "                                documents=[docs[i]],\n",
    "                                metadatas=[metas[i]],\n",
    "                            )\n",
    "                except Exception as inner:\n",
    "                    # As a last resort, stringify all non-scalar meta fields and retry once\n",
    "                    fallback_meta = {\n",
    "                        k: (v if isinstance(v, (str, int, float, bool)) or v is None else json.dumps(v, ensure_ascii=False))\n",
    "                        for k, v in metas[i].items()\n",
    "                    }\n",
    "                    if hasattr(collection, \"upsert\"):\n",
    "                        collection.upsert(\n",
    "                            ids=[ids[i]],\n",
    "                            embeddings=[vecs[i]],\n",
    "                            documents=[docs[i]],\n",
    "                            metadatas=[fallback_meta],\n",
    "                        )\n",
    "                    else:\n",
    "                        if collection.get(ids=[ids[i]]).get(\"ids\"):\n",
    "                            collection.update(\n",
    "                                ids=[ids[i]],\n",
    "                                embeddings=[vecs[i]],\n",
    "                                documents=[docs[i]],\n",
    "                                metadatas=[fallback_meta],\n",
    "                            )\n",
    "                        else:\n",
    "                            collection.add(\n",
    "                                ids=[ids[i]],\n",
    "                                embeddings=[vecs[i]],\n",
    "                                documents=[docs[i]],\n",
    "                                metadatas=[fallback_meta],\n",
    "                            )\n",
    "\n",
    "        total += len(ids)\n",
    "        if SHOW_PROGRESS:\n",
    "            tqdm.write(f\"âœ… Flushed {len(ids)} items (total={total})\")\n",
    "        if torch.cuda.is_available() and total % max(64, START_BATCH_SIZE) == 0:\n",
    "            cuda_mem_summary(f\"after {total}\")\n",
    "\n",
    "    # Stream â†’ buffer â†’ flush\n",
    "    for obj in iterator:\n",
    "        # Minimal schema validation\n",
    "        if not isinstance(obj, dict) or \"content\" not in obj or \"meta\" not in obj:\n",
    "            continue\n",
    "        BUFFER.append(obj)\n",
    "        if len(BUFFER) >= current_bs:\n",
    "            flush(BUFFER)\n",
    "            BUFFER.clear()\n",
    "\n",
    "    # Flush remainder\n",
    "    if BUFFER:\n",
    "        flush(BUFFER)\n",
    "        BUFFER.clear()\n",
    "\n",
    "    print(f\"\\nğŸ‰ Done. Inserted {total} chunks into '{COLLECTION_NAME}' at '{PERSIST_DIR}'.\")\n",
    "    print(f\"Model: {EMBEDDING_MODEL} | Device: {device} | Start batch: {START_BATCH_SIZE} | max_seq_len: {MAX_SEQ_LEN}\")\n",
    "    if torch.cuda.is_available():\n",
    "        cuda_mem_summary(\"end\")\n",
    "\n",
    "# ---------------------------\n",
    "# Entry point\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a14076f",
   "metadata": {},
   "source": [
    "The above is the updated, production-ready, and teaching-oriented script with all the fixes you asked for:\n",
    "\n",
    "* OOM-safe on RTX 4070: dynamic batch backoff, FP16 autocast, shorter max sequence length, cache clears, and optional CPU fallback.\n",
    "\n",
    "* Duplicate-ID proof: upsert (with add+update fallback), stronger IDs, intra-batch de-duplication, and a final â€œper-item repairâ€ safety net.\n",
    "\n",
    "* Metadata-safe: robust sanitize_meta(...) to prevent ValueError â€¦ got ['ç§¦ç…œ'] by converting lists/dicts to scalars/JSON.\n",
    "\n",
    "* Rich comments explaining inputs, outputs, and why each tactic exists."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0dd41f",
   "metadata": {},
   "source": [
    "### 3.3 è¯å“æ•°æ®ï¼ˆExcel â†’ SQLiteï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e12e585",
   "metadata": {},
   "source": [
    "å¯ç›´æ¥åœ¨æœ¬åœ°è¿è¡Œçš„ Python è„šæœ¬ï¼ˆå•æ–‡ä»¶ï¼‰ï¼Œç”¨äºä» DrugBankï¼ˆå¯é€‰ï¼Œéœ€ API keyï¼‰ä¸ NMPA æ•°æ®æŸ¥è¯¢ç«™æŠ“å–/è§£æè¯å“ä¿¡æ¯ï¼Œç”Ÿæˆ caremind/data/drugs.xlsxã€‚è„šæœ¬åœ¨æ²¡æœ‰ DrugBank APIçš„æƒ…å†µä¸‹ä¹Ÿèƒ½å·¥ä½œï¼ˆåªæŠ“ NMPAï¼‰ï¼Œå¹¶æ”¯æŒç¦»çº¿è§£ææœ¬åœ° NMPA è¯´æ˜ä¹¦ PDF/HTMLä½œä¸ºå…œåº•ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdf6f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --in IN_FILE --out OUT_FILE\n",
      "                             [--nmpa-offline-dir NMPA_OFFLINE_DIR]\n",
      "                             [--no-nmpa-online] [--no-drugbank]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --in, --out\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/myunix/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "caremind_drugs.py\n",
    "\n",
    "åŒç”¨æ–¹å¼\n",
    "--------\n",
    "1) Notebook å¯¼å…¥ï¼š\n",
    "   from caremind_drugs import run_from_notebook\n",
    "   run_from_notebook(\n",
    "       names=[\"æ°¨æ°¯åœ°å¹³\",\"äºŒç”²åŒèƒ\",\"é˜¿æ‰˜ä¼ä»–æ±€\"],\n",
    "       out_file=\"~/caremind/data/drugs.xlsx\",\n",
    "       nmpa_offline_dir=\"~/caremind/data/nmpa_labels\",\n",
    "       use_nmpa_online=False,\n",
    "       use_drugbank=False\n",
    "   )\n",
    "\n",
    "   æˆ–ä½¿ç”¨åº•å±‚ APIï¼š\n",
    "   from caremind_drugs import build_records, save_to_excel\n",
    "   recs = build_records([...], use_nmpa_online=False, nmpa_offline_dir=\"...\", use_drugbank=False)\n",
    "   save_to_excel(recs, \"~/caremind/data/drugs.xlsx\")\n",
    "\n",
    "2) å‘½ä»¤è¡Œï¼š\n",
    "   python caremind_drugs.py \\\n",
    "     --in ~/caremind/data/drug_list.txt \\\n",
    "     --out ~/caremind/data/drugs.xlsx \\\n",
    "     --nmpa-offline-dir ~/caremind/data/nmpa_labels \\\n",
    "     --no-nmpa-online \\\n",
    "     --no-drugbank\n",
    "\n",
    "ç¯å¢ƒ\n",
    "----\n",
    "- Python 3.10+\n",
    "- pip install: requests beautifulsoup4 lxml pandas openpyxl tenacity pdfminer.six pypdf2 chardet python-dotenv\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations # all type hints are treated as strings, meaning they are not evaluated until later. This is called \"deferred evaluation\" of annotations.\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import argparse # a powerful and flexible module used for parsing command-line arguments\n",
    "from dataclasses import dataclass # a way to simplify the creation of classes that are primarily used to store data\n",
    "from typing import Optional, Dict, List # used for type hinting to specify expected data types. Optional means a value can be of a type or None.\n",
    "\n",
    "import requests # a popular library for making HTTP requests in Python\n",
    "from bs4 import BeautifulSoup # a library for parsing HTML and XML documents\n",
    "import pandas as pd # a powerful data manipulation and analysis library\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type # a library for retrying operations that can fail\n",
    "from pdfminer.high_level import extract_text as pdf_extract_text # a library for extracting text from PDF files\n",
    "from PyPDF2 import PdfReader # a library for reading and manipulating PDF files\n",
    "import chardet # a library for detecting the encoding of text data\n",
    "from dotenv import load_dotenv # a library for loading environment variables from a .env file\n",
    "\n",
    "# ---------------------------\n",
    "# åŸºç¡€é…ç½®\n",
    "# ---------------------------\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (compatible; CareMindDrugBot/1.0; +https://example.org)\"\n",
    "}\n",
    "NEXT_SECTION_RE = re.compile(r\"ã€[^ã€‘]{1,20}ã€‘\")\n",
    "SECTION_PATTERNS = [\n",
    "    (\"é€‚åº”ç—‡\", r\"(?:ã€é€‚åº”ç—‡ã€‘|é€‚\\s*åº”\\s*ç—‡|é€‚åº”è¯|é€‚åº”ç—‡çŠ¶)[ï¼š:\\s]*\"),\n",
    "    (\"ç¦å¿Œç—‡\", r\"(?:ã€ç¦å¿Œã€‘|ç¦\\s*å¿Œ|ç¦å¿Œç—‡)[ï¼š:\\s]*\"),\n",
    "    (\"è¯ç‰©ç›¸äº’ä½œç”¨\", r\"(?:ã€è¯ç‰©ç›¸äº’ä½œç”¨ã€‘|ç›¸äº’ä½œç”¨|è¯ç‰©-è¯ç‰©ç›¸äº’ä½œç”¨)[ï¼š:\\s]*\"),\n",
    "    (\"å¦Šå¨ åˆ†çº§\", r\"(?:ã€å­•å¦‡åŠå“ºä¹³æœŸç”¨è¯ã€‘|å­•å¦‡åŠå“ºä¹³æœŸç”¨è¯|å­•æœŸç”¨è¯|å¦Šå¨ ç”¨è¯)[ï¼š:\\s]*\"),\n",
    "]\n",
    "\n",
    "@dataclass\n",
    "class DrugRecord:\n",
    "    name: str\n",
    "    indications: Optional[str] = None\n",
    "    contraindications: Optional[str] = None\n",
    "    interactions: Optional[str] = None\n",
    "    pregnancy_category: Optional[str] = None\n",
    "    source: Optional[str] = None\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# å·¥å…·å‡½æ•°\n",
    "# ---------------------------\n",
    "def _ensure_utf8(text_bytes) -> str:\n",
    "    if not isinstance(text_bytes, (bytes, bytearray)):\n",
    "        return str(text_bytes)\n",
    "    det = chardet.detect(text_bytes)\n",
    "    enc = det.get(\"encoding\") or \"utf-8\"\n",
    "    try:\n",
    "        return text_bytes.decode(enc, errors=\"ignore\")\n",
    "    except Exception:\n",
    "        return text_bytes.decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "\n",
    "def _slice_section(text: str, start_pat: str) -> Optional[str]:\n",
    "    m = re.search(start_pat, text, flags=re.IGNORECASE)\n",
    "    if not m:\n",
    "        return None\n",
    "    start = m.end()\n",
    "    tail = text[start:]\n",
    "    nxt = NEXT_SECTION_RE.search(tail)\n",
    "    body = tail[:nxt.start()] if nxt else tail\n",
    "    return re.sub(r\"\\s+\", \" \", body).strip() or None\n",
    "\n",
    "\n",
    "def _parse_cn_label_text(full_text: str) -> Dict[str, Optional[str]]:\n",
    "    out = {\"é€‚åº”ç—‡\": None, \"ç¦å¿Œç—‡\": None, \"è¯ç‰©ç›¸äº’ä½œç”¨\": None, \"å¦Šå¨ åˆ†çº§\": None}\n",
    "    text = re.sub(r\"\\s+\", \" \", full_text)\n",
    "    for key, pat in SECTION_PATTERNS:\n",
    "        if key != \"å¦Šå¨ åˆ†çº§\":\n",
    "            out[key] = _slice_section(text, pat) or out[key]\n",
    "    preg_txt = _slice_section(text, SECTION_PATTERNS[-1][1])\n",
    "    if preg_txt:\n",
    "        if re.search(r\"(ç¦ç”¨|ç»å¯¹ç¦ç”¨|ç¦æ­¢ä½¿ç”¨)\", preg_txt):\n",
    "            out[\"å¦Šå¨ åˆ†çº§\"] = \"ç¦ç”¨ï¼ˆæœªæ ‡æ³¨åˆ†çº§ï¼‰\"\n",
    "        elif re.search(r\"(æ…ç”¨|æƒè¡¡åˆ©å¼Š|é£é™©.*æ”¶ç›Š)\", preg_txt):\n",
    "            out[\"å¦Šå¨ åˆ†çº§\"] = \"æ…ç”¨ï¼ˆæœªæ ‡æ³¨åˆ†çº§ï¼‰\"\n",
    "        else:\n",
    "            out[\"å¦Šå¨ åˆ†çº§\"] = \"æœªæ ‡æ³¨\"\n",
    "    else:\n",
    "        out[\"å¦Šå¨ åˆ†çº§\"] = \"æœªæ ‡æ³¨\"\n",
    "    return out\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# NMPA åœ¨çº¿ï¼ˆå ä½ï¼šéœ€æŒ‰å®é™…æ¥å£å®šåˆ¶ï¼‰\n",
    "# ---------------------------\n",
    "class NMPAClient:\n",
    "    def __init__(self, rate_sec: float = 1.2):\n",
    "        self.sess = requests.Session()\n",
    "        self.rate_sec = rate_sec\n",
    "        self.base = \"https://www.nmpa.gov.cn/datasearch/\"\n",
    "\n",
    "    @retry(reraise=True, stop=stop_after_attempt(3),\n",
    "           wait=wait_exponential(multiplier=1, min=1, max=8),\n",
    "           retry=retry_if_exception_type((requests.RequestException,)))\n",
    "    def _get(self, url: str, params: Optional[dict] = None) -> requests.Response:\n",
    "        resp = self.sess.get(url, headers=HEADERS, params=params, timeout=15)\n",
    "        if resp.status_code != 200:\n",
    "            raise requests.RequestException(f\"HTTP {resp.status_code}\")\n",
    "        return resp\n",
    "\n",
    "    def search_label_urls(self, drug_name: str) -> List[str]:\n",
    "        # æé†’ï¼šéœ€æŠ“ç«™ç‚¹çœŸå®æœç´¢æ¥å£åæ›¿æ¢æ­¤å®ç°\n",
    "        return []\n",
    "\n",
    "    def fetch_label_text(self, url: str) -> Optional[str]:\n",
    "        try:\n",
    "            r = self._get(url)\n",
    "            time.sleep(self.rate_sec)\n",
    "            html = _ensure_utf8(r.content)\n",
    "            soup = BeautifulSoup(html, \"lxml\")\n",
    "            container = soup.find(\"div\", class_=\"article\") or soup.find(id=\"article\") or soup\n",
    "            text = container.get_text(separator=\"\\n\")\n",
    "            return re.sub(r\"\\n+\", \"\\n\", text).strip()\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# DrugBank APIï¼ˆå¯é€‰ï¼‰\n",
    "# ---------------------------\n",
    "class DrugBankClient:\n",
    "    def __init__(self):\n",
    "        load_dotenv()\n",
    "        self.api_key = os.getenv(\"DRUGBANK_API_KEY\")\n",
    "        self.base = os.getenv(\"DRUGBANK_BASE\", \"https://api.drugbank.com/v1\")\n",
    "        self.sess = requests.Session()\n",
    "        if self.api_key:\n",
    "            self.sess.headers.update({\n",
    "                \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "                \"Accept\": \"application/json\",\n",
    "                \"User-Agent\": HEADERS[\"User-Agent\"],\n",
    "            })\n",
    "\n",
    "    def available(self) -> bool:\n",
    "        return bool(self.api_key)\n",
    "\n",
    "    @retry(reraise=True, stop=stop_after_attempt(3),\n",
    "           wait=wait_exponential(multiplier=1, min=1, max=8),\n",
    "           retry=retry_if_exception_type((requests.RequestException,)))\n",
    "    def _get(self, path: str, params: Optional[dict] = None) -> dict:\n",
    "        url = self.base.rstrip(\"/\") + \"/\" + path.lstrip(\"/\")\n",
    "        resp = self.sess.get(url, params=params, timeout=20)\n",
    "        resp.raise_for_status()\n",
    "        return resp.json()\n",
    "\n",
    "    def query_by_name(self, name: str) -> Optional[dict]:\n",
    "        try:\n",
    "            j = self._get(\"drugs\", params={\"name\": name})\n",
    "            first = (j[0] if isinstance(j, list) and j else j) or None\n",
    "            if not first:\n",
    "                return None\n",
    "            did = first.get(\"id\") or first.get(\"drugbank_id\")\n",
    "            return self._get(f\"drugs/{did}\") if did else first\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def pick_fields(detail: dict) -> Dict[str, Optional[str]]:\n",
    "        def g(d, *ks, default=None):\n",
    "            cur = d\n",
    "            for k in ks:\n",
    "                if cur is None:\n",
    "                    return default\n",
    "                cur = cur.get(k)\n",
    "            return cur if cur is not None else default\n",
    "\n",
    "        indications = g(detail, \"indication\") or g(detail, \"indications\")\n",
    "        contraindications = g(detail, \"contraindications\")\n",
    "        interactions = None\n",
    "        intr = g(detail, \"drug_interactions\")\n",
    "        if isinstance(intr, list) and intr:\n",
    "            parts = []\n",
    "            for it in intr[:30]:\n",
    "                desc = it.get(\"description\") or it.get(\"text\")\n",
    "                partner = it.get(\"name\") or it.get(\"drug\") or \"\"\n",
    "                if desc:\n",
    "                    parts.append(f\"{partner}: {desc}\")\n",
    "            if parts:\n",
    "                interactions = \"ï¼›\".join(parts)\n",
    "        preg = g(detail, \"pregnancy_category\") or g(detail, \"fda_pregnancy_category\")\n",
    "        return {\"é€‚åº”ç—‡\": indications, \"ç¦å¿Œç—‡\": contraindications,\n",
    "                \"è¯ç‰©ç›¸äº’ä½œç”¨\": interactions, \"å¦Šå¨ åˆ†çº§\": preg}\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# ç¦»çº¿è¯´æ˜ä¹¦è§£æ\n",
    "# ---------------------------\n",
    "def _read_pdf_text(path: str) -> str:\n",
    "    try:\n",
    "        txt = pdf_extract_text(path) or \"\"\n",
    "        if not txt:\n",
    "            reader = PdfReader(path)\n",
    "            buf = [(p.extract_text() or \"\") for p in reader.pages]\n",
    "            txt = \"\\n\".join(buf)\n",
    "        return txt\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def _read_html_text(path: str) -> str:\n",
    "    try:\n",
    "        with open(path, \"rb\") as f:\n",
    "            raw = f.read()\n",
    "        soup = BeautifulSoup(_ensure_utf8(raw), \"lxml\")\n",
    "        return re.sub(r\"\\n+\", \"\\n\", soup.get_text(separator=\"\\n\")).strip()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def _scan_offline_label(drug_name: str, folder: Optional[str]) -> Optional[str]:\n",
    "    if not folder or not os.path.isdir(os.path.expanduser(folder)):\n",
    "        return None\n",
    "    folder = os.path.expanduser(folder)\n",
    "    cands = [os.path.join(folder, fn) for fn in os.listdir(folder)\n",
    "             if re.search(re.escape(drug_name), fn, flags=re.IGNORECASE)]\n",
    "    cands = sorted(cands, key=lambda p: (0 if p.lower().endswith((\".html\", \".htm\")) else 1, p))\n",
    "    for p in cands:\n",
    "        if p.lower().endswith((\".html\", \".htm\")):\n",
    "            txt = _read_html_text(p)\n",
    "        elif p.lower().endswith(\".pdf\"):\n",
    "            txt = _read_pdf_text(p)\n",
    "        else:\n",
    "            continue\n",
    "        if txt and len(txt) > 50:\n",
    "            return txt\n",
    "    return None\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# æ ¸å¿ƒæ„å»ºä¸è¾“å‡º\n",
    "# ---------------------------\n",
    "def build_records(\n",
    "    names: List[str],\n",
    "    use_nmpa_online: bool = True,\n",
    "    nmpa_offline_dir: Optional[str] = None,\n",
    "    use_drugbank: bool = True\n",
    ") -> List[DrugRecord]:\n",
    "\n",
    "    nmpa = NMPAClient()\n",
    "    db = DrugBankClient()\n",
    "    if use_drugbank and not db.available():\n",
    "        use_drugbank = False\n",
    "\n",
    "    out: List[DrugRecord] = []\n",
    "    for name in names:\n",
    "        rec = DrugRecord(name=name)\n",
    "\n",
    "        # NMPA åœ¨çº¿ï¼ˆå½“å‰ä¸ºå ä½ï¼›è¿”å›ç©ºå³è·³è¿‡ï¼‰\n",
    "        if use_nmpa_online:\n",
    "            urls = nmpa.search_label_urls(name)\n",
    "            text = None\n",
    "            for u in urls:\n",
    "                text = nmpa.fetch_label_text(u)\n",
    "                if text:\n",
    "                    break\n",
    "            if text:\n",
    "                parsed = _parse_cn_label_text(text)\n",
    "                rec.indications = parsed[\"é€‚åº”ç—‡\"]\n",
    "                rec.contraindications = parsed[\"ç¦å¿Œç—‡\"]\n",
    "                rec.interactions = parsed[\"è¯ç‰©ç›¸äº’ä½œç”¨\"]\n",
    "                rec.pregnancy_category = parsed[\"å¦Šå¨ åˆ†çº§\"]\n",
    "                rec.source = \"NMPAè¯´æ˜ä¹¦ï¼ˆåœ¨çº¿ï¼‰\"\n",
    "\n",
    "        # DrugBankï¼ˆå¯é€‰ï¼‰\n",
    "        if use_drugbank and not all([rec.indications, rec.contraindications, rec.interactions, rec.pregnancy_category]):\n",
    "            detail = db.query_by_name(name)\n",
    "            if detail:\n",
    "                picked = DrugBankClient.pick_fields(detail)\n",
    "                rec.indications = rec.indications or picked[\"é€‚åº”ç—‡\"]\n",
    "                rec.contraindications = rec.contraindications or picked[\"ç¦å¿Œç—‡\"]\n",
    "                rec.interactions = rec.interactions or picked[\"è¯ç‰©ç›¸äº’ä½œç”¨\"]\n",
    "                rec.pregnancy_category = rec.pregnancy_category or picked[\"å¦Šå¨ åˆ†çº§\"]\n",
    "                rec.source = (rec.source + \" + DrugBank\") if rec.source else \"DrugBank\"\n",
    "\n",
    "        # ç¦»çº¿å…œåº•\n",
    "        if not any([rec.indications, rec.contraindications, rec.interactions, rec.pregnancy_category]):\n",
    "            text = _scan_offline_label(name, nmpa_offline_dir)\n",
    "            if text:\n",
    "                parsed = _parse_cn_label_text(text)\n",
    "                rec.indications = parsed[\"é€‚åº”ç—‡\"]\n",
    "                rec.contraindications = parsed[\"ç¦å¿Œç—‡\"]\n",
    "                rec.interactions = parsed[\"è¯ç‰©ç›¸äº’ä½œç”¨\"]\n",
    "                rec.pregnancy_category = parsed[\"å¦Šå¨ åˆ†çº§\"]\n",
    "                rec.source = \"NMPAè¯´æ˜ä¹¦ï¼ˆç¦»çº¿ï¼‰\"\n",
    "\n",
    "        # å¡«è¡¥æœªæ ‡æ³¨\n",
    "        rec.indications = rec.indications or \"æœªæ ‡æ³¨\"\n",
    "        rec.contraindications = rec.contraindications or \"æœªæ ‡æ³¨\"\n",
    "        rec.interactions = rec.interactions or \"æœªæ ‡æ³¨\"\n",
    "        rec.pregnancy_category = rec.pregnancy_category or \"æœªæ ‡æ³¨\"\n",
    "        rec.source = rec.source or \"æœªè·å–ï¼ˆè¯·è¡¥å……æºï¼‰\"\n",
    "\n",
    "        out.append(rec)\n",
    "    return out\n",
    "\n",
    "\n",
    "def save_to_excel(records: List[DrugRecord], out_path: str):\n",
    "    os.makedirs(os.path.dirname(os.path.expanduser(out_path)), exist_ok=True)\n",
    "    rows = [{\n",
    "        \"è¯å“åç§°\": r.name,\n",
    "        \"é€‚åº”ç—‡\": r.indications,\n",
    "        \"ç¦å¿Œç—‡\": r.contraindications,\n",
    "        \"è¯ç‰©ç›¸äº’ä½œç”¨\": r.interactions,\n",
    "        \"å¦Šå¨ åˆ†çº§\": r.pregnancy_category,\n",
    "        \"æ¥æº\": r.source,\n",
    "    } for r in records]\n",
    "    df = pd.DataFrame(rows, columns=[\"è¯å“åç§°\",\"é€‚åº”ç—‡\",\"ç¦å¿Œç—‡\",\"è¯ç‰©ç›¸äº’ä½œç”¨\",\"å¦Šå¨ åˆ†çº§\",\"æ¥æº\"])\n",
    "    df.to_excel(os.path.expanduser(out_path), index=False)\n",
    "    print(f\"âœ… å·²å†™å‡º {len(df)} æ¡ â†’ {os.path.expanduser(out_path)}\")\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Notebook å‹å¥½å…¥å£\n",
    "# ---------------------------\n",
    "def run_from_notebook(\n",
    "    names: Optional[List[str]] = None,\n",
    "    in_file: Optional[str] = None,\n",
    "    out_file: str = \"~/caremind/data/drugs.xlsx\",\n",
    "    nmpa_offline_dir: Optional[str] = None,\n",
    "    use_nmpa_online: bool = False,\n",
    "    use_drugbank: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    - names å’Œ in_file äºŒé€‰ä¸€ï¼›éƒ½æä¾›æ—¶ä»¥ names ä¼˜å…ˆã€‚\n",
    "    - é»˜è®¤å…³é—­åœ¨çº¿æ£€ç´¢ä¸ DrugBankï¼Œåˆ©äºå…ˆè·‘é€šç¦»çº¿æµç¨‹ã€‚\n",
    "    \"\"\"\n",
    "    if not names:\n",
    "        if not in_file:\n",
    "            raise ValueError(\"è¯·æä¾› names åˆ—è¡¨æˆ– in_file è·¯å¾„\")\n",
    "        in_file = os.path.expanduser(in_file)\n",
    "        if not os.path.exists(in_file):\n",
    "            raise FileNotFoundError(f\"æœªæ‰¾åˆ°è¯å“æ¸…å•ï¼š{in_file}\")\n",
    "        with open(in_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            names = [ln.strip() for ln in f if ln.strip()]\n",
    "\n",
    "    recs = build_records(\n",
    "        names=names,\n",
    "        use_nmpa_online=use_nmpa_online,\n",
    "        nmpa_offline_dir=os.path.expanduser(nmpa_offline_dir) if nmpa_offline_dir else None,\n",
    "        use_drugbank=use_drugbank\n",
    "    )\n",
    "    save_to_excel(recs, out_file)\n",
    "    return recs\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# CLI å…¥å£\n",
    "# ---------------------------\n",
    "def _load_names(path: str) -> List[str]:\n",
    "    with open(os.path.expanduser(path), \"r\", encoding=\"utf-8\") as f:\n",
    "        return [ln.strip() for ln in f if ln.strip()]\n",
    "\n",
    "def main():\n",
    "    ap = argparse.ArgumentParser(description=\"Build drugs.xlsx from NMPA/DrugBank/local labels\")\n",
    "    ap.add_argument(\"--in\", dest=\"in_file\", required=True, help=\"è¯å“æ¸…å• txtï¼ˆæ¯è¡Œä¸€ä¸ªè¯åï¼‰\")\n",
    "    ap.add_argument(\"--out\", dest=\"out_file\", required=True, help=\"è¾“å‡º Excel è·¯å¾„\")\n",
    "    ap.add_argument(\"--nmpa-offline-dir\", dest=\"nmpa_offline_dir\", default=None, help=\"ç¦»çº¿ NMPA è¯´æ˜ä¹¦ç›®å½•ï¼ˆå¯é€‰ï¼‰\")\n",
    "    ap.add_argument(\"--no-nmpa-online\", action=\"store_true\", help=\"ç¦ç”¨ NMPA åœ¨çº¿æ£€ç´¢\")\n",
    "    ap.add_argument(\"--no-drugbank\", action=\"store_true\", help=\"ç¦ç”¨ DrugBank API\")\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    names = _load_names(args.in_file)\n",
    "    recs = build_records(\n",
    "        names=names,\n",
    "        use_nmpa_online=not args.no_nmpa_online,\n",
    "        nmpa_offline_dir=args.nmpa_offline_dir,\n",
    "        use_drugbank=not args.no_drugbank\n",
    "    )\n",
    "    save_to_excel(recs, args.out_file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3cc22e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'run_from_notebook' from 'caremind_drugs' (/home/myunix/caremind/caremind_drugs.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      3\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m~/caremind\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcaremind_drugs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m run_from_notebook\n\u001b[1;32m      7\u001b[0m recs \u001b[38;5;241m=\u001b[39m run_from_notebook(\n\u001b[1;32m      8\u001b[0m     names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mæ°¨æ°¯åœ°å¹³\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mäºŒç”²åŒèƒ\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mé˜¿æ‰˜ä¼ä»–æ±€\u001b[39m\u001b[38;5;124m\"\u001b[39m],        \u001b[38;5;66;03m# æˆ–ç”¨ in_file=\"~/caremind/data/drug_list.txt\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     out_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m~/caremind/data/drugs.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     use_drugbank\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m       \u001b[38;5;66;03m# è‹¥é…ç½®äº† DrugBank API å†æ”¹ True\u001b[39;00m\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mlen\u001b[39m(recs)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'run_from_notebook' from 'caremind_drugs' (/home/myunix/caremind/caremind_drugs.py)"
     ]
    }
   ],
   "source": [
    "# å‡è®¾ caremind_drugs.py ä¸ Notebook åŒåœ¨ ~/caremind/\n",
    "import sys, os\n",
    "sys.path.append(os.path.expanduser(\"~/caremind\"))\n",
    "\n",
    "from caremind_drugs import run_from_notebook\n",
    "\n",
    "recs = run_from_notebook(\n",
    "    names=[\"æ°¨æ°¯åœ°å¹³\",\"äºŒç”²åŒèƒ\",\"é˜¿æ‰˜ä¼ä»–æ±€\"],        # æˆ–ç”¨ in_file=\"~/caremind/data/drug_list.txt\"\n",
    "    out_file=\"~/caremind/data/drugs.xlsx\",\n",
    "    nmpa_offline_dir=\"~/caremind/data/nmpa_labels\",\n",
    "    use_nmpa_online=False,   # ä½ ç¨åå®Œå–„ NMPA åœ¨çº¿æ¥å£åå¯æ”¹ True\n",
    "    use_drugbank=False       # è‹¥é…ç½®äº† DrugBank API å†æ”¹ True\n",
    ")\n",
    "len(recs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321db1f6",
   "metadata": {},
   "source": [
    "æ‰‹å·¥æ•´ç† data/drugs.xlsxï¼ˆå­—æ®µï¼šnameã€indicationsã€contraindicationsã€interactionsã€dosageã€pregnancy_categoryã€sourceï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a34132c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RangeIndex(start=0, stop=0, step=1)\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "near \")\": syntax error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDrugs loaded into SQLite.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 35\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 29\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mhead())\n\u001b[0;32m---> 29\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdrugs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_exists\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreplace\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m con\u001b[38;5;241m.\u001b[39mcommit()\n\u001b[1;32m     31\u001b[0m con\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/caremind/lib/python3.10/site-packages/pandas/util/_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    332\u001b[0m     )\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/caremind/lib/python3.10/site-packages/pandas/core/generic.py:3106\u001b[0m, in \u001b[0;36mNDFrame.to_sql\u001b[0;34m(self, name, con, schema, if_exists, index, index_label, chunksize, dtype, method)\u001b[0m\n\u001b[1;32m   2908\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2909\u001b[0m \u001b[38;5;124;03mWrite records stored in a DataFrame to a SQL database.\u001b[39;00m\n\u001b[1;32m   2910\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3102\u001b[0m \u001b[38;5;124;03m[(1,), (None,), (2,)]\u001b[39;00m\n\u001b[1;32m   3103\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m   3104\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sql\n\u001b[0;32m-> 3106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_sql\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3107\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mif_exists\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_exists\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3117\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/caremind/lib/python3.10/site-packages/pandas/io/sql.py:844\u001b[0m, in \u001b[0;36mto_sql\u001b[0;34m(frame, name, con, schema, if_exists, index, index_label, chunksize, dtype, method, engine, **engine_kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mframe\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m argument should be either a Series or a DataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    841\u001b[0m     )\n\u001b[1;32m    843\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pandasSQL_builder(con, schema\u001b[38;5;241m=\u001b[39mschema, need_transaction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[0;32m--> 844\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpandas_sql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_sql\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_exists\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_exists\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/caremind/lib/python3.10/site-packages/pandas/io/sql.py:2840\u001b[0m, in \u001b[0;36mSQLiteDatabase.to_sql\u001b[0;34m(self, frame, name, if_exists, index, index_label, schema, chunksize, dtype, method, engine, **engine_kwargs)\u001b[0m\n\u001b[1;32m   2829\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmy_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) not a string\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2831\u001b[0m table \u001b[38;5;241m=\u001b[39m SQLiteTable(\n\u001b[1;32m   2832\u001b[0m     name,\n\u001b[1;32m   2833\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2838\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   2839\u001b[0m )\n\u001b[0;32m-> 2840\u001b[0m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2841\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m table\u001b[38;5;241m.\u001b[39minsert(chunksize, method)\n",
      "File \u001b[0;32m~/miniconda3/envs/caremind/lib/python3.10/site-packages/pandas/io/sql.py:991\u001b[0m, in \u001b[0;36mSQLTable.create\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mif_exists \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    990\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpd_sql\u001b[38;5;241m.\u001b[39mdrop_table(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema)\n\u001b[0;32m--> 991\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_create\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mif_exists \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    993\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/caremind/lib/python3.10/site-packages/pandas/io/sql.py:2514\u001b[0m, in \u001b[0;36mSQLiteTable._execute_create\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2512\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpd_sql\u001b[38;5;241m.\u001b[39mrun_transaction() \u001b[38;5;28;01mas\u001b[39;00m conn:\n\u001b[1;32m   2513\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stmt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtable:\n\u001b[0;32m-> 2514\u001b[0m         \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstmt\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOperationalError\u001b[0m: near \")\": syntax error"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3, os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "db_path = os.getenv(\"SQLITE_PATH\", \"./db/drugs.sqlite\")\n",
    "os.makedirs(os.path.dirname(db_path), exist_ok=True)\n",
    "\n",
    "schema = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS drugs (\n",
    "  id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "  name TEXT UNIQUE,\n",
    "  indications TEXT,\n",
    "  contraindications TEXT,\n",
    "  interactions TEXT,\n",
    "  dosage TEXT,\n",
    "  pregnancy_category TEXT,\n",
    "  source TEXT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "def main():\n",
    "    df = pd.read_excel(\"data/drugs.xlsx\")\n",
    "    con = sqlite3.connect(db_path)\n",
    "    cur = con.cursor()\n",
    "    cur.executescript(schema)\n",
    "    print(df.columns)\n",
    "    print(df.head())\n",
    "    df.to_sql(\"drugs\", con, if_exists=\"replace\", index=False)\n",
    "    con.commit()\n",
    "    con.close()\n",
    "    print(\"Drugs loaded into SQLite.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "caremind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
